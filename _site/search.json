[
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nNHS-R Community has prepared the content of this website responsibly and carefully. However, NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nNHS-R Community has prepared the content of this website responsibly and carefully. However, NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nNHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nAll content published through this site is open under the CC0 licence unless otherwise stated. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s)."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nNHS-R Community editors and contributors recommend external web links on the basis of their suitability and usefulness for our users.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders of any product, service, policy or opinion of the organisation or individual. NHS-R Community, its editors, The Strategy Unit, NHS Midlands and Lancashire Commissioning Support Unit, The Health Foundation or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of NHS-R Community is final on publishing of content and no correspondence will be entered into.\nIf you wish to report a concern, please email nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct and further details for contribution can be found in NHS-R Statement on Tools.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nThe NHS-R Community website is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting."
  },
  {
    "objectID": "ts-and-cs.html#code-of-conduct",
    "href": "ts-and-cs.html#code-of-conduct",
    "title": "Terms and conditions",
    "section": "Code of conduct",
    "text": "Code of conduct\nHow to technically contribute to this website can be found in the [Statement on Tools](https://tools.nhsrcommunity.com/contribution.html and the Code of Conduct can be found in the repository Code of Conduct.\nDetails of the Notice and Takedown Policy can be found in the NHS-R Way book along with details on how submissions with sensitive information will be rectified."
  },
  {
    "objectID": "licence.html",
    "href": "licence.html",
    "title": "Licence",
    "section": "",
    "text": "This website, its content and code are released under a CC0 1.0 Universal.\nBlogs that are published through this site will also be under this license and can be referenced to personal sites if the author wishes.\n\n  \n\nFor more information on the licences used by NHS-R Community go to the chapter Style Guide for code in the NHS-R Way book.\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "NHS-R Community Quarto website",
    "section": "Books",
    "text": "Books\n\n\n\n\n\n\n\n\n\n\n\nHealth Inequalities\n\n\nThis project is a collection of information and knowledge related to analytical work on “Health Inequalities”, as performed in the NHS and the wider UK Health and Social…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Way book\n\n\nEverything you need or want to know about NHS-R Community including: how to contribute and get involved, code styles, training preparation materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Analytics Resources\n\n\nUseful links for health and care analysts and data scientists.\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "NHS-R Community Quarto website",
    "section": "Blog",
    "text": "Blog\n\n\n\n\n\n\n\n\n\nBuilding a Quarto website for NHS-R Community\n\n\n\n\n\n\nZoë Turner\n\n\nFeb 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Data-Driven Frontier: Insights from an NHS-R Committee Member\n\n\n\n\n\n\nPrajwal Khairnar\n\n\nFeb 6, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCount of working days function\n\n\n\n\n\n\nZoë Turner\n\n\nJul 16, 2019\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nhs.rcommunity@nhs.net. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nhs.rcommunity@nhs.net. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "books/posts/open-analytics-resources/index.html",
    "href": "books/posts/open-analytics-resources/index.html",
    "title": "Open Analytics Resources",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "books/posts/health-inequalities/index.html",
    "href": "books/posts/health-inequalities/index.html",
    "title": "Health Inequalities",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html",
    "href": "blog/why-government-needs-sustainable-software-too.html",
    "title": "Why Government needs sustainable software too",
    "section": "",
    "text": "Unlike most of the 2017/2018 cohort, when I applied to become a fellow of the Software Sustainability Institute, I was a civil servant rather than an academic. In this blog post I want to talk about why Government needs sustainable software, the work being done to deliver it, and the lessons we learnt after the first year. But Government already has sustainable software…\nThere’s quite a bit of disambiguation that needs to be done to the statement ‘Government needs sustainable software’. In fact, Government already has sustainable software, and lots of it. One need only look at alphagov, the GitHub organisation for the Government Digital Service. Sustainable, often open source, software is alive and well here, written by professional software developers, and in many other places in central and local Government alike. But this isn’t the whole story.\nThere are other parts of Government that write software, but like many in academia, you may have a hard time convincing them of this fact. In central Government (this is where my experience lies, so I will focus largely upon it) there are literally thousands of statisticians, operational researchers, social researchers, economists, scientists, and engineers. Any one of these may be writing code in a variety of languages in the course of their daily work, but don’t identify as software developers. It’s among these professions that there are tasks that will look most familiar to the academic researcher. Government statisticians in particular are tasked with producing periodic publications which incorporate data, visualisations, and analyses, much like academic outputs.\nSo in this blog post, I’m really talking about bespoke software that is used to create Government statistical publications."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#why-sustainability-is-so-important-for-government",
    "href": "blog/why-government-needs-sustainable-software-too.html#why-sustainability-is-so-important-for-government",
    "title": "Why Government needs sustainable software too",
    "section": "Why sustainability is so important for Government",
    "text": "Why sustainability is so important for Government\nThe reasons for sustainability in academic publications have been well documented by the Software Sustainability Institute, but I would argue that it is even more important that Government writes reproducible and sustainable software for its statistical publications. Here’s why:\n\nThe outputs really matter\nI don’t want to downplay the importance of research outputs, publishing accurate science is critical to advancing human knowledge. What is different about research is that there is rarely a single source of truth. If a research group publishes a groundbreaking finding, we all take notice; but we don’t trust the findings until they have been replicated preferably by several other groups.\nIt’s not like that in Government. If a Government department publishes a statistic, in many cases that is the single source of truth, so it is critical that the statistics are both timely and accurate.\n\n\nPublications are often produced by multiple people\nThe second way that Government statistical publications differ from academic scientific publications is that they are often produced by a team of people that is regularly changing. This means that even at the point that it is being produced it needs to be easy for another member of the team to pick up the work and run with it. If someone goes on holiday, or is sick at the critical moment, their colleagues need to be able to pick up from where they left off immediately, and understand all the idiosyncrasies perfectly. The knowledge simply cannot rest in one person’s head.\nMore than that, since publications are often periodic (e.g. monthly, or annual) and analysts typically change role once a year, the work will very likely need to be handed off to someone new on a regular basis. It is essential therefore that these processes are well documented, and that the code that is being handed over works as expected.\n\n\nThe taxpayer pays for it\nObviously, the longer it takes a team of statisticians to produce a statistical report in Government, the more it costs to the taxpayer, and all Government departments have an interest in being efficient, and reducing unnecessary waste.\nAdditionally, since Government statistical publications are paid for by the public, where possible Government should be open and publish its workings. Coding in the open is already an important part of the culture among digital professions, adopting sustainable software practices allows statistical publications to be produced with the same openness.\n\n\nWorking towards sustainability\nI started working in Government as a Data Scientist after doing a PhD and post-doc in environmental science. I’d attended two Software Carpentry workshops during this time, and wrote my PhD in LaTeX and R. On joining Government it was clear that we could apply some of these lessons to improve the reporting workflow in Government.\nWorking with the Department for Digital, Culture, Media, and Sport (DCMS) we had a first attempt at implementing a reproducible workflow for a statistical publication that was being produced with manual processes using a number of spreadsheets, a statistical computing package, and a word processor. We used RMarkdown to rewrite the publication, and abstracted the logic into an R package freely available on GitHub, complete with continuous integration from travis and appveyor.\nIn March of 2017 we published this work in a blog post, and worked hard to publicise this work with a large number of presentations and demonstrations to other Government departments. The prototype generated lots of interest; in particular an initial estimate that it could save 75% of the time taken to produce the same publication using the old methods.\nBy November we blogged again about successful trials of this approach in two further departments: the Ministry of Justice (MoJ), the Department for Education (DfE). We also produced a GitBook describing the various steps in more detail. Most of this is sensible software development practice; but it’s something that many Government analysts have not done before.\nBy the end of the year, the ideas had gained enough traction in the Government statistical community, that the Director General for the Office of Statistics Regulation (the body responsible for ensuring quality among official statistics) reported that this work was his favourite innovation of the year, although he wasn’t so keen on the name!\nWork continues to bring these techniques to a wider audience. There’s now a free online course built by one of my former colleagues to help civil servants get started, and a number of departments, particularly the MoJ are making great strides to incorporate these techniques into their workflows."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#lessons-learnt",
    "href": "blog/why-government-needs-sustainable-software-too.html#lessons-learnt",
    "title": "Why Government needs sustainable software too",
    "section": "Lessons learnt",
    "text": "Lessons learnt\nA year or so after we set out with the intention of bringing sustainable software to Government statisticians, here are some of the lessons that I would like to share."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#reproducibility-is-technical-sustainability-is-social",
    "href": "blog/why-government-needs-sustainable-software-too.html#reproducibility-is-technical-sustainability-is-social",
    "title": "Why Government needs sustainable software too",
    "section": "Reproducibility is technical, sustainability is social",
    "text": "Reproducibility is technical, sustainability is social\nWe called the first prototype a ‘Reproducible Analytical Pipeline’ and acronym ‘RAP’ has stuck. This is not a very good name on reflection because it belies the main difficulty in transitioning from manual workflows into something more automated: making it sustainable. It’s very well creating beautiful, abstracted, replicable data workflows, but they are completely useless if no one knows how to use them, or to update them. That situation is more dangerous than the manual workflows that exist in many places at present, because at least the barrier to entry for tortuous manual processes is lower: you don’t need to know how to program to interpret a complicated spreadsheet, you just need a lot of patience.\nWhat this move from manual to automated implies is a recognition of the need for specialists; organisations will need to recruit specialists, make use of the ones they already have, and upskill other staff. This is a challenge that all organisations will need to rise to if they are to make these new methods stick.\nThis is likely to be less of a problem for academia, where within certain fields there is already an expectation that researchers will be able to use particular tools, and there may be more time to develop expertise away from operational pressures. However, there also exists a powerful disincentive: because journal articles are closer to ‘one off’ than a periodic report, it is less critical that researchers leave the code behind a paper in a good state, as they may never need to come back to it again."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#senior-buy-in-is-critical",
    "href": "blog/why-government-needs-sustainable-software-too.html#senior-buy-in-is-critical",
    "title": "Why Government needs sustainable software too",
    "section": "Senior buy-in is critical",
    "text": "Senior buy-in is critical\nIn just over a year, we went from seeing an opportunity to scaling the idea across a number of Government departments, traditionally very conservative organisations. Getting the buy-in of senior officials was absolutely critical in our ability to get the idea accepted.\nIt’s important to realise early that senior managers are often interested in very different things to the users of the software, so messages need to be targeted to gain traction with the right audience. For instance, an incentive for managers in academia might be: mitigating the risk of errors that could lead to retraction, rather than by the expectation of cost savings."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#time-is-money",
    "href": "blog/why-government-needs-sustainable-software-too.html#time-is-money",
    "title": "Why Government needs sustainable software too",
    "section": "Time is money",
    "text": "Time is money\nOne of the reasons that we managed to make a big impact quickly is because Government departments are always keen to reduce costs. If a publication takes a team of four people a few weeks to produce, the cost quickly adds up. This is a feature of Government (and indeed industry) which is not shared by academia. Yes, it matters that work is delivered on time, but in my experience researcher time is a much more elastic resource. I was much more likely to work all evening or over the weekend as a PhD student or post doctoral researcher than I was as a civil servant; it was almost an expectation. For this reason, the financial imperative seems to be a much less powerful incentive in academia."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#its-not-all-about-the-code",
    "href": "blog/why-government-needs-sustainable-software-too.html#its-not-all-about-the-code",
    "title": "Why Government needs sustainable software too",
    "section": "It’s not all about the code",
    "text": "It’s not all about the code\nNotwithstanding my comments about sustainability, it is important to note that reproducibility does not stop with reproducible code. We also need to worry about the data, and the environment. The former is particularly difficult in a Government setting, as one department often relies on another to provide the data, meaning that there is a less clear route to source than many academics enjoy. There are important initiatives underway in Government, such as GOV.UK Registers, which oversees the development of canonical lists of important information critical to the running of the country. Not all data can be treated in this way, and whilst taking snapshots of data may be a blunt instrument, it works when you don’t have control of where it comes from."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#call-to-arms",
    "href": "blog/why-government-needs-sustainable-software-too.html#call-to-arms",
    "title": "Why Government needs sustainable software too",
    "section": "Call to arms",
    "text": "Call to arms\nAlmost all the projects I have referred to in this blog post are open source, and available on GitHub, so follow the links above if you are interested. There’s also two presentations on the topic available as slides (Earl conference 2017 and Government Statistical Service conference 2017) which give more technical details on the projects.\nThis blog is written by Matthew Upson, Data Scientist at Juro and was originally posted on the Software Sustainability website."
  },
  {
    "objectID": "blog/the-joy-of-r.html",
    "href": "blog/the-joy-of-r.html",
    "title": "The joy of R",
    "section": "",
    "text": "Hello. My name is Julian and I am an R addict. I got hooked about 3 years ago when I took on a new role in Public Health England developing a public health data science team. My professional background is as a doctor and Consultant in Public Health and have spent the last 15 years in the health intelligence field so I thought I knew something about data and analysis. I realised I didn’t know anything about data science so I decided to do a course and ended up doing the Coursera data science MOOC from Johns Hopkins because it was health related. For the course, you need to learn R - and so my habit started. (It turned out I knew nothing about data and analysis as well).\nI had done an R course 15 years ago but never used it. Any analysis I did used spreadsheets, SPSS, Mapinfo and host of other tools, and I had never written a single line of code until 3 years ago (apart some very basic SQL). That’s all changed.\nApart from a brief obsession with Tableau a few years ago (which I still love), learning R has for me, been utterly transformational. Now my basic analytical workflow is R + Google (for getting answers when you are stuck) + Git (for sharing and storing code) + Mendeley (reference management software). That’s it.\nI barely open Excel except to look at data structure so I can import data into R; I don’t use GIS; I hardly even open word to write a document - I do that in R (like this blog); and recently the option to output to power point has appeared in R Markdown so I’ve started using that as well.\nOn top of that I have learned a whole heap of analytical and other skills through using R. I feel comfortable getting and analysing data of any size, shape and complexity including text, websites, APIs, very large datasets; and quickly. I can now rapidly produce scientific reports, scrape websites, mine text, automate analysis, build machine learning pipelines, create high quality graphics using the fab ggplot2 and its relatives, have co-authored a package to read Fingertips data (fingertipsR - very proud of this) and am getting my head around regular expressions. I have even managed a couple of Shiny documents. There is nothing I have wanted to do that I can’t do in R; and a huge range of things I didn’t know you could do or had never heard of.\nSo what is it about R that makes it so great? In the last 5 years it has moved from an academic stats package to a professional data science tool. One of the reasons is the development of the tidy data framework [1] and tools to make data wrangling or munging much easier. This is a much overlooked part of the analysts life - all the things you need to do with data before you can analyse it (50 - 70% of the process) has been paid serious attention and made much easier with packages like dplyr and tidyr. And a lot of attention has been made to making coding more logical and syntax more “English”. Another reason is the development of R Studio and R Markdown which give you button press outputs in a range of high quality formats. And there is a focus on reproducibility - the ability for analysis to be repeated exactly which requires combining data, analysis and results in a form others can follow. This is good science and will become much more widespread. You can do this in R and Git.\nMy addiction has infected my team and the analytical community in PHE. We are spreading R rapidly and writing packages to automate routine analysis and reporting. We routinely use Gitlab to share and collaborate on code, and are introducing software development ideas like code review and unit testing. In short we are trying to help analysts (if they want to) become analyst-developers.\nThere are downsides to R of course. There is a (big) learning curve, ICT get twitchy, there is a huge range of packages and any number of ways of doing things, and things often break. But as any addict would say, these are just obstacles to be overcome and there is a lot of support out there.\nR is not the only direction of travel - we do use PowerBI (running R scripts), and we do a bit of development in Python, but one thing is certain - I can’t go back to pre R days.\nSo there’s my confession. I’m a data junkie and an R addict. If you want to see my descent I put stuff on an RPubs page from time to time and I have a Github page. If you want to help me - feel free to get into touch or send me a pull request.\nThanks to Seb Fox at PHE and David Whiting of Medway Council for inspiration and support."
  },
  {
    "objectID": "blog/the-joy-of-r.html#references",
    "href": "blog/the-joy-of-r.html#references",
    "title": "The joy of R",
    "section": "References",
    "text": "References\n1 Wickham H. 2014;59:1–23. doi:10.18637/jss.v059.i10"
  },
  {
    "objectID": "blog/r-studio-shortcuts.html",
    "href": "blog/r-studio-shortcuts.html",
    "title": "R studio shortcuts",
    "section": "",
    "text": "I love keyboard shortcuts. I work in R studio and using keyboard shortcuts has saved me a lot of time. There is a full list of short cuts and I have pulled together my three most used shortcuts.\nCtrl+enter or cmd+enter (Mac) will run the command where the cursor is and then move the cursor down. This is perfect for when you want to run your code line by line.\nCtrl+shift+m or cmd+shift+m (Mac) will insert a pipe (if you don’t already use pipes then you can learn more in R for Data Science.\nCtrl+shift+F10 or cmd+shift+F10 (Mac) will restart your R session. It unloads your packages but leaves the elements in your environment untouched. I use this a lot when I am jumping between scripts to minimise conflicts between packages.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nVestesson, Emma. 2018. “R Studio Shortcuts.” May 21, 2018.\nhttps://nhs-r-community.github.io/nhs-r-community//blog/r-studio-shortcuts.html."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "",
    "text": "Greetings, esteemed members of the NHS-R Community, data enthusiasts, and healthcare aficionados! It is with great excitement that I share my recent journey as a committee member with the NHS-R Community, a vibrant hub dedicated to championing the use of R and data science tools in the UK health and care system."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#the-community",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#the-community",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "The Community",
    "text": "The Community\nEstablished in 2018, the NHS-R Community has rapidly evolved into a dynamic collective of professionals from diverse backgrounds. Our community encompasses members from public sector organizations, including Local Authorities and Civil Service, academia, and the voluntary sector. We are united by a shared passion for advancing healthcare through the transformative power of data. While our core language is R, we are equally enthusiastic about embracing a broad spectrum of data science tools, recognizing their integral role in shaping the future of healthcare analytics."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#steering-the-course-the-committees-impact",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#steering-the-course-the-committees-impact",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Steering the Course: The Committee’s Impact",
    "text": "Steering the Course: The Committee’s Impact\nAs a committee member, my role involves active participation in shaping the community’s approach, focus, and overall strategy. We meet regularly to deliberate on initiatives, conferences, and the overarching direction for the community. The committee serves as a crucial forum for collective decision-making, ensuring that our initiatives align with the evolving needs of the community and contribute to positive change."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#committee-meetings-a-glimpse-into-our-world",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#committee-meetings-a-glimpse-into-our-world",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Committee Meetings: A Glimpse into Our World",
    "text": "Committee Meetings: A Glimpse into Our World\nOur meetings are a lively exchange of ideas, experiences, and visions for the future. Whether we’re brainstorming innovative projects, refining strategies, or planning impactful conferences, each committee member brings a unique perspective to the table. The collaborative spirit is palpable as we navigate the dynamic landscape of data science in healthcare."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#driving-positive-change",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#driving-positive-change",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Driving Positive Change",
    "text": "Driving Positive Change\nBeing part of the committee means playing a pivotal role in fostering collaboration and steering the community toward meaningful outcomes. We celebrate diversity in thought and expertise, recognizing that it is the key to unlocking the full potential of data science in healthcare."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#contributions-welcome",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#contributions-welcome",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Contributions Welcome",
    "text": "Contributions Welcome\nOne of the hallmarks of our community is the open invitation for contributions. Our GitHub repository: NHS-R Community hosts a wealth of resources, including packages and training materials, and we wholeheartedly welcome contributions from community members. It’s an empowering experience to witness the collective impact of our shared knowledge and skills."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#join-the-conversation",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#join-the-conversation",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Join the Conversation",
    "text": "Join the Conversation\nIf you’re passionate about data science, healthcare, and making a positive impact, the NHS-R Community is the place to be. Whether you’re an R enthusiast, a data science wizard, or a healthcare professional with an interest in analytics, there’s a place for you in our diverse and welcoming community.\nAs I continue my journey as a committee member, I’m excited about the endless possibilities that lie ahead. Together, we’re shaping the future of healthcare analytics, one R script at a time.\nStay tuned for more updates from the NHS-R Community, where data meets healthcare, and innovation knows no bounds. See you in the data-driven frontier!"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html",
    "href": "blog/importing-and-exporting-data.html",
    "title": "Importing and exporting Data",
    "section": "",
    "text": "This blog originally appeared in http://gastrodatascience.com\nThere are a large number of file types that are able to store data. R is usually able to import most of them but there are some caveats. Below is a summary of methods I use for data imports using the most common file types.\nIt is worth saying that most datasets will come from excel or csv files. It is unusual to gain direct access to the database and these are the normal export types from most data storage systems."
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-csv-or-text",
    "href": "blog/importing-and-exporting-data.html#import-csv-or-text",
    "title": "Importing and exporting Data",
    "section": "Import csv or text",
    "text": "Import csv or text\n\nread.table(\"mydata.txt\",header=T,stringsAsFActors=F) \n\n#or, and using tab as a delimiter:\n\nread_delim(\"SomeText.txt\", \"\\t\",trim_ws = TRUE)\n\n#Maybe get a csv off the internet:\ntbl &lt;- read.csv(\"http://www.example.com/download/data.csv\")\n\nTo prevent strings being imported as factors, add the parameter stringsAsFActors=F"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-from-excel",
    "href": "blog/importing-and-exporting-data.html#import-from-excel",
    "title": "Importing and exporting Data",
    "section": "Import from excel",
    "text": "Import from excel\n\nlibrary(XLConnect)\nwk = loadWorkbook(\"~Mydata.xlsx\")\ndfw = readWorksheet(wk, sheet=\"Sheet3\",header=TRUE)\n\n#Alternative and super friendly way\n#For excel imports using readxl package:\nlibrary(readxl)\nread_excel(\"~Mydata.xlsx\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-from-database",
    "href": "blog/importing-and-exporting-data.html#import-from-database",
    "title": "Importing and exporting Data",
    "section": "Import from database",
    "text": "Import from database\n\nlibrary(RODBC)\nchannel &lt;- odbcConnect(\"MyDatabase\", believeNRows=FALSE)\n#Get one of the tables\ntbl_PatientDetails&lt;-sqlFetch(channel, \"tblPtDetails\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#export-to-excel",
    "href": "blog/importing-and-exporting-data.html#export-to-excel",
    "title": "Importing and exporting Data",
    "section": "Export to excel",
    "text": "Export to excel\n\nlibrary(XLConnect)\nexc &lt;- loadWorkbook(\"~Mydata.xls\", create = TRUE)\ncreateSheet(exc,'Input')\nsaveWorkbook(exc)\nXLConnect::writeWorksheet(exc,mydata,sheet = \"Input\", startRow = 1, startCol = 2)\n\n#Another way is:\nlibrary(xlsx)\nwrite.xlsx(mydata, \"c:/mydata.xlsx\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#export-to-csv-or-a-tab-delimited-file",
    "href": "blog/importing-and-exporting-data.html#export-to-csv-or-a-tab-delimited-file",
    "title": "Importing and exporting Data",
    "section": "Export to csv or a tab delimited file",
    "text": "Export to csv or a tab delimited file\n\n write.csv(mydata, file=\"filename\", row.names=FALSE)\n write.table(mydata, \"c:/mydata.txt\", sep=\"\\t\")\n\nThere are also many other file types that can be imported and exported but these are the most common so the most practical."
  },
  {
    "objectID": "blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html",
    "href": "blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html",
    "title": "Diverging Dot Plot and Lollipop Charts – Plotting Variance with ggplot2",
    "section": "",
    "text": "Creating the Dot Plot Variance chart\nThe data preparation was used in the previous blog entitled: Diverging Bar Charts – Plotting Variance with ggplot2.\n\n# 20240222 Added for qmd to run\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nRefer to that if you need to know how to create the data prior to this tutorial.\nSetting up the Dot Plot Variance chart\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n  geom_point(stat='identity', aes(col=mpg_type), size=6) +\n  scale_color_manual(name=\"Mileage (deviation)\",\n                     labels = c(\"Above Average\", \"Below Average\"),\n                     values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n  geom_text(color=\"white\", size=2) +\n  labs(title=\"Diverging Dot Plot (ggplot2)\",\n       subtitle=\"Z score showing Normalised mileage\", caption=\"Produced by Gary Hutson\") +\n  ylim(-2.5, 2.5) +\n  coord_flip()\n\nThis is very similar to the previous plot we created in the previous post, however there are a few differences. The main difference is that we use a geom_point() geometry and set the colour of the points based on whether the said point deviates above and below the average. In addition, we use the geom_text() to set the colour of the text in the points to white and specify the size of the text. The final difference is that I have added a Y limit (ylim) range of -2.5 standard deviation to positive 2.5 standard deviations.\nRunning this block of code, along with the data preparation code, will give you a chart that looks as below:\n\n\n\n\n\n\n\n\nCreating the Diverging Lollipop Chart\nThe code below shows how to build the diverging lollipop chart in R and ggplot2:\n\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n  geom_point(stat='identity', aes(col=mpg_type), size=6) +\n  scale_color_manual(name=\"Mileage (deviation)\",\n                     labels = c(\"Above Average\", \"Below Average\"),\n                     values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n  geom_segment(aes(y = 0,\n                   x = CarBrand,\n                   yend = mpg_z_score,\n                   xend = CarBrand),\n               color = \"black\") +\n  geom_text(color=\"white\", size=2) +\n  labs(title=\"Diverging Lollipop Chart\",\n       subtitle=\"Z score for normalised mileage\",\n       caption=\"Produced by Gary Hutson\") +\n  ylim(-2.5, 2.5) + coord_flip() + theme(panel.grid.major = element_blank(), panel.grid.minor =\n  element_blank())\n\nSimilar geometries are used here. What has been added here is the geom_segment() this shows how the line segments need to be added. The starting y is equal to 0 on the Y scale and the starting x is the first car by the car brand. Similarly, the end of the x (xend) is also the CarBrand.\nThe only other difference is to add a theme constraint to the end of the code to turn off the major and minor grid lines, this is achieved by setting the panel.grid.major and panel.grid.minor equal to element_blank().\nThe completed graph and plot is shown below:\n\n\n\n\n\n\n\n\nThere – we now have some lovely looking charts that can be put into a report to report on variance between categorical variables.\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutsons-Hacks.\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “Diverging Dot Plot and Lollipop Charts –\nPlotting Variance with Ggplot2.” May 24, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html."
  },
  {
    "objectID": "blog/count-of-working-days-function.html",
    "href": "blog/count-of-working-days-function.html",
    "title": "Count of working days function",
    "section": "",
    "text": "It’s at this time of year I need to renew my season ticket and I usually get one for the year. Out of interest, I wanted to find out how much the ticket cost per day, taking into account I don’t use it on weekends or my paid holidays. I started my workings out initially in Excel but got as far as typing the formula =WORKDAYS() before I realised it was going to take some working out and perhaps I should give it a go in R as a function…\nChris Beeley had recently shown me functions in R and I was surprised how familiar they were as I’ve seen them on Stack Overflow (usually skimmed over those) and they are similar to functions in SQL which I’ve used (not written) where you feed in parameters.\nWhen I write code I try to work out how each part works and build it up but writing a function requires running the whole thing and then checking the result, the objects that are created in the function do not materialise so are never available to check. Not having objects building up in the environment console is one of the benefits of using a function, that and not repeating scripts which then ALL need updating if something changes."
  },
  {
    "objectID": "blog/count-of-working-days-function.html#bus-ticket-function",
    "href": "blog/count-of-working-days-function.html#bus-ticket-function",
    "title": "Count of working days function",
    "section": "Bus ticket function",
    "text": "Bus ticket function\nThis is the final function which if you run you’ll see just creates a function.\n\n# Week starts on Sunday (1)\nDailyBusFare_function &lt;- function(StartDate, EmployHoliday, Cost, wfh){\n\n  startDate &lt;- dmy(StartDate)\n  endDate &lt;- as.Date(startDate) %m+% months(12)\n\n# Now build a sequence between the dates:\n  myDates &lt;-seq(from = startDate, to = endDate, by = \"days\")\n\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7)-length(holidayLONDON(year = lubridate::year(startDate))) - EmployHoliday - wfh\n\nper_day &lt;- Cost/working_days\n\nprint(per_day)\n}\n\nRunning the function you feed in parameters which don’t create their own objects:\nDailyBusFare_function(\"11/07/2019\",27,612,1)\n[1] 2.707965"
  },
  {
    "objectID": "blog/count-of-working-days-function.html#going-through-each-line",
    "href": "blog/count-of-working-days-function.html#going-through-each-line",
    "title": "Count of working days function",
    "section": "Going through each line:",
    "text": "Going through each line:\nTo make sure each part within the function works it’s best to write it in another script or move the bit betweeen the curly brackets {}.\nFirstly, the startDate is self explanatory but within the function I’ve set the endDate to be dependent upon the startDate and be automatically 1 year later.\nOriginally when I was trying to find the “year after” a date I found some documentation about {lubridate}’s function dyear():\n\n# Next couple of lines needed to run the endDate line!\nlibrary(lubridate)\nstartDate &lt;- dmy(\"11/07/2019\")\n\nendDate &lt;- startDate + dyears(1)\n\nbut this gives an exact year after a given date and doesn’t take into account leap years. I only remember this because 2020 will be a leap year so the end date I got was a day out!\nInstead, Chris Beeley suggested the following:\n\nendDate &lt;- as.Date(startDate) %m+% months(12)\n\nNext, the code builds a sequence of days. I got this idea of building up the days from the blogs on getting days between two dates but it has also come in use when plotting over time in things like SPCs when some of the time periods are not in the dataset but would make sense appearing as 0 count.\n\nlibrary(lubridate)\n\n# To run so that the sequencing works\n# using as.Date() returns incorrect date formats 0011-07-20 so use dmy from\n# lubridate to transform the date\n\n  startDate &lt;- dmy(\"11/07/2019\")\n  endDate &lt;- as.Date(startDate) %m+% months(12)\n\n# Now build a sequence between the dates:\n  myDates &lt;- seq(from = startDate, to = endDate, by = \"days\")\n\nAll of these return values so trying to open them from the Global Environment won’t do anything. It is possible view the first parts of the values but also typing:\n\n# compactly displays the structure of object, including the format (date in this case)\nstr(myDates)\n\n Date[1:367], format: \"2019-07-11\" \"2019-07-12\" \"2019-07-13\" \"2019-07-14\" \"2019-07-15\" ...\n\n# gives a summary of the structure\nsummary(myDates)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2019-07-11\" \"2019-10-10\" \"2020-01-10\" \"2020-01-10\" \"2020-04-10\" \"2020-07-11\" \n\n\nTo find out what a function does type ?str or?summary in the console. The help file will then appear in the bottom right Help screen.\nNext I worked out working_days. I got the idea from a blog which said to use length and which:\n\n  working_days &lt;- length(which((wday(myDates)&gt;1&wday(myDates)&lt;7)))\n\nNote that the value appears as 262L which is a count of a logical vector. Typing ?logical into the Console gives this in the Help:\nLogical vectors are coerced to integer vectors in contexts where a numerical value is required, with TRUE being mapped to 1L, FALSE to 0L and NA to NA_integer._\nI was familiar with length(), it does a count essentially of factors or vectors (type ?length into the Console for information) but which() wasn’t something I knew about. Chris Beeley explained what which does with the following example:\n\n# Generate a list of random logical values\na &lt;- sample(c(TRUE, FALSE), 10, replace = TRUE)\n\n# Look at list\na\n\n [1] FALSE  TRUE FALSE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\n# using which against the list gives the number in the list where the logic = TRUE\nwhich(a)\n\n[1] 2 6 9\n\n# counts how many logical arguments in the list (should be 10)\nlength(a)\n\n[1] 10\n\n# counts the number of TRUE logical arguments\nlength(which(a))\n\n[1] 3\n\n\nThen Chris Beeley suggested just using sum instead of length(which()) which counts a logical vector:\n\nsum(a)\n\n[1] 3\n\n\nIt seems this has been discussed on Stack Overflow before: https://stackoverflow.com/questions/2190756/how-to-count-true-values-in-a-logical-vector\nIt’s worthy of note that using sum will also count NAs so the example on Stack overflow suggest adding:\n\nsum(a, na.rm = TRUE)\n\n[1] 3\n\n\nThis won’t affect the objects created in this blog as there were no NAs in them but it’s just something that could cause a problem if used in a different context.\n\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7)\n\n# Just to check adding na.rm = TRUE gives the same result\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7, na.rm = TRUE)\n\nI then wanted to take into account bank/public holidays as I wouldn’t use the ticket on those days so I used the function holidayLONDON( from the package {timeDate}:\n\nlength(holidayLONDON(year = lubridate::year(startDate)))\n\n[1] 8\n\n\nI used lubridate::year because the package {timeDate} uses a parameter called year so the code would read year = year(startDate) which is confusing to me let alone the function!\nAgain, I counted the vectors using length(). This is a crude way of getting bank/public holidays as it is looking at a calendar year and not a period (July to July in this case).\nI did look at a package called {bizdays} but whilst that seemed to be good for building a calendar I couldn’t work out how to make it work so I just stuck with the {timeDate} package. I think as I get more confident in R it might be something I could look into the actual code for because all packages are open source and available to view through CRAN or GitHub.\nBack to the function…\nI then added - EmployHoliday so I could reduce the days by my paid holidays and - wfh to take into account days I’ve worked from home and therefore not travelled into work.\nThe next part of the code takes the entered Cost and divides by the Working_days, printing the output to the screen:\nper_day &lt;- Cost/working_days\nprint(per_day)\nAnd so the answer to the cost per day is printed in the Console:\n\nDailyBusFare_function(\"11/07/2019\",27,612,1)\n\n[1] 2.707965"
  },
  {
    "objectID": "blog/count-of-working-days-function.html#a-conclusion-of-sorts",
    "href": "blog/count-of-working-days-function.html#a-conclusion-of-sorts",
    "title": "Count of working days function",
    "section": "A conclusion… of sorts",
    "text": "A conclusion… of sorts\nWhilst this isn’t really related to the NHS it’s been useful to go through the process of producing a function to solve a problem and then to explain it, line by line, for the benefit of others.\nI’d recommend doing this to further your knowledge of R at whatever level you are and particularly if you are just learning or consider yourself a novice as sometimes blogs don’t always detail the reasons why things were done (or why they were not done because it all went wrong!)."
  },
  {
    "objectID": "blog/aiming-for-a-wrangle-free-or-reduced-world.html",
    "href": "blog/aiming-for-a-wrangle-free-or-reduced-world.html",
    "title": "Aiming for a wrangle-free (or reduced) world",
    "section": "",
    "text": "I work as a Data Scientist at Public Health England. I am part of a small team that have a role in trying to modernise how we “do” data. I have been an analyst in one way or another for most of my working life. In my role as an analyst, as with most analysts, my biggest focus was on the accuracy of my outputs, but I’ve always got frustrated very quickly with repetitive tasks, which are all too common for analytical roles. In fact, I remember when these frustrations first began. It was during my Masters when, as part of my dissertation, my supervisor asked me to draw a map using ArcMap software. I hadn’t had any previous experience of the software before this moment. Instead of asking for help, which I should have done, I went away and tried to import the files I was given. What I didn’t know was that I didn’t have access to the correct licence to import the files I was using. I thought I was doing something wrong. I did manage to open the files in Excel though and I could see they contained coordinates. I could see that if I interpolated the coordinates of the points, I could create a file that I would be able to upload into ArcMap and it would look like what I was aiming for - the problem was that there were millions of coordinates and they were split over multiple files! This is the moment in my life where I discovered “Record Macro”. I managed to record a few instances of what I wanted to do, and then manipulated the recorded code to repeat the task for everything. I felt very smug going to my supervisor the following week and handing him a map with pink, yellow and red blobs illustrating height contours of a water basin. To say he wasn’t impressed would be an understatement. He pulled up a map on his screen to show me what it should look like. His screen essentially showed what Google Satellite now provides us. My smugness quickly turned to self-doubt.\nIn many ways this example is typical of my experience as an analyst. I have received data in many ways, from people or through systems and databases, but to manipulate (or wrangle, as it is commonly called now) those data to what my manager wanted to see would take a number of days. The data may contain a table for each month, where each month was a different tab in a spreadsheet. If I was lucky each tab would be formatted identically, but more often than not there would be different numbers of columns or rows (sigh). Sometimes, one month might (helpfully) have an extra blank row at the top or maybe some merged cells. I would sit there bringing all those data into one place thinking of myself as part of a sandwich assembly line, picking up all the raw ingredients, assembling them in the right way for somebody else to enjoy. I really wanted to enjoy it! Surely this can be done better and faster. How did analysts on detective shows instantly get the information the senior detective required at the tap of a few keys?\nI first used R 3 years ago. R has completely changed the way that I see data. It has formalised all my previous frustrations. It has words for things that I have thought but could never explain. It encourages data to be “done” properly. Before R I had never heard of an analytical pipeline (I realise this isn’t exclusively an R thing). Everything I had done was about getting data, spending time wrangling it, analysing it and finally presenting it to someone else (for their enjoyment). R gave me R Markdown. Here I could do all of these steps in one script. There was no need for me to write a Word document to sit alongside my Excel workbook to explain where I got the data from, what tabs 2 to 7 do, and why I’ve hard-coded 34.84552 in cell D4. There was no need for me to write step by step instructions for how to draw a bar chart on one axis and a line chart on another within the same graph. The ability to become transparent in my workings was ideal for my lazy nature as the description is written in the code. Not only was my working transparent, it was also completely reproducible. If someone else had access to the same data as me, they could run my script and it would produce the same outputs.\nMy biggest revelation though was being introduced to tidy data. This was my game changer. I had often heard the quote that analysts spent 80% of their time manipulating data and 20% analysing it. That chimed with me. As is written in the paper referenced above:\n“tidy datasets are all alike but every messy dataset is messy in its own way”\nAs the paper describes, tidy data has three features:\nEach variable forms a column. Each observation forms a row. Each type of observational unit forms a table.\nIt is hard to describe or appreciate this really until you think about the dataset you’re working with. Are you really struggling to get it into the format you need to make it easy to work with? The example the paper provides can be seen below:\n\nlibrary(gt)\nlibrary(tidyr)\nlibrary(dplyr)\n\nuntidy_data &lt;- tibble::tribble(\n  ~person, ~treatmentA, ~treatmentB,\n  \"John Smith\",          NA,          2L,\n  \"Jane Doe\",         16L,         11L,\n  \"Mary Johnson\",          3L,          1L\n)\n\n\ngt(untidy_data)\n\n\n\n\n\nperson\ntreatmentA\ntreatmentB\n\n\n\nJohn Smith\nNA\n2\n\n\nJane Doe\n16\n11\n\n\nMary Johnson\n3\n1\n\n\n\n\n\ntidy_data &lt;- untidy_data |&gt; \n  pivot_longer(cols = c(\"treatmentA\", \"treatmentB\"),\n               names_to = \"treatment\",\n               values_to = \"result\") |&gt; \n  arrange(treatment)\n\ngt(tidy_data)\n\n\n\n\n\nperson\ntreatment\nresult\n\n\n\nJohn Smith\ntreatmentA\nNA\n\n\nJane Doe\ntreatmentA\n16\n\n\nMary Johnson\ntreatmentA\n3\n\n\nJohn Smith\ntreatmentB\n2\n\n\nJane Doe\ntreatmentB\n11\n\n\nMary Johnson\ntreatmentB\n1\n\n\n\n\n\n\nFigure 1, the table on above illustrates an untidily formatted table. The table below presents the same data but in tidy format\nAs an analyst, working with tidy data is a rare pleasure. Analytical tasks become seamless as it allows you to use the tidyverse package. Summarising data for groups within your dataset or creating models based on subgroups are an additional one or two lines of understandable code rather than 20 to 30.\nI look forward to a world where tidy data becomes the norm. In this world analysts will be spending 80% of their time analysing the data. We will be using data in a timely fashion, and it will be informing decision making even more than it currently does. We will be combining different datasets to create fuller pictures for the decisions we are informing. We will be learning new techniques for analysing the data rather than new techniques for manipulating them. Wrangling will become a thing of the past and most importantly, we will get to enjoy the sandwich that we’ve made.\n\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nFox, Sebastian. 2018. “Aiming for a Wrangle-Free (or Reduced)\nWorld.” March 23, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/aiming-for-a-wrangle-free-or-reduced-world.html."
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "NHS-R Community Quarto website",
    "section": "",
    "text": "The NHS-R Community started in 2018 to promote the use of R in the NHS and the community has grown rapidly ever since. It is a community that is broader than the NHS as members come from public sector organisations across the UK, including Local Authorities and Civil Service, as well as academics and voluntary sector people who have an interest in healthcare. Whilst R is a core language for support by the community, there is always support for data science tools more generally, particularly where they cross over with R."
  },
  {
    "objectID": "about/index.html#more-information",
    "href": "about/index.html#more-information",
    "title": "NHS-R Community Quarto website",
    "section": "More information",
    "text": "More information\nMore can be found in individual books including:\n\n\nNHS-R Way\n\n\n\nLink →\n\n\nStatement on Tools\n\n\n\nLink →\n\n\nOpen Analytics Resources\n\n\n\nLink →\n\n\nTraining courses\n\n\n\nLink →"
  },
  {
    "objectID": "about/index.html#special-thanks",
    "href": "about/index.html#special-thanks",
    "title": "NHS-R Community Quarto website",
    "section": "Special thanks",
    "text": "Special thanks\nTo the creators of Quarto and to those who publish their code to share their knowledge particularly the website Real World Data Science and Silvia Canelón’s personal site.\nBoth were instrumental in building this site as they share the code from their website but have also contributed to the NHS-R Community Conferences:\n\nBrian Tarran opened the NHS-R Community Conference in 2023 with his talk Forging community links: NHS-R, the Royal Statistical Society and Real World Data Science.\nSilvia Canelón ran a two-part Xaringan (including CSS) workshop at the 2020 NHS-R Conference - Part 1 and Part 2. And as part of this workshop Silvia kindly created {NHSRthemes} which contains the code for the NHS Branding colours used in this site."
  },
  {
    "objectID": "accessibility.html",
    "href": "accessibility.html",
    "title": "Accessibility commitment",
    "section": "",
    "text": "Return to home page or about page.\nInspired from Silvia Canelon’s committment to accessiblity NHS-R Community welcomes feedback on the accessibility of this site and any links to its materials including code and training materials.\nPlease let us know if you encounter any accessibility barriers by emailing nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "accessibility.html#website-practices",
    "href": "accessibility.html#website-practices",
    "title": "Accessibility commitment",
    "section": "Website practices",
    "text": "Website practices\nThis site has been designed with the following features in mind:\n\nThe colour palette is from NHS branding.\nFonts are set to Arial which is an NHS font.\nAlternative text for all informative images.\n\nWe are aware there is much more to inclusive and accessible design and we welcome feedback on how to better implement accessibility practices. Any learning will be used to improve the site but will also be shared with the community more widely through book resources like our Open Analytics page where we have started to collect links that support greater Accessibility."
  },
  {
    "objectID": "blog/building-a-quarto-website.html",
    "href": "blog/building-a-quarto-website.html",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "A few years ago Mohammed A Mohammed, who was integral to getting NHS-R Community set up, suggested I might like to get involved by creating a website for NHS-R Community. At that time I had no idea where to start so my involvement stalled and a website was built, with help from a 3rd party, in WordPress. WordPress is a great website tool that can support very complicated sites like NHS-R Community’s, as we have both a static site and an Events Management system which means that we don’t need to rely (or pay!) for Eventbrite. We’ve used the system for sign up to hundreds of webinars and workshops, not to mention several hundred people attending conferences over the years.\n\n\nIn the time it’s taken from that initial suggestion about building the website to now, where I help to manage the site, I’ve had the opportunity to build websites for myself using {distill} and Hugo Apéro as well as the release of Quarto which I was incredibly keen to try out for website creation.\nI’d used Quarto for slides and reports and been hugely impressed with. Getting started with Quarto is relatively easy if you’ve used R for a while, but like with anything, you have to start off at the basics and quickly want more complex functionality as you get going. This is even more of an issue when you are moving from one thing to another and it’s more of a translation than a build-from-scratch project.\nLuckilyfor me, many people had already jumped into Quarto websites and, as is usual for R users, their code was available online to delve into. I particularly was keen to follow the work of the Brian Tarran for the Royal Statistical Society’s Real World Data Science website because, at first glance, this doesn’t give itself away as being built on Quarto. And I also really like Silvia Canelón’s personal blog site for its beauty and functionality. Silvia has particularly worked hard on accessibility of her website and has written a lot of CSS code which I’m learning through what she has shared.\nAccessibility is something that the WordPress site needed considerable work on, particularly when checked with a website called the Web Accessbility Evaluation Tool which Silvia recommends on her blog site. The new Quarto site will need this too of course but now that the code is out in the open, anyone can contribute. That can be either making changes to the code but also highlighting problems as issues that can then be open to everyone to see.\n\n\n\n\n\nI’ll mention accessibility again as that’s hugely important and I like the fact that the people working on Quarto also think a lot about accessibility. Adding alt text has never been easier and any code (in the YAML) that refers to an image nearly always has functionality to add image alt text too.\n\n\n\nThe search function for Quarto books is really impressive and the same functionality can be found in Quarto websites. There might be a plug in within WordPress to make searching through blogs for particular words but with Quarto it’s not something that needs adding - it’s there from the start. I now no longer have to think about tags or categories for blogs to make them easier to find.\n\n\n\nThis was a bit of a niggle with WordPress in that it’s not really a website for coders. Having the show code in the format that it appears to the person coding is a nice to have but Quarto goes an extra step by letting you run the code. It’s mean that the charts that are talked about in some blogs are produced as the code is run, not as pictures.\nUsually a blogger will choose a nice image to complement their writing and Quarto gives that functionality but I had a lovely surprise when charts that are created by the blog code became that thumbnail image automatically. This saves a lot of time and, no doubt, a lot of server space for static pictures to be stored. Although there is no cost to storing these on GitHub there is the environmental impact of the servers that they have to maintain to store this data.\n\n\n\nHaving a website that can publish R or Python code is an absolute joy. I used to write blogs in RMarkdown, render them to html and then copy the output to WordPress and in the copying and pasting often lose links or formatting.\nBeing open to contribution of course also means that the repository will be available to people to do pull requests. I’ve set the repository so that it’s not necessary to Render the website to contribute so any additions or changes just need to be made to the qmd files.\n\n\n\nNow this point could have come from any change in website but I’ve taken the opportunity to go through each blog and transfer it to Quarto. There are no doubt quicker ways to do this as I can export from WordPress and then work on code to to get it to a format that can be published, but I started with moving a couple of blogs to get started and I’m hooked.\nStarting in 2018 when the blogs first started I didn’t know R at all. I started learning with NHS-R Community so these early blogs meant very little to me. As I’ve gone through each blog, formatting them and checking the R code runs (I found a couple of small mistakes this way!) and the links still work I’ve been more like an archivist than a coder. My favourite discover, and shock, was that I found that RAP (Reproducible Analytical Pipelines) was shared with the NHS-R Community in 2018 although I only really heard about it from 2020. It was very much like the moment I experienced when I went through my old Geography school books and found I’d been taught in those lessons about Index of Multiple Deprivation but I had absolutely no recollection of it at all. It hadn’t been the right time for me to really listen to it even though it’s become something that I work with a lot and want to ensure more people know about and have helped with the work on an NHS-R Community Quarto book for Health Inequalities.\n\n\n\n\nAs part of thinking about the website I’ve also had an opportunity to write out how to contribute to GitHub"
  },
  {
    "objectID": "blog/building-a-quarto-website.html#moving-to-purely-coded-websites",
    "href": "blog/building-a-quarto-website.html#moving-to-purely-coded-websites",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "In the time it’s taken from that initial suggestion about building the website to now, where I help to manage the site, I’ve had the opportunity to build websites for myself using {distill} and Hugo Apéro as well as the release of Quarto which I was incredibly keen to try out for website creation.\nI’d used Quarto for slides and reports and been hugely impressed with. Getting started with Quarto is relatively easy if you’ve used R for a while, but like with anything, you have to start off at the basics and quickly want more complex functionality as you get going. This is even more of an issue when you are moving from one thing to another and it’s more of a translation than a build-from-scratch project.\nLuckilyfor me, many people had already jumped into Quarto websites and, as is usual for R users, their code was available online to delve into. I particularly was keen to follow the work of the Brian Tarran for the Royal Statistical Society’s Real World Data Science website because, at first glance, this doesn’t give itself away as being built on Quarto. And I also really like Silvia Canelón’s personal blog site for its beauty and functionality. Silvia has particularly worked hard on accessibility of her website and has written a lot of CSS code which I’m learning through what she has shared.\nAccessibility is something that the WordPress site needed considerable work on, particularly when checked with a website called the Web Accessbility Evaluation Tool which Silvia recommends on her blog site. The new Quarto site will need this too of course but now that the code is out in the open, anyone can contribute. That can be either making changes to the code but also highlighting problems as issues that can then be open to everyone to see."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#benefits-to-quarto",
    "href": "blog/building-a-quarto-website.html#benefits-to-quarto",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "I’ll mention accessibility again as that’s hugely important and I like the fact that the people working on Quarto also think a lot about accessibility. Adding alt text has never been easier and any code (in the YAML) that refers to an image nearly always has functionality to add image alt text too.\n\n\n\nThe search function for Quarto books is really impressive and the same functionality can be found in Quarto websites. There might be a plug in within WordPress to make searching through blogs for particular words but with Quarto it’s not something that needs adding - it’s there from the start. I now no longer have to think about tags or categories for blogs to make them easier to find.\n\n\n\nThis was a bit of a niggle with WordPress in that it’s not really a website for coders. Having the show code in the format that it appears to the person coding is a nice to have but Quarto goes an extra step by letting you run the code. It’s mean that the charts that are talked about in some blogs are produced as the code is run, not as pictures.\nUsually a blogger will choose a nice image to complement their writing and Quarto gives that functionality but I had a lovely surprise when charts that are created by the blog code became that thumbnail image automatically. This saves a lot of time and, no doubt, a lot of server space for static pictures to be stored. Although there is no cost to storing these on GitHub there is the environmental impact of the servers that they have to maintain to store this data.\n\n\n\nHaving a website that can publish R or Python code is an absolute joy. I used to write blogs in RMarkdown, render them to html and then copy the output to WordPress and in the copying and pasting often lose links or formatting.\nBeing open to contribution of course also means that the repository will be available to people to do pull requests. I’ve set the repository so that it’s not necessary to Render the website to contribute so any additions or changes just need to be made to the qmd files.\n\n\n\nNow this point could have come from any change in website but I’ve taken the opportunity to go through each blog and transfer it to Quarto. There are no doubt quicker ways to do this as I can export from WordPress and then work on code to to get it to a format that can be published, but I started with moving a couple of blogs to get started and I’m hooked.\nStarting in 2018 when the blogs first started I didn’t know R at all. I started learning with NHS-R Community so these early blogs meant very little to me. As I’ve gone through each blog, formatting them and checking the R code runs (I found a couple of small mistakes this way!) and the links still work I’ve been more like an archivist than a coder. My favourite discover, and shock, was that I found that RAP (Reproducible Analytical Pipelines) was shared with the NHS-R Community in 2018 although I only really heard about it from 2020. It was very much like the moment I experienced when I went through my old Geography school books and found I’d been taught in those lessons about Index of Multiple Deprivation but I had absolutely no recollection of it at all. It hadn’t been the right time for me to really listen to it even though it’s become something that I work with a lot and want to ensure more people know about and have helped with the work on an NHS-R Community Quarto book for Health Inequalities."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#contributions-are-welcome",
    "href": "blog/building-a-quarto-website.html#contributions-are-welcome",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "As part of thinking about the website I’ve also had an opportunity to write out how to contribute to GitHub"
  },
  {
    "objectID": "blog/diverging-bar-charts-plotting-variance-with-ggplot2.html",
    "href": "blog/diverging-bar-charts-plotting-variance-with-ggplot2.html",
    "title": "Diverging Bar Charts – Plotting Variance with ggplot2",
    "section": "",
    "text": "Diverging Bar Charts\nThe aim here is to create a diverging bar chart that shows variance above and below an average line. In this example I will use Z Scores to calculate the variance, in terms of standard deviations, as a diverging bar. This example will use the mtcars stock dataset, as most of the data I deal with day-to-day is patient sensitive.\nData preparation\nThe code below sets up the plotting libraries, attaches the data and sets a theme:\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nNext, we will change some of the columns in the data frame and perform some calculations on the data frame:\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nAs commented, this line uses the existing mtcars data frame and uses the dollar sign notation i.e. add a new column, or refer to a column, to create a column name called CarBrand. Then we assign the car brand (&lt;-) with the rownames from the data frame. This is obviously predicated on there being some row names in the data frame, otherwise you would have to name the rows using rownames().\nAdding a Z Score calculation\nA Z score is a calculation which uses the x observation subtracts said observation from the mean and divides by the standard deviation. The link shows the mathematics behind this, for anyone who is interested.\nThe following code shows how we would implement this score:\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nThe statistics behind the calculation have already been explained, but I have also used the round() function to round the results down to 2 digits.\nCreating a cut off (above/below mean)\nThe next step is to use conditional algebra (first advocated by one of my heroes George Boole) to check whether the Z score I have just created is greater or less than 0:\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nThe ifelse() block looks at whether the Z Score is below 0, if so tag as below average, otherwise show this as above.\nThe next two steps are to convert the Car Brand into a unique factor and to sort by the Z Score calculations:\nNow, I have everything I need to start to compute the plot. Great stuff, so let’s get plotting.\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nCreating the plot\nFirst, I will start with creating the base plot:\n\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score))\n\nHere, I pass in the mtcars data frame and set the aesthetics layer (aes) of the x axis to the brand of car (CarBrand). The y axis is the Z score I created for miles per gallon (mpg) and the label is also set to the z score.\nNext, I will add on the geom_bar geometry:\n\n    ggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n    geom_bar(stat='identity', aes(fill=mpg_type), width=.5) +\n\nThis indicates that I need to use the mpg_z_score field by forcing the stat=\"identity\" option. If this was not added, then it would simply count the number of times the Car Brand appears as a frequency count (not what I want!). Then, I stipulate the fill type of the bar to be equal to whether the value deviates above and below 0 – remember we created a field in the data preparation stage to store whether this deviates below and above 0 and called it mpg_type. The last parameter is the width parameter to indicate the width of the bars.\nNext:\n\n  ggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n   geom_bar(stat='identity', aes(fill=mpg_type), width=.5) +\n   scale_fill_manual(name=\"Mileage (deviation)\",\n                    labels = c(\"Above Average\", \"Below Average\"),\n                    values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n\nI use the scale_fill_manual() ggplot option to add the name to the legend, specify the label names using the combine function and stipulate that the values that are above average need to be hex coded by the value and the below values to a different code. I have weirdly chosen blue and green as an alternative to red, as I know we have accessibility there. We are nearly there, the final step is:\n\n ggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n   geom_bar(stat='identity', aes(fill=mpg_type), width=.5) +\n   scale_fill_manual(name=\"Mileage (deviation)\",\n                    labels = c(\"Above Average\", \"Below Average\"),\n                    values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n   labs(subtitle=\"Z score (normalised) mileage for mtcars'\",\n         title= \"Diverging Bar Plot (ggplot2)\", caption=\"Produced by Gary Hutson\") +\n   coord_flip()\n\nHere, I have added the labs layer on to the plot. This is a way to label your plots to show more meaningful values than would be included by default. So, within labs I use subtitle, title and caption to add labels to the chart. Finally, the important command is to add the coord_flip() command to the chart – without this you would have vertical bars instead of horizontal. I think this type of chart looks better horizontal, thus the reason for the inclusion of the command.\nThe final chart, looks as illustrated hereunder:\n\n\n\n\n\n\n\n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutson-Hacks.\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “Diverging Bar Charts – Plotting Variance with\nGgplot2.” May 24, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/diverging-bar-charts-plotting-variance-with-ggplot2.html."
  },
  {
    "objectID": "blog/histogram-with-auto-binning-in-ggplot2.html",
    "href": "blog/histogram-with-auto-binning-in-ggplot2.html",
    "title": "Histogram with auto binning in ggplot2",
    "section": "",
    "text": "# 20240224 Added for qmd to run\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nHistograms (with auto binning)\nAgain, we will use the mtcars dataset and use the fields in that to produce the chart, as we are doing this there is nothing to do on the data preparation side. That leaves us to have fun with the plot.\nBuilding the Histogram with auto binning\nI set up the plot, as per below:\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\nI import the ggplot2 library and set my chart theme to a classic theme. The process next is to create the histogram plot and feed in the relevant data:\n\nplot &lt;- ggplot(mpg, aes(displ)) + scale_fill_brewer(palette = \"Blues\")\n\nI create a plot placeholder in memory so I can reuse this plot again and again in memory. This sets the aes layer equal to the displacement metric in the mtcars data frame. I then use the scale_fill_brewer command and select the palette to the Blues palette. A list of palettes can be found on the R Graph Gallery.\n\nThe next section uses the geom_histogram() geometry to force this to be a histogram:\n\nplot + geom_histogram(aes(fill=class),\n                      binwidth = .1,\n                      col=\"black\",\n                      size=.1) +\n  labs(title=\"Histogram with Auto Binning\",\n       caption=\"Produced by Gary Hutson\") + xlab(\"Displacement\")\n\nThe histogram uses the class of vehicle as the histogram fill, the binwidth is the width of the bins required, the colour is equal to black and the size is stipulated here. All that I then do is add the data labels to it and you have a lovely looking histogram built. This can be applied to any dataset. The output is as below:\n\n\n\n\n\n\n\n\nSpecifying binning values\nThe script can be simply changed in the histogram layer by adding the bins parameter:\n\nplot + geom_histogram(aes(fill=class),\n                   bins=5,\n                   col=\"black\",\n                   size=.1) +\n  labs(title=\"Histogram with Auto Binning\",\n       caption=\"Produced by Gary Hutson\") + xlab(\"Displacement\")\n\n\n\n\n\n\n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutsons-Hacks.\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nHutson, Gary. 2018. “Histogram with Auto Binning in\nGgplot2.” May 24, 2018. https://nhs-r-community.github.io/nhs-r-community//blog/histogram-with-auto-binning-in-ggplot2.html."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Building a Quarto website for NHS-R Community\n\n\n“Delving into the blog history of NHS-R Community as the site moves into the future with Quarto” \n\n\n\nPersonal Story\n\n\n\n\n\n\nFeb 24, 2024\n\n\nZoë Turner\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Data-Driven Frontier: Insights from an NHS-R Committee Member\n\n\n‘It is with great excitement that I share my recent journey as a committee member with the NHS-R Community, a vibrant hub dedicated to championing the use of R and data science tools in the UK health and care system.’ \n\n\n\nCommittee\n\n\n\n\n\n\nFeb 6, 2024\n\n\nPrajwal Khairnar\n\n\n\n\n\n\n\n\n\n\n\n\nCount of working days function\n\n\nCreating a function to work out the number of working days a seasonal bus ticket can be used. \n\n\n\nFunction\n\n\n\n\n\n\nJul 16, 2019\n\n\nZoë Turner\n\n\n\n\n\n\n\n\n\n\n\n\nSimpler SQL with dplyr\n\n\n“Comparing dplyr with SQL nested queries” \n\n\n\nR tips\n\n\nSQL\n\n\ndplyr\n\n\nPatient Flow\n\n\n\n\n\n\nJun 7, 2018\n\n\nJohn MacKintosh\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Bar Charts – Plotting Variance with ggplot2\n\n\n“The aim here is to create a diverging bar chart that shows variance above and below an average line.” \n\n\n\nR tips\n\n\nggplot2\n\n\nBase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Dot Plot and Lollipop Charts – Plotting Variance with ggplot2\n\n\n“The aim here is to create a diverging bar chart that shows variance above and below an average line.” \n\n\n\nR tips\n\n\nggplot2\n\n\nBase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram with auto binning in ggplot2\n\n\n“Building the Histogram with auto binning” \n\n\n\nR tips\n\n\nggplot2\n\n\nBase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Government needs sustainable software too\n\n\n“In this blog post I want to talk about why Government needs sustainable software, the work being done to deliver it, and the lessons we learnt after the first year.” \n\n\n\nSoftware\n\n\n\n\n\n\nMay 24, 2018\n\n\nMatthew Upson\n\n\n\n\n\n\n\n\n\n\n\n\nR studio shortcuts\n\n\n“There is a full list of short cuts here and I have pulled together my three most used shortcuts.” \n\n\n\nR tips\n\n\n\n\n\n\nMay 21, 2018\n\n\nEmma Vestesson\n\n\n\n\n\n\n\n\n\n\n\n\nThe :: operator\n\n\n“Sometimes two packages will have a function with the same name but they will do different things.” \n\n\n\nR tips\n\n\n\n\n\n\nMay 21, 2018\n\n\nEmma Vestesson\n\n\n\n\n\n\n\n\n\n\n\n\nImporting and exporting Data\n\n\n‘There are a large number of file types that are able to store data. R is usually able to import most of them but there are some caveats.’ \n\n\n\nR tips\n\n\n\n\n\n\nMay 15, 2018\n\n\nS Zeki\n\n\n\n\n\n\n\n\n\n\n\n\nThe joy of R\n\n\n‘Hello. My name is Julian and I am an R addict. I got hooked about 3 years ago when I took on a new role in Public Health England developing a public health data science team.’ \n\n\n\nPersonal Story\n\n\n\n\n\n\nApr 9, 2018\n\n\nJulian Flowers\n\n\n\n\n\n\n\n\n\n\n\n\nAiming for a wrangle-free (or reduced) world\n\n\n‘I work as a Data Scientist at Public Health England. I am part of a small team that have a role in trying to modernise how we “do” data.’ \n\n\n\nPersonal Story\n\n\n\n\n\n\nMar 23, 2018\n\n\nSebastian Fox\n\n\n\n\n\n\n\n\n\n\n\n\nNHS meets R\n\n\n‘Hello and welcome to our nascent NHS-R Community; a community dedicated to promoting the learning, application and utilisation of R in the National Health Service (NHS) in the United Kingdom.’ \n\n\n\nNHS-R\n\n\n\n\n\n\nMar 19, 2018\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n Back to topReuseCC0"
  },
  {
    "objectID": "blog/nhs-meets-r.html",
    "href": "blog/nhs-meets-r.html",
    "title": "NHS meets R",
    "section": "",
    "text": "Hello and welcome to our nascent NHS-R Community; a community dedicated to promoting the learning, application and utilisation of R in the National Health Service (NHS) in the United Kingdom. Like any community, NHS-R relies on the vibrancy of its participants to be relevant and productive – and fun.\n\n\nThe NHS is one of the best healthcare systems in the world[1]. It was launched in 1948 with the guiding principle of being free at the point of delivery – a kind of crowd funded open-source freeware equivalent of healthcare. More than half (52%) of the public say the NHS is what makes them most proud to be British, placing it above the armed forces (47%), the Royal Family (33%), Team GB (26%) and the BBC (22%)1.\nThe NHS in England deals with about 1 million people every 36 hours[2] and is continually generating vast amounts of data about the health and care of people[3]. This data is one of the most precious, yet under tapped, resources in the NHS. “Data is the new oil of the digital economy”[4] and drilling and mining NHS data could improve the NHS[5]. But mining these mountains of data is a colossal task.\nThis is where R comes in. R was conceived in 1992[6] as a free open-source statistical programming environment, which is now widely used in industry[7] (Google, Microsoft, Airbnb, New York Times, Lloyds of London, etc) and academia, and is now ranked amongst the most popular (sixth as of 2017) programming languages[8]. But its use in the NHS is almost non-existent. Whilst there are several reasons for this, the absence of R at scale in the NHS, means that the NHS is unable to take advantage of the huge benefits of R, including cutting-edge visualisation and statistical tools, and a worldwide R community, which freely shares learning and resources.\nSo, our aim is to promote the use of R in the NHS, and help to make the NHS better.\nTo kick-start the NHS-R Community, we have developed a website [www.nhsrcommunity.com] and are offering four free workshops (3 days each, repeated in Yorkshire and Wales). Workshop (1) will be an introduction to R for healthcare analysts. Subsequent workshops will focus on the following problems:- (2) understanding and reporting hospital mortality statistics, (3) predicting urgent demand for hospital care and (4) evaluation of interventions using matched retrospective controls. Each workshop will be led by experts in both the problem domain and R, and captured electronically for wider dissemination. Registration for the workshops is now open*.\nHowever, anyone can contribute to the NHS-R Community, so why not share your experience (novice, beginner, or otherwise) of using R in the healthcare setting? Write a blog, share R tips, do an on-line R tutorial, suggest topics for ongoing development and support, and share ideas on how to embed R into the NHS.\nFrom the NHS-R Team [Posted: 19 March 2018]\nThe NHS-R Community project is funded by The Health Foundation.\n*NB: To be eligible for the workshops you must have a working NHS email address. Places are limited.\n\nhttps://www.newscientist.com/article/2140698-us-ranked-worst-healthcare-system-while-the-nhs-is-the-best/ Accessed 20240221\nhttps://www.nhs.uk/NHSEngland/thenhs/about/Pages/overview.aspx Accessed 20240221\nhttps://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Accessed 20240221\nhttp://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html Accessed 20240221\nhttp://makemeanalyst.com/companies-using-r/ Accessed 20240221\nhttps://spectrum.ieee.org/computing/software/the-2017-top-programming-languages Accessed 20240221"
  },
  {
    "objectID": "blog/nhs-meets-r.html#so-why-get-involved",
    "href": "blog/nhs-meets-r.html#so-why-get-involved",
    "title": "NHS meets R",
    "section": "",
    "text": "The NHS is one of the best healthcare systems in the world[1]. It was launched in 1948 with the guiding principle of being free at the point of delivery – a kind of crowd funded open-source freeware equivalent of healthcare. More than half (52%) of the public say the NHS is what makes them most proud to be British, placing it above the armed forces (47%), the Royal Family (33%), Team GB (26%) and the BBC (22%)1.\nThe NHS in England deals with about 1 million people every 36 hours[2] and is continually generating vast amounts of data about the health and care of people[3]. This data is one of the most precious, yet under tapped, resources in the NHS. “Data is the new oil of the digital economy”[4] and drilling and mining NHS data could improve the NHS[5]. But mining these mountains of data is a colossal task.\nThis is where R comes in. R was conceived in 1992[6] as a free open-source statistical programming environment, which is now widely used in industry[7] (Google, Microsoft, Airbnb, New York Times, Lloyds of London, etc) and academia, and is now ranked amongst the most popular (sixth as of 2017) programming languages[8]. But its use in the NHS is almost non-existent. Whilst there are several reasons for this, the absence of R at scale in the NHS, means that the NHS is unable to take advantage of the huge benefits of R, including cutting-edge visualisation and statistical tools, and a worldwide R community, which freely shares learning and resources.\nSo, our aim is to promote the use of R in the NHS, and help to make the NHS better.\nTo kick-start the NHS-R Community, we have developed a website [www.nhsrcommunity.com] and are offering four free workshops (3 days each, repeated in Yorkshire and Wales). Workshop (1) will be an introduction to R for healthcare analysts. Subsequent workshops will focus on the following problems:- (2) understanding and reporting hospital mortality statistics, (3) predicting urgent demand for hospital care and (4) evaluation of interventions using matched retrospective controls. Each workshop will be led by experts in both the problem domain and R, and captured electronically for wider dissemination. Registration for the workshops is now open*.\nHowever, anyone can contribute to the NHS-R Community, so why not share your experience (novice, beginner, or otherwise) of using R in the healthcare setting? Write a blog, share R tips, do an on-line R tutorial, suggest topics for ongoing development and support, and share ideas on how to embed R into the NHS.\nFrom the NHS-R Team [Posted: 19 March 2018]\nThe NHS-R Community project is funded by The Health Foundation.\n*NB: To be eligible for the workshops you must have a working NHS email address. Places are limited.\n\nhttps://www.newscientist.com/article/2140698-us-ranked-worst-healthcare-system-while-the-nhs-is-the-best/ Accessed 20240221\nhttps://www.nhs.uk/NHSEngland/thenhs/about/Pages/overview.aspx Accessed 20240221\nhttps://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Accessed 20240221\nhttp://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html Accessed 20240221\nhttp://makemeanalyst.com/companies-using-r/ Accessed 20240221\nhttps://spectrum.ieee.org/computing/software/the-2017-top-programming-languages Accessed 20240221"
  },
  {
    "objectID": "blog/simpler-sql-with-dplyr.html",
    "href": "blog/simpler-sql-with-dplyr.html",
    "title": "Simpler SQL with dplyr",
    "section": "",
    "text": "Comparing dplyr with SQL nested queries\nFollowing on from my last post, where I demonstrated R to some first time R users, I want to do a wee comparison of dplyr V SQL, so that folks, particularly those in the NHS who might be R curious, can see just what the fuss is about.\nTo do so I want to recap on the example I showed at the AphA Scotland event.\nThis,in turn goes back to some work I’ve been doing with Neil Pettinger, where we are looking at ways to visualise patient flow.\nThis relies on a spreadsheet that Neil originally put together. Part of my demo was to explain how to recreate the visualisation in R, but I also showed some of the data transformation steps carried out using dplyr and some fellow tidyverse helpers.\nIn this post I want to focus on that a but further, by showing the SQL code I would write to arrive at the same end result.\nIn order to do this I imported Neil’s spreadsheet (which I’ve uploaded - with Neil’s permission to the repo RowOfDots) to into a SQL Server table (by using the built in import wizard, for a quick but not reproducible way of ingesting the data).\nHere’s how that looks:\n\nNB - ALL patient names are entirely made up.\nAs a reminder, for this task we need to create a column that mimics Excel’s floor function and reduces the MovementDateTime field to the nearest 15 mins. We also want to get a count of how many patient were either moving IN or OUT during each 15 minute segment of the day.\n\nSELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME]\nGO\n\nYou’d need to replace the database and table names to suit. I’m not going to explain the code for flooring the datetime field - just know that it works, but you may want to compare the syntax for the case when statement with the equivalent dplyr code ( see later).\nHere is the table output - with the 2 new columns at the end:\n\nNow things get more complicated.\nI have a counter field, but I want to get a cumulative count by each 15 minute segment, staging post and whether this was a movement in or out.\nOne way to do this is to wrap the original query inside another query, so that our newly created counter column can be utilised. This is a similar idea to the the method of mutating a column in dplyr, and having it available within the next pipe.\nWe have to make use of SQL’s windowing functionality to create virtual groupings and orders within the data ( SQL is a set based language, and there is no concept of row order within a set. Therefore to get a cumulative count, we need to make SQL think in terms of rows by partitioning the data by the desired grouping columns and providing columns to order by):\n\nSELECT        x.[MovementDateTime],\nx.[FirstName],\nx.[LastName],\nx.[Ward_Dept],\nx.[Staging_Post],\nx.[Movement_Type],\nx.[IN_OUT],\nx.[Movement15],\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x\nUnderstanding windowing techniques is a great SQL skill to have. Don’t forget where you first saw this ;)!\n\nUnderstanding windowing techniques is a great SQL skill to have. Don’t forget where you first saw this ;)!\nA couple of things to note here are that when we wrap or “nest” the original query, I gave it the alias ‘x’. You do need to provide an alias for this inner query, or the outer query won’t work. Although not strictly necessary, I also prefixed the column names in the outer query so it’s clear that I am selecting the columns from the “virtual” table defined by the inner query.\nHere’s the output with our new Row number (or RNumber) field.\n\nAlmost done, but this is still not in the right format - I need to get an accurate cumulative count. Once more, I take the previous query, and nest that inside a new query - so you can see this is similar to lots of base R style manipulation where the code starts from the middle, or an end, and works back.\n\nSELECT y.MovementDateTime,\ny.FirstName,\ny.LastName,\ny.Ward_Dept,\ny.Staging_Post,\ny.Movement_Type,\ny.IN_OUT,\ny.Movement15,\ny.[counter],\ny.[counter] * y.R_Number AS Movement_15_SEQNO\nFROM (\nSELECT x.MovementDateTime,\nx.FirstName,\nx.LastName,\nx.Ward_Dept,\nx.Staging_Post,\nx.Movement_Type,\nx.IN_OUT,\nx.Movement15,\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x) y\nORDER BY MovementDateTime\nGO\n\nTo recap - our first query floored the movement time to 15 minute intervals and gave us a counter field, we then used that counter field to generate a row number field. Now, even if I’d ordered the result of the second query by MovementDateTime, it still wouldn’t suffice because the rownumbers are all positive, and I want them to be negative when the movement was a movement OUT.\nWe can’t manipulate the row number field within the same query that it is created, so we nest the whole lot once more, this time arranging in the correct time order and multiplying the counter field by our row number field.\nYou’ll notice the second query has been aliased (with a ‘y’) and the columns prefixed so that is is clear exactly where the query is obtaining the data from.\nThis gives us our final output:\n\nA reminder of the dplyr code I used:\n\nplot_data &lt;- data %&gt;%\nmutate(Movement15 = lubridate::floor_date(MovementDateTime,\"15 minutes\")) %&gt;%\ngroup_by(IN_OUT, Movement_Type,Staging_Post,Movement15) %&gt;%\nmutate(counter = case_when(\nIN_OUT == 'IN' ~ 1,\nIN_OUT == 'OUT' ~ -1)) %&gt;%\nmutate(Movement_15_SEQNO = cumsum(counter)) %&gt;%\nungroup()\n\nAnd here is the output - compare with above:\n\nA lot more elegant? Definitely.\nAnother approach to writing the code in SQL would be to use a Common Table Expression, which is a more straightforward of writing and reading it. It’s a similar idea in that you create virtual tables with queries that then run top to bottom until you get your final output. However that is a post for another day :)\nWhat I hope you get from this post is that dplyr and other packages (lubridate for example) really do make life easier for data manipulation.\nLook at the SQL for flooring the date, compared to the lubridate call. Look at the elegance of mutating new columns and having them available within the next chain, compared to horrendous multi-layered nested queries (this one was pretty tame - imagine a few more levels on top of that). You can see how traditional SQL can get unwieldy.\nDplyr is a fantastic asset to the R community, and I hope it might prove to be a great hook to get R further established within the analytical departments of the NHS.\nThis blog was written by John MacKintosh, NHS data analyst based in Inverness, Scotland, and was originally posted on his blog site [johnmackintosh.net]](https://johnmackintosh.net/blog/2018-05-31-dplyr-for-the-win/).\n\n\n Back to topReuseCC0CitationFor attribution, please cite this work as:\nMacKintosh, John. 2018. “Simpler SQL with Dplyr.” June 7,\n2018. https://nhs-r-community.github.io/nhs-r-community//blog/simpler-sql-with-dplyr.html."
  },
  {
    "objectID": "blog/the-operator.html",
    "href": "blog/the-operator.html",
    "title": "The :: operator",
    "section": "",
    "text": "Most of the functionality in R comes from additional packages that you load. Sometimes two packages will have a function with the same name but they will do different things. In a situation where you have multiple packages with functions with the same name loaded, R will use the the function from the package you loaded the latest. As you can imagine, this can sometimes create problems. If you are lucky, you get an error message but if you are unlucky your code runs but with an unexpected result.\nLet me give you an example. I always load the dplyr package. Look what happens when I use summarize to calculate the mean sepal length by species.\n\nlibrary(dplyr)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; # A tibble: 3 × 2\n#&gt;   Species    sepal_length_mean\n#&gt;   &lt;fct&gt;                  &lt;dbl&gt;\n#&gt; 1 setosa                  5.01\n#&gt; 2 versicolor              5.94\n#&gt; 3 virginica               6.59\n\nCreated on 2024-02-21 with reprex v2.1.0\nSay that I then realise that I need the Hmisc package and load it. Look what happens when I rerun the same code as above.\n\n``` r\nlibrary(Hmisc)\n#&gt; Warning: package 'Hmisc' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     format.pval, units\n\niris %&gt;% \n group_by(Species) %&gt;% \n summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nR is now using the summarize function from the Hmisc package and I get an error because the syntax is wrong. The best way to solve this problem is to use the :: operator.Writing packagename::functionname tells R which package to get the function from.\n\niris3 &lt;- iris %&gt;% \n group_by(Species) %&gt;% \n dplyr::summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0"
  },
  {
    "objectID": "blog/the-operator.html#namespace-issues",
    "href": "blog/the-operator.html#namespace-issues",
    "title": "The :: operator",
    "section": "",
    "text": "Most of the functionality in R comes from additional packages that you load. Sometimes two packages will have a function with the same name but they will do different things. In a situation where you have multiple packages with functions with the same name loaded, R will use the the function from the package you loaded the latest. As you can imagine, this can sometimes create problems. If you are lucky, you get an error message but if you are unlucky your code runs but with an unexpected result.\nLet me give you an example. I always load the dplyr package. Look what happens when I use summarize to calculate the mean sepal length by species.\n\nlibrary(dplyr)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\niris %&gt;% \n  group_by(Species) %&gt;% \n  summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; # A tibble: 3 × 2\n#&gt;   Species    sepal_length_mean\n#&gt;   &lt;fct&gt;                  &lt;dbl&gt;\n#&gt; 1 setosa                  5.01\n#&gt; 2 versicolor              5.94\n#&gt; 3 virginica               6.59\n\nCreated on 2024-02-21 with reprex v2.1.0\nSay that I then realise that I need the Hmisc package and load it. Look what happens when I rerun the same code as above.\n\n``` r\nlibrary(Hmisc)\n#&gt; Warning: package 'Hmisc' was built under R version 4.3.2\n#&gt; \n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     format.pval, units\n\niris %&gt;% \n group_by(Species) %&gt;% \n summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nR is now using the summarize function from the Hmisc package and I get an error because the syntax is wrong. The best way to solve this problem is to use the :: operator.Writing packagename::functionname tells R which package to get the function from.\n\niris3 &lt;- iris %&gt;% \n group_by(Species) %&gt;% \n dplyr::summarize(sepal_length_mean=mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0"
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "NHS-R Way book\n\n\n\ncode-of-conduct\n\n\ntraining\n\n\n\nEverything you need or want to know about NHS-R Community including: how to contribute and get involved, code styles, training preparation materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatement on Tools\n\n\n\ndata-science\n\n\ninstallation\n\n\ngetting-started\n\n\n\nIn this book we’ve compiled a set of technical resources, links and write down our experiences of using open data science programs like R and Python within the NHS and…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Inequalities\n\n\n\ndata-science\n\n\nhealth-inequalities\n\n\n\nThis project is a collection of information and knowledge related to analytical work on “Health Inequalities”, as performed in the NHS and the wider UK Health and Social…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Analytics Resources\n\n\n\ndata-science\n\n\nanalysis\n\n\nlinks\n\n\ntraining\n\n\n\nUseful links for health and care analysts and data scientists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "books/posts/NHSR-way/index.html",
    "href": "books/posts/NHSR-way/index.html",
    "title": "NHS-R Way book",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "books/posts/statement-on-tools/index.html",
    "href": "books/posts/statement-on-tools/index.html",
    "title": "Statement on Tools",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact us",
    "section": "",
    "text": "Email: nhs.rcommunity@nhs.net\nGitHub: @nhs-r-community\nLinkedIn: NHS-R Community\nFosstodon (Mastodon): @NHSrCommunity@fosstodon.org\n\nFor details on how NHS-R Community use social media can be found in the NHS-R Community book chapter Community Handbook.\n\n\n\n Back to top"
  }
]