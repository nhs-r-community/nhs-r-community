[
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "",
    "text": "HEADLINE: The NHS-R Community is greatly valued and heavily utilised. It is a unique resource allowing colleagues, both domestic and international, to access high quality, timely advice, and spaces to communicate, collaborate and contribute. No other community, inside the NHS or outside, compares to NHS-R in terms of how welcoming, supportive, and useful we are, especially to those working on coding or data analysis alone within their team.\nThe 2025 NHS-R Community Questionnaire was opened on 2025-04-09 and closed on 2025-05-07 (28 days in total) . In total we received 34 responses."
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#respondent-description",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#respondent-description",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "Respondent description",
    "text": "Respondent description\nMost respondents were in an advanced stage of their R career (5+ years of experience). This may reflect the maturity of coders in the community, or be a selection bias effect created through greater confidence in questionnaire completion by more experienced coders. Alternatively, perhaps the early career coders in the NHS and beyond are less aware of communities like NHS-R, and that we are not currently ‘breaking through’ into their networks well enough.\n\n\n\n\n\n\n\n\nRather unsurprisingly (given the places the questionnaire was advertised), no respondents reported being unaware of the NHS-R Community.\nThere were 5 respondents who reported being very active in the community (agreeing with the statement “I have contributed to the NHS-R Community, its code, Slack engagement, events, training and/or resources”), and a similar number who reported being inactive.\nThe majority of respondents (n = 25, 74%) reported being either active or less active (i.e. agreeing with the statements “I often/have occasionally use(d) the resources available via the NHS-R Community (code, Slack engagement, events, training and/or resources)”. All those reporting to be very active in the community were respondents in the two most experienced categories, possibly suggesting that we need to do more to encourage early R-coders to feel empowered to contribute or engage. All of the inactive community members were also either experienced or advanced coders.\nIn terms of work types, the breakdown of respondents is shown below. Most respondents reported working in provider trusts."
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#blockers-on-more-engagement",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#blockers-on-more-engagement",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "Blockers on more engagement",
    "text": "Blockers on more engagement\nWe asked: “Please tell us why you have engaged at that level, and what has stopped you engaging with NHS-R further?”.\nMost respondents to this question cited a desire to engage more but with a lack of time to do so.\n“It’s me not you!”"
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#frequency-of-resource-use",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#frequency-of-resource-use",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "Frequency of resource use",
    "text": "Frequency of resource use\n\n\n\n\n\n\n\n\nWatching the RPYSOC conference live remotely was the most used resource. Of those resources used often or occasionally by 25% or more of respondents, all but Coffee and Code were unscheduled, meaning they were resources the users could access at times that suited them.\nFew respondents reported often or occasionally using the social media platforms LinkedIn, Bluesky and Mastodon. Similarly few respondents reported attending the RPYSOC conference in person, however it must be noted that fees, travel and greater amounts of time away from work are required to make use of this resource compared to those accessed on the user’s schedule. Also, attending live training is subject to restricted class sizes, so it is perhaps unsurprising that this outcompetes accessing recordings of training.\nThe resource that most respondents picked as their most valued resources were the community chat on Slack and in person conference attendance. Coffee and code was also very popular. N.B. that we did not include an option to choose recordings of Coffee and Code, so we cannot differentiate between proponents of live sessions or recordings of these.\n\n\n\n\nMost Valued Resource\nn\n\n\n\nNHS-R conference – in person attendance\n8\n\n\nSlack\n8\n\n\nCoffee and code\n4\n\n\nNHS-R GitHub/packages\n2\n\n\nTraining - Youtube recordings\n2\n\n\nWebinars – YouTube recordings\n2\n\n\nBlogs\n1\n\n\nBook:NHS-R Way\n1\n\n\nNHS-R Community website\n1\n\n\nNHS-R Conference\n1\n\n\nNHS-R Conference / Slack group\n1\n\n\nTraining – online attendance\n1\n\n\nTrainings/webinars online\n1\n\n\n\n\n\nAll but one respondent reported intending to continue using the NHS-R resources in future.\nTestimonials on the RPYSOC Conference and the NHS-R Community Slack group\n“We have also abandoned some project ideas based on the presentations. This may sound like a negative but I think this may be the most important and impactful contribution of them all. In the public sector, we don’t want to put resources into projects that don’t have a high chance of success and the sooner we understand that a project belongs in that category, the better. I particularly want to commend the conference organizers for finding and scheduling speakers who speak about the real world of working with healthcare data - not a fantasy future world where everything works perfectly the first time and the data are clean and complete. I think this is only possible as no-one is trying to sell anyone on anything as there are no consultants or suppliers pushing for their magical solution. I think this is really valuable and I hope you have the resources to keep doing this!”\n“The Youtube recording of my talk was super helpful - I sent the recording link around my department (a genetics lab where no-one else codes in R) and I think my colleagues finally got why I keep promoting R. Several colleagues approached me afterwards and said they wanted to learn how to code.”\n“The RPYSoc conference is my favorite conference of the year, and I always leave feeling inspired and excited about all the projects that get presented.”\n“Such a great way to keep in touch and keep abreast of development and ask for help. An invaluable resource.”\n“This year I’ve set up an organisational GitHub and project management system for my team, almost entirely based on guidance and resources which were presented / discussed at the NHS-R conference, then with additional support via the slack group. I find the NHS-R conference so practically useful and I’ve learned things I’ve applied straight away every year I’ve attended. It would be an incredibly sad loss to the community if it were not to happen. The slack group is also invaluable as a source of news and again I’ve received really high quality assistance and support through the slack channel.”"
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#what-does-the-community-mean-to-you",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#what-does-the-community-mean-to-you",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "What does the Community mean to you?",
    "text": "What does the Community mean to you?\n“Big question! It means a lot - really important part of learning from other analysts and contributing”\n“Is tangible support for doing things more effectively and efficiently as well as sharing good practice. Out of all the many existing and attempted community groups NHS-R is by far the best community for doing this”\n“It means I’m not alone and I’m part of a community of practice that is committed to continuous improvement and producing the highest quality, best value outputs.”\n“It provides me with access to a diverse group of people to learn from, I work in a small team of 3 and can’t get this from my workplace.”\nRecurrent themes in this section included;\n\n👋 Community atmosphere\n🤗 Welcoming\n👩‍⚕️ Sense of belonging & shared purpose\n💯 Best practice\n💖 Sharing\n🏝 Support for isolated coders\n👩‍🏫 Encouraging beginners"
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#alternative-support-sources",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#alternative-support-sources",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "Alternative support sources",
    "text": "Alternative support sources\nWe asked our members where they may go for similar content and community support should NHS-R cease to operate. There were many suggestions, but almost all were either not healthcare-specific, too advanced and intimidating for beginners, too exclusive (e.g. only for course alumni), or not free and open-source.\n\nNHS Pycom\nNHS Futures\nTuring Way\nGovernment Data Science Slack\nHSMA Slack (for alumni only)\nHACA\nPosit Community\nStack Overflow\nYouTube\nAI\nPluralsight\nMidlands Analyst Network\nAnalystX\nR podcasts"
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#suggestions-for-improvement",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#suggestions-for-improvement",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "Suggestions for improvement",
    "text": "Suggestions for improvement\nAlthough almost all respondents reported themes like: “I am very happy” or “It is a great community. Please keep the events and training coming - it is amazing to have a conduit to find and learn from very talented people.”,\nothers did offer some suggestions. These can be grouped into the following categories:\n\nOffer more training sessions\nAdvertise on Trust websites\nOffer training for more advanced coders, not just beginners\nFund the Slack to preserve its history\nHost a better, more complete calendar of events\nImproved booking system for coffee and code"
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#testimonials",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#testimonials",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "Testimonials",
    "text": "Testimonials\n“I think NHSR is irreplaceable, and I think it should be funded by NHSE rather than disappear. The NHS benefits from many forms of analytics improvement enabled by NHSE.”\n“I have found the NHS-R community an invaluable resource for developing my analytics and data science skills and the conference is the highlight of my working year. This has become even more important now I’m developing a data science function within my team and NHS-R is a big part of enabling this to happen. I find the NHS-R conference so practically useful and I’ve learned things I’ve applied straight away every year I’ve attended. NHS-R means I’m part of a community of practice that is committed to continuous improvement and collaboration and facilitating sharing and support between organisations. It is the friendliest and most proactive and practical of all the networks I am a part of.”\n“NHSR is busy being the difference we all need. It is enabling and upskilling analysts to be more productive, do more advanced work, and collaborate across organisational boundaries”\n“The NHS is hundreds of separate organisations - NHSR is one of very few who are successfully helping analysts work across those boundaries, and patients and staff are benefiting as a result.”\n“I can’t imagine the NHS ceasing to exist. It would be massively short sighted. I am really proud to be part of this community. I think the NHS R community is vital to further the aims of the Goldacre report and to support the work required based on the Darzi report. I regularly mention the NHS R community in my work as a consultant doctor in the NHS. I don’t have an alternative to the Slack community. And the conference is really excellent. I wouldn’t know how to code in R without the community, as in I would have given up without the help I got on the slack channel.”\n“The NHS-R/pycom conferences organised by the NHS-R and python communities have been a great value-add for both myself as a senior analytical leader, as well as my team of healthcare analysts and data scientists. Providing a broad mix of content that caters to analysts at different skill levels and/or points in their career, it offers ‘something for everyone’ and surfaces work on common themes and problems from across the NHS. The welcoming nature of the conferences made it easy for my team members not only to attend, but also to contribute themselves through talks and webinars, and develop more confidence and ‘pride’ in their work and skills along the way.”\n“The resources available are extremely useful. The back catalogue of recorded webinars available on YouTube have been invaluable”"
  },
  {
    "objectID": "reports/reports/NHS-R_questionnaire_2025/index.html#prize-draw-winners",
    "href": "reports/reports/NHS-R_questionnaire_2025/index.html#prize-draw-winners",
    "title": "NHS-R 2025 Questionnaire Analysis",
    "section": "🥂 Prize draw winners 🎉!",
    "text": "🥂 Prize draw winners 🎉!\nThe randomly selected winners of the two £25 Amazon vouchers go to Oliver Peatman and Helena Robinson. Congratulations both!"
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "R Packages",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "blog/a-thank-you-note-to-nhs-r-community.html",
    "href": "blog/a-thank-you-note-to-nhs-r-community.html",
    "title": "A thank you note to NHS-R Community",
    "section": "",
    "text": "The less said about 2020 the better, but I want to write about something positive that has happened to me this week, but which was the result of work done by, and for, NHS-R Community throughout the year-that-shall-not-be-mentioned.\nAt the end of November last year, R Studio announced its 2021 conference and their diversity scholarships https://blog.rstudio.com/2020/11/30/diversity-scholarships/. In previous years, these scholarships have been used to support people to attend the conference in person, but this year – as it’s virtual – the conference is free, so the scholarship includes:\n\nOpportunities for online networking and support before and during the virtual conference\nTwo workshops, taught online the week after rstudio::global(2021)\nPractical support, if needed, to enable participation in the virtual conference (such as an accessibility aid, a resource for internet access, or childcare)\n\nand the number of scholarships was increased from 44 to 70.\nSeeing this blog, I listed out all the things I had done for NHS-R Community, tidied up my GitHub (which also increased my commit numbers somewhat!) and sent in my application.\nThis week I heard that I was successful.\nThis is a jaw-dropping form of recognition as the scholarships were not restricted by country this year so I expect a little over 70 people will have applied! It means so much more to me than a line on my CV because it’s an acknowledgement of all that the NHS-R Community means to me; if it weren’t for our community, I wouldn’t have had the opportunities and support I needed to get this far in my R journey.\nWhen I wrote out the list of what I’d done on behalf of NHS-R Community, it turned out to be quite a surprising list – so much so that even my husband (who I’d asked to ‘sense check’ my writing) felt compelled to remark on how much work I had done. But such work isn’t hard when you have fun and feel safe to try things out without fear of failure. And that’s why I did so much, because if you want to try out something – blog, teach, share some code or just ask a question – there are places to go for support, but none quite like the support you get from a group of like-minded R and healthcare data enthusiasts, such as those we find at NHS-R Community.\nThank you NHS-R Community; you are the best.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html",
    "href": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html",
    "title": "NHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…",
    "section": "",
    "text": "Originally appeared on: https://hutsons-hacks.info/nhs-r-2020-annual-conference and https://www.r-bloggers.com/2020/11/nhs-r-2020-week-long-conference/.\nThe NHS-R conference concluded, and I am emotional that it has ended. There were some fantastic speakers and the whole event, from start to finish was a blast."
  },
  {
    "objectID": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#openers",
    "href": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#openers",
    "title": "NHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…",
    "section": "Openers",
    "text": "Openers\nI would like to include everyone on here, but the openers that had an impact for me are included below:\n\nConference opening – the journey so far – Professor Mohammed Amin Mohammed\n\n\nWhat a great opener from Mohammed and it was really interesting to get a view of the direction of the NHS-R Community.\n\n\nOpenSAFELY.org: proving the power of open methods for NHS data analysis\nBen Goldacre, you may have heard of his from his books on Bad Science and Bad Pharma, dropped in for a chat about the OpenSAFELY.org platform and the need for open methods for NHS data analysis.\n\n\n\n\nOpening up Analytics by NHS-x\nSarah Culkin did a good talk on opening up new data analytics and the need for an open source mindset. Advanced analytics, was mentioned as another revolution and interesting work is at foot in the AI lab. Watch the video to find out more:"
  },
  {
    "objectID": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#excellent-workshops",
    "href": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#excellent-workshops",
    "title": "NHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…",
    "section": "Excellent workshops",
    "text": "Excellent workshops\nPrior to the conference, there were a week of excellent workshops. The ones I found most interesting were:\n\nRegression Modelling\nChris Mainey did an excellent introduction to Regression Modelling. Check it out:\n\n\n\n\nPretty R Markdown and Presentations with Xaringan\nThis was a two part session by Silvia Canelón:\n\n\n\n\n\n\nIntroduction to and RMarkdown\nZoë Turner did a brilliant job of onboarding newer developers with {dplyr}:"
  },
  {
    "objectID": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#awesome-plenary-talks",
    "href": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#awesome-plenary-talks",
    "title": "NHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…",
    "section": "Awesome plenary talks",
    "text": "Awesome plenary talks\nThe plenary talks were awesome this year, as well as the Lightening talks. I have not watched all of the sessions, so I do apologise if I missed you out, but all the sessions can be found on YouTube.\n\nComputer Vision – how it can aid clinicians\nShameless self promotion – I did a talk on how Computer Vision can aid clinicians:\n\n\n\n\nBuilding Predictive Models with HES data\nChris Mainey did another excellent slot looking at GAMS and Mixed Effects models, and how they apply to HES datasets:\n\n\n\n\nIntegrating R and QlikSense\nJamie-Leigh Chapman did a stellar job at showing the integration capabilities of R and QlikSense:\n\n\n\n\nDecision Modelling in R and Shiny\nA very interesting session by Robert Smith:\n\n\n\n\nCausal Inference in Predictive Modelling\nGreat session from Andi Orlowski and Bruno Petrungaro:\n\n\n\n\nUsing polygon and spatial data to plot brain atlases\nAthanasia Mowinckel has created an amazing package for displaying brain atlases:\n\n\n\n\nR Code Quality: Does it Really Matter?\nA great session on code quality and packages that can be used to scare the life of people with bad coding practice, sounds like all of my mates, and me:\n\n\n\n\nAPIs in R with Plumber\nThis session focussed on passing model parameters through from PlumbeR. Definitely something that will be very useful in the future:"
  },
  {
    "objectID": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#wowsome-lightening-talks",
    "href": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#wowsome-lightening-talks",
    "title": "NHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…",
    "section": "Wowsome Lightening Talks",
    "text": "Wowsome Lightening Talks\nThe Lightning talks this year, like everything else, were fascinating. The links to the three days worth of lightning talks are included in this post:"
  },
  {
    "objectID": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#the-full-playlist",
    "href": "blog/nhs-r-2020-week-long-conference-so-much-great-content-so-little-time-to-catch-it-all.html#the-full-playlist",
    "title": "NHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…",
    "section": "The full playlist",
    "text": "The full playlist\nThe below YouTube link has all the sessions from the workshop. The ones contained in my blog were the ones I got chance to watch and I found these really useful and interesting. I am sure there are many more in the playlist that I have not had time to watch yet. So much great content, so little time.\nThese can be accessed hereunder:\nhttps://www.youtube.com/playlist?list=PLXCrMzQaI6c3EAh10hDZectzBALWPk19w\nhttps://www.youtube.com/playlist?list=PLXCrMzQaI6c0Kvs7lE-Rxdig5nc5-4Xx2\nSubscribe to this YouTube channel and watch out for the excellent content coming out of this community. The belief is for 100% open source in the NHS and all our code is sharable.\nSee you for the conference in 2021\nThe blog has been edited for NHS-R Style"
  },
  {
    "objectID": "blog/nhs-r-community-needs-help.html",
    "href": "blog/nhs-r-community-needs-help.html",
    "title": "NHS-R Community needs help",
    "section": "",
    "text": "I’ve been an analyst for over a decade in the NHS and for the most part I’d never been to a conference related to my work. Partly it was that they didn’t really exist but also that there is a cost involved for tickets, travel, sometimes hotel, and there was never any budget. Then in 2018 things changed for me. NHS-R Community was set up and I went to one of the first free training sessions in Leeds and because it was free I got a rare agreement for the Trust to cover the costs for travel and hotel. Also in 2018 we had the first (still free!) NHS-R Community conference and I got a lift there so the only cost was the of me being away from my work.\nSince 2020 the conference has been either fully in person or streamed so more people can join 1 and one of the great side effects of going virtual is that things are recorded so we have a YouTube channel full of wonderful content from the NHS-R Community. And we also have increasing opportunity to go to other conferences like HACA and AphA but with greater choice it can become even harder to justify the time out of the job to attend or catch up on conferences.\nThis year’s NHS-R/NHS.pycom conference was a success for those that could attend and partly that’s because we tried things out:\nBut we got things wrong:\nWe can do better.\nBut what is it that we want to be better at?\nIs it putting on a better conference?\nIs it producing more output that convinces managers who are not sure of the uses of open source programming?\nIs it changing our name as it suggests we are just “NHS” and “R” – which we aren’t?\nHave we done enough?\nWe might have a lot of work to do but one thing I do know is that if we are going to do more we are going to do this together because all that we’ve built has been by us: the NHS-R Community.\nAnd how do we do better? I think we need to start with a new Committee to help shape the next 3 years. We have questions around rebranding and promotion of the work we do, how do we measure how successful we are (which helps with funding opportunities), what shall we do with our space at the HACA conference 2024, and of course there is our own conference to plan."
  },
  {
    "objectID": "blog/nhs-r-community-needs-help.html#can-you-help",
    "href": "blog/nhs-r-community-needs-help.html#can-you-help",
    "title": "NHS-R Community needs help",
    "section": "Can you help?",
    "text": "Can you help?\nWould you come to the Committee and share your views? The more diverse the better.\nWe need commitment of your time and, more importantly, we need your enthusiasm and desire to keep the NHS-R Community going so in the next few weeks I’ll be setting up Committee meeting where our first task will be to shape what we, as a Committee and a Community, will do.\nIf you are interested in being involved with the Committee please contact me via our Slack group or email zoe.turner3@nhs.net"
  },
  {
    "objectID": "blog/nhs-r-community-needs-help.html#join-the-slack-groups",
    "href": "blog/nhs-r-community-needs-help.html#join-the-slack-groups",
    "title": "NHS-R Community needs help",
    "section": "Join the Slack groups",
    "text": "Join the Slack groups\nDetails are in the Open Analytics Resources and feel free to contact NHS-R Community via email at nhs.rcommunity@nhs.net"
  },
  {
    "objectID": "blog/nhs-r-community-needs-help.html#footnotes",
    "href": "blog/nhs-r-community-needs-help.html#footnotes",
    "title": "NHS-R Community needs help",
    "section": "Footnotes",
    "text": "Footnotes\n\nHybrid is a tricky label to give a conference as you need to have both participants and speakers to be in person and virtual so I can’t say that we’ve done that in NHS-R Community..↩︎\nComments\n\n\n\nCalum Polwart\n20 October 2023\nSorry I didn’t attend this year. I attended for the first time last year, because I had something to share. I’m sure I will be picking up some video too. I’m sure everyone will be different but some off the cuff thoughts from me - It’s hard until you go, to know quite how geeky it will/will not be (answer as geeky as you want!) Timing. I don’t know why but Oct-Dec for me seems to be peak conference season! This isn’t my only part of my job and so I need to balance things. Posters. To me posters are a big thing at conferences. You can get ‘up close and personal’ with the presenter. Is there scope for poster presentation as well as stage? And what about opportunities to critique… Bring your last graph… We will tell you what could be better? I’m there to learn so I need to know how to improve. Any opportunities for a Hackathon?\nZoë Turner\n20 October 2023\nThanks for these comments! Much appreciated and we will be definitely be talking about some of these (posters, timing, hackathons) at the Committee. You’d be very welcome to join in :) Details are in the blog.\nCharlotte Foster\n26 October 2023\nIs this a call for help to analysts in all departments? I’d be interested in having a conversation! :)\nZoë Turner\n30 October 2023\nAbsolutely yes it’s a call for anyone in the NHS-R Community and that includes all public sector and Government organisations. Please do get in touch! Thanks"
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html",
    "href": "blog/coding-to-and-from-NA.html",
    "title": "Recoding an NA and back again",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course."
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#create-data",
    "href": "blog/coding-to-and-from-NA.html#create-data",
    "title": "Recoding an NA and back again",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course."
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#recoding-to-na",
    "href": "blog/coding-to-and-from-NA.html#recoding-to-na",
    "title": "Recoding an NA and back again",
    "section": "Recoding to NA",
    "text": "Recoding to NA\n\nsurvey &lt;- tibble::tribble(\n  ~Survey.Response, ~Code,\n       \"Response1\",   -9L,\n       \"Response2\",    2L,\n       \"Response3\",   10L,\n       \"Response4\",    0L,\n       \"Response5\",    5L,\n       \"Response6\",   -9L,\n        \"Missing\", NA\n  )"
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#recode-to-na",
    "href": "blog/coding-to-and-from-NA.html#recode-to-na",
    "title": "Recoding an NA and back again",
    "section": "Recode to NA",
    "text": "Recode to NA\n\nlibrary(tidyverse)\n\nsurvey |&gt; \n  mutate(new_column = na_if(Code, -9))\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column\n  &lt;chr&gt;           &lt;int&gt;      &lt;int&gt;\n1 Response1          -9         NA\n2 Response2           2          2\n3 Response3          10         10\n4 Response4           0          0\n5 Response5           5          5\n6 Response6          -9         NA\n7 Missing            NA         NA\n\n\nIt’s also possible to use the numbers and case_when():\n\nsurvey |&gt; \n1  mutate(new_column = case_when(Code &lt; 0 ~ NA,\n2                                Code == 0 ~ 1000,\n3                                .default = Code))\n\n\n1\n\nWhere Code is less than 0 then code to NA.\n\n2\n\nWhere Code is equal to 0 then recode to 1000 which is a number that will stand out.\n\n3\n\nFor everything else return the original data from column Code.\n\n\n\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column\n  &lt;chr&gt;           &lt;int&gt;      &lt;dbl&gt;\n1 Response1          -9         NA\n2 Response2           2          2\n3 Response3          10         10\n4 Response4           0       1000\n5 Response5           5          5\n6 Response6          -9         NA\n7 Missing            NA         NA\n\n\nOr ifelse() where there are only two options:\n\nsurvey |&gt; \n  mutate(new_column = ifelse(Code &lt; 0, NA, Code))\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column\n  &lt;chr&gt;           &lt;int&gt;      &lt;int&gt;\n1 Response1          -9         NA\n2 Response2           2          2\n3 Response3          10         10\n4 Response4           0          0\n5 Response5           5          5\n6 Response6          -9         NA\n7 Missing            NA         NA"
  },
  {
    "objectID": "blog/coding-to-and-from-NA.html#recode-from-na",
    "href": "blog/coding-to-and-from-NA.html#recode-from-na",
    "title": "Recoding an NA and back again",
    "section": "Recode from NA",
    "text": "Recode from NA\n\nsurvey |&gt; \n  mutate(new_column2 = replace_na(Code, 1000))\n\n# A tibble: 7 × 3\n  Survey.Response  Code new_column2\n  &lt;chr&gt;           &lt;int&gt;       &lt;int&gt;\n1 Response1          -9          -9\n2 Response2           2           2\n3 Response3          10          10\n4 Response4           0           0\n5 Response5           5           5\n6 Response6          -9          -9\n7 Missing            NA        1000"
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html",
    "href": "blog/why-government-needs-sustainable-software-too.html",
    "title": "Why Government needs sustainable software too",
    "section": "",
    "text": "Unlike most of the 2017/2018 cohort, when I applied to become a fellow of the Software Sustainability Institute, I was a civil servant rather than an academic. In this blog post I want to talk about why Government needs sustainable software, the work being done to deliver it, and the lessons we learnt after the first year. But Government already has sustainable software…\nThere’s quite a bit of disambiguation that needs to be done to the statement ‘Government needs sustainable software’. In fact, Government already has sustainable software, and lots of it. One need only look at alphagov, the GitHub organisation for the Government Digital Service. Sustainable, often open source, software is alive and well here, written by professional software developers, and in many other places in central and local Government alike. But this isn’t the whole story.\nThere are other parts of Government that write software, but like many in academia, you may have a hard time convincing them of this fact. In central Government (this is where my experience lies, so I will focus largely upon it) there are literally thousands of statisticians, operational researchers, social researchers, economists, scientists, and engineers. Any one of these may be writing code in a variety of languages in the course of their daily work, but don’t identify as software developers. It’s among these professions that there are tasks that will look most familiar to the academic researcher. Government statisticians in particular are tasked with producing periodic publications which incorporate data, visualisations, and analyses, much like academic outputs.\nSo in this blog post, I’m really talking about bespoke software that is used to create Government statistical publications."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#why-sustainability-is-so-important-for-government",
    "href": "blog/why-government-needs-sustainable-software-too.html#why-sustainability-is-so-important-for-government",
    "title": "Why Government needs sustainable software too",
    "section": "Why sustainability is so important for Government",
    "text": "Why sustainability is so important for Government\nThe reasons for sustainability in academic publications have been well documented by the Software Sustainability Institute, but I would argue that it is even more important that Government writes reproducible and sustainable software for its statistical publications. Here’s why:\n\nThe outputs really matter\nI don’t want to downplay the importance of research outputs, publishing accurate science is critical to advancing human knowledge. What is different about research is that there is rarely a single source of truth. If a research group publishes a groundbreaking finding, we all take notice; but we don’t trust the findings until they have been replicated preferably by several other groups.\nIt’s not like that in Government. If a Government department publishes a statistic, in many cases that is the single source of truth, so it is critical that the statistics are both timely and accurate.\n\n\nPublications are often produced by multiple people\nThe second way that Government statistical publications differ from academic scientific publications is that they are often produced by a team of people that is regularly changing. This means that even at the point that it is being produced it needs to be easy for another member of the team to pick up the work and run with it. If someone goes on holiday, or is sick at the critical moment, their colleagues need to be able to pick up from where they left off immediately, and understand all the idiosyncrasies perfectly. The knowledge simply cannot rest in one person’s head.\nMore than that, since publications are often periodic (for example monthly, or annual) and analysts typically change role once a year, the work will very likely need to be handed off to someone new on a regular basis. It is essential therefore that these processes are well documented, and that the code that is being handed over works as expected.\n\n\nThe taxpayer pays for it\nObviously, the longer it takes a team of statisticians to produce a statistical report in Government, the more it costs to the taxpayer, and all Government departments have an interest in being efficient, and reducing unnecessary waste.\nAdditionally, since Government statistical publications are paid for by the public, where possible Government should be open and publish its workings. Coding in the open is already an important part of the culture among digital professions, adopting sustainable software practices allows statistical publications to be produced with the same openness.\n\n\nWorking towards sustainability\nI started working in Government as a Data Scientist after doing a PhD and post-doc in environmental science. I’d attended two Software Carpentry workshops during this time, and wrote my PhD in LaTeX and R. On joining Government it was clear that we could apply some of these lessons to improve the reporting workflow in Government.\nWorking with the Department for Digital, Culture, Media, and Sport (DCMS) we had a first attempt at implementing a reproducible workflow for a statistical publication that was being produced with manual processes using a number of spreadsheets, a statistical computing package, and a word processor. We used RMarkdown to rewrite the publication, and abstracted the logic into an R package freely available on GitHub, complete with continuous integration from travis and appveyor.\nIn March of 2017 we published this work in a blog post, and worked hard to publicise this work with a large number of presentations and demonstrations to other Government departments. The prototype generated lots of interest; in particular an initial estimate that it could save 75% of the time taken to produce the same publication using the old methods.\nBy November we blogged again about successful trials of this approach in two further departments: the Ministry of Justice (MoJ), the Department for Education (DfE). We also produced a GitBook describing the various steps in more detail. Most of this is sensible software development practice; but it’s something that many Government analysts have not done before.\nBy the end of the year, the ideas had gained enough traction in the Government statistical community, that the Director General for the Office of Statistics Regulation (the body responsible for ensuring quality among official statistics) reported that this work was his favourite innovation of the year, although he wasn’t so keen on the name!\nWork continues to bring these techniques to a wider audience. There’s now a free online course built by one of my former colleagues to help civil servants get started, and a number of departments, particularly the MoJ are making great strides to incorporate these techniques into their workflows."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#lessons-learnt",
    "href": "blog/why-government-needs-sustainable-software-too.html#lessons-learnt",
    "title": "Why Government needs sustainable software too",
    "section": "Lessons learnt",
    "text": "Lessons learnt\nA year or so after we set out with the intention of bringing sustainable software to Government statisticians, here are some of the lessons that I would like to share."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#reproducibility-is-technical-sustainability-is-social",
    "href": "blog/why-government-needs-sustainable-software-too.html#reproducibility-is-technical-sustainability-is-social",
    "title": "Why Government needs sustainable software too",
    "section": "Reproducibility is technical, sustainability is social",
    "text": "Reproducibility is technical, sustainability is social\nWe called the first prototype a ‘Reproducible Analytical Pipeline’ and acronym ‘RAP’ has stuck. This is not a very good name on reflection because it belies the main difficulty in transitioning from manual workflows into something more automated: making it sustainable. It’s very well creating beautiful, abstracted, replicable data workflows, but they are completely useless if no one knows how to use them, or to update them. That situation is more dangerous than the manual workflows that exist in many places at present, because at least the barrier to entry for tortuous manual processes is lower: you don’t need to know how to program to interpret a complicated spreadsheet, you just need a lot of patience.\nWhat this move from manual to automated implies is a recognition of the need for specialists; organisations will need to recruit specialists, make use of the ones they already have, and upskill other staff. This is a challenge that all organisations will need to rise to if they are to make these new methods stick.\nThis is likely to be less of a problem for academia, where within certain fields there is already an expectation that researchers will be able to use particular tools, and there may be more time to develop expertise away from operational pressures. However, there also exists a powerful disincentive: because journal articles are closer to ‘one off’ than a periodic report, it is less critical that researchers leave the code behind a paper in a good state, as they may never need to come back to it again."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#senior-buy-in-is-critical",
    "href": "blog/why-government-needs-sustainable-software-too.html#senior-buy-in-is-critical",
    "title": "Why Government needs sustainable software too",
    "section": "Senior buy-in is critical",
    "text": "Senior buy-in is critical\nIn just over a year, we went from seeing an opportunity to scaling the idea across a number of Government departments, traditionally very conservative organisations. Getting the buy-in of senior officials was absolutely critical in our ability to get the idea accepted.\nIt’s important to realise early that senior managers are often interested in very different things to the users of the software, so messages need to be targeted to gain traction with the right audience. For instance, an incentive for managers in academia might be: mitigating the risk of errors that could lead to retraction, rather than by the expectation of cost savings."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#time-is-money",
    "href": "blog/why-government-needs-sustainable-software-too.html#time-is-money",
    "title": "Why Government needs sustainable software too",
    "section": "Time is money",
    "text": "Time is money\nOne of the reasons that we managed to make a big impact quickly is because Government departments are always keen to reduce costs. If a publication takes a team of four people a few weeks to produce, the cost quickly adds up. This is a feature of Government (and indeed industry) which is not shared by academia. Yes, it matters that work is delivered on time, but in my experience researcher time is a much more elastic resource. I was much more likely to work all evening or over the weekend as a PhD student or post doctoral researcher than I was as a civil servant; it was almost an expectation. For this reason, the financial imperative seems to be a much less powerful incentive in academia."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#its-not-all-about-the-code",
    "href": "blog/why-government-needs-sustainable-software-too.html#its-not-all-about-the-code",
    "title": "Why Government needs sustainable software too",
    "section": "It’s not all about the code",
    "text": "It’s not all about the code\nNotwithstanding my comments about sustainability, it is important to note that reproducibility does not stop with reproducible code. We also need to worry about the data, and the environment. The former is particularly difficult in a Government setting, as one department often relies on another to provide the data, meaning that there is a less clear route to source than many academics enjoy. There are important initiatives underway in Government, such as GOV.UK Registers, which oversees the development of canonical lists of important information critical to the running of the country. Not all data can be treated in this way, and whilst taking snapshots of data may be a blunt instrument, it works when you don’t have control of where it comes from."
  },
  {
    "objectID": "blog/why-government-needs-sustainable-software-too.html#call-to-arms",
    "href": "blog/why-government-needs-sustainable-software-too.html#call-to-arms",
    "title": "Why Government needs sustainable software too",
    "section": "Call to arms",
    "text": "Call to arms\nAlmost all the projects I have referred to in this blog post are open source, and available on GitHub, so follow the links above if you are interested. There’s also two presentations on the topic available as slides (Earl conference 2017 and Government Statistical Service conference 2017) which give more technical details on the projects.\nThis blog is written by Matthew Upson, Data Scientist at Juro and was originally posted on the Software Sustainability website.\nThis blog has been formatted to remove Latin Abbreviations."
  },
  {
    "objectID": "blog/success-story-william-bryant.html",
    "href": "blog/success-story-william-bryant.html",
    "title": "Success story - Dr William Bryant, Senior Data Scientist, GOSH",
    "section": "",
    "text": "The Challenge:\nTo create a template workflow for a stream of analytics and data presentation projects for research and operational use at GOSH. To ensure that the codebase is: interpretable, reusable & maintainable and to control technical debt around the solutions we provide, so that they can be handed over to other technical teams in the Trust.\n\n\nThe Solution:\nWe have created a ‘Data Science’ template, inspired by Cookiecutter Data Science and using the targets R package to logically break down code into small reusable chunks applied to our research datasets.\n\n\nThe Results:\nWe are now running (and have completed) a number of projects from estimation of surgery lengths with XGBOOST, to dashboarding cardiac data for MDTs, to creating articles on historic data trends at GOSH and many more. This has provided us a rich common codebase applicable in all sorts of contexts, as well as giving us a headstart when understanding each other’s code and collaborating on development.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/forecasting-r-virtual-workshop-the-pupils-perspective.html",
    "href": "blog/forecasting-r-virtual-workshop-the-pupils-perspective.html",
    "title": "Forecasting R (Virtual) Workshop – the pupil’s perspective",
    "section": "",
    "text": "I saw on the NHS-R Community slack channel that the much sought after Forecasting R Workshop led by Bahman Rostami-Tabar was moving to a virtual environment whilst most of the country was shut due to the COVID pandemic. I had previously read the “Forecasting: Principles and Practice” online textbook from cover to cover and learnt the ‘motions’ of time series forecasting but hadn’t really understood the ‘why’. I had used previously black-box style models (Prophet), and understood how to make it work rather than why it works. I was looking forward to the course cementing some of the fundamentals in theory, and then refine the practice in the software.\nIt delivered.\nLooking back through the 12 pages of notes I took to go alongside the excellent content produced by Bahman, we covered a huge amount in 6 short, two hour sessions.\nBefore even picking up a dataset however, we talked through some of the key factors behind forecasting – the high level process, when we might want to or when we might not want to do it, and in which context it works best.\n\n\n\nThe time series forecasting process – prep data, visualise, specify a model, model estimation, fit model, evaluate, forecast output\n\n\nAnother key item before diving in that we covered were some of the key ‘glossary’ terms that are used, such as the difference between frequency (the time intervals between points in your prediction) and horizon (how far into the future you are going to make your prediction). For some, this might be bread and butter, but adding structure to the language of forecasting really helped me get it clear in my head.\nWe spent a good share of the sessions looking at the theory of forecasting, and intermingled this with sessions in R. We moved through baseline predictions using simple average, naïve, seasonal naïve and drift methods (and their corresponding functions) through to exponential smoothing, auto-regressive integrated moving average (ARIMA) and regression.\n\n\n\nBaseline models, what they do, and how to do it in R\n\n\nWe also looked at several measures within the forecasting toolbox, such as residual diagnostics and forecasting errors. It was going through these that asking Bahman to pause and explain in a bit more detail some of these concepts that really set an instructor led course in a league above trying to work through a textbook, re-reading the same cryptic passage and still being none the wiser.\nMy key takeaway from the learning was that R makes it really easy to have a hands off approach to modelling (without a calculator in sight!), but picking apart some of the automation meant I was able to convey what was happening back to colleagues from a much better informed perspective.\n\n\n\nMy mind map summary of my key takeaways\n\n\nThe virtual environment worked well from a pupil’s perspective, with some caveats. First, I found that reading the pre-session work was vital. Although Bahman was very open to going back over content, the pace at which we had to move was quick due to time constraints, so having a sense of the content beforehand really helped slot things in to place.\nAlongside the reading, it was so important to have the lab scripts prepped and ready to go from day one. The NHSR Community team did a great job of getting the projects set up on RStudio Cloud, which meant that everything was ready to go, but if (like me) you wanted to get it running locally on your machine spending the time making sure your setup was working as expected prior to the first session was vital.\nOn the whole, I found the course both hugely enjoyable and informative. I am looking forward to integrating all that I learnt into my role as a demand and capacity analyst, finally feeling like I am coming from a perspective of understanding as to what methods to use and how they work. The instructor-led approach meant that I could finally get to grips with ideas that had been mystifying from just reading a textbook, for which I would like to extend my thanks to Bahman, Tom and Alysia for their work on running the course.\nPaul Bullard, NHS England & NHS Improvement - Demand and Capacity Analyst\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/software-licensing.html",
    "href": "blog/software-licensing.html",
    "title": "Software licensing",
    "section": "",
    "text": "Other resources\n\n\n\n\n\nChris did a video on open source for analysts and data scientists published on our YouTube which also covers licences and copyright.\nThere is a also a chapter in the NHS-R Way book related to licences and copyright and what we use.\n\n\n\nAt NHS-R we all support open source software. R itself is open source. So is RStudio. CRAN packages are open source. And many of us use things with R that are open source- Linux servers to host Shiny applications, MySQL databases to store data, and PyCharm to run Python code. Although most people have a rough idea about what open source means (sort of cuddly and nice and you don’t have to pay for it) open source licensing is not well understood even within the NHS-R community, let alone among the non-technical people in the rest of the health and social care community.\nThe first thing to understand about open source is that it’s not necessarily free. In fact, being able to charge for software is a fundamental part of the definition of the freedom of open source. As the Free Software Foundation say: you should think of “free” as in “free speech,” not as in “free beer”\nOpen source software gives users the freedom to “run, copy, distribute, study, change and improve”\nIn practice, this means that the source code of the software must be available for inspection, and users must have the right to run and modify the source code. This is the second thing to understand about open source software – just putting the code on GitHub without a licence does not make it open source, and it does not give users to right to modify or even run your code. The copyright of code is automatically given to whomever produced it (or, more usually in the NHS and other organisations, to their employer). Explicit licence must be granted to allow others to use and modify code, even if the source is available (it’s worth saying, if you don’t want to or can’t properly open source your code, it’s still worth posting it, because at least then we can see how it works, even if we can’t run it).\nAll NHS-R branded solutions are open source, and being open source is a precondition for being a solution that is funded or branded by NHS-R. Applications for NHS-R solution funding must, therefore, include a choice of licence that the code will be released under. We recommend the MIT licence but any open source licence is acceptable. There are two main types of software licence, and the difference between them is important. The MIT licence is the most popular “permissive” licence. What permissive means in this context is essentially that the code can be freely reused anywhere with few other restrictions on it. In particular, the code can be incorporated into software which is not open source- for example in paid proprietary software, like Tableau, or Excel.\nThe other main type of software licence is a “copyleft” licence. A copyleft licence is like a permissive licence in the sense that it allows code to be reused and modified, and even incorporated into other codebases. However, the fundamental difference with copyleft licences is that if copyleft licensed code is included in another codebase, all the code, not just the open sourced bit, must be released under the same software licence conditions. This makes this licence extremely unattractive to proprietary software vendors, since incorporating 100 lines of your copyleft licensed code into, say, Excel, would force Microsoft to release the whole program, all of the code, under an open source licence. The most common copyleft licence is the GPL3.\nThe choice is yours. If you would like your code to be incorporated within a vendor’s products, for example if you have some analytic code that you would like to see embedded in the proprietary EPR system that your organisation uses, you will need to use a permissive licence like MIT. If you don’t want software vendors to take your code and include it in paid offerings to yours and other organisations, you will need to use a copyleft licence like GPL3. It’s worth thinking about carefully.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/animating-a-graph-over-time-in-shiny.html",
    "href": "blog/animating-a-graph-over-time-in-shiny.html",
    "title": "Animating a Graph Over Time in Shiny",
    "section": "",
    "text": "I was interested in creating an animated graph in Shiny, which allows for an interactive input. As a mock example, I created an app which shows the distribution of a waitlist’s volume over time.\nExcel offers the ability to easily create graphs, though when it comes to animating graphs, the process can be tricky, using complicated VBA. In my opinion, R is far better suited to dealing with this, due to the object oriented nature.\n\nSimple, right?\nWhen I first had a go with making a simple animated plot, I thought it would be as simple as:\n\nCreate vector of distribution\nPlot this vector as a bar chart\nUpdate the new vector every time there is a trigger\n\nUnfortunately, that’s when I ran into reactivity.\n\n\nReactivity\n\nIn short (and to my understanding), if you have anything in R Shiny that will be updated, it should be within a reactive environment. Flows of information in R work by pull, rather than push mechanisms. This means the data is “looked at” only when needed by the output (that is when it is created), but the data doesn’t push forward any change to the output if it updates.\nAbove, case 1 represents the normal situation in R once data has been updated. In case 2, a reactive environment is used, which effectively reverses the direction of the flow of information in R.\nAnother, perhaps more relatable way of interpreting reactivity, is to imagine reading in a csv file to a table in R called, for example, my_table. If you were to update values in the csv file, the object my_table would not be updated; it would require the data to be re-read into R.\nReactive objects can live inside something called reactiveValues(), which acts like a list. There are 2 ways to create / update this:\n\nCreate the reactive list using my_reactive_list \\&lt;- reactiveValues(), then update values in that list using \\$ (for example my_reactive_list$somedata &lt;- c(1,20,23,42,98))\nCreate the items while creating the list (for example my_reactive_list &lt;- reactiveValues(somedata = c(1,20,23,42,98)))\n\nTo then use this data, simply call the object from the output you would like to use it for. For example, ggplot(my_reactive_list$somedata). When somedata changes, so will the plot output.\nIt is worth noting that, just like lists, items within the reactiveValues() object are not limited to vectors.\nMore about reactivity can be found on the Shiny page:\nhttps://shiny.rstudio.com/articles/understanding-reactivity.html\nCode Here is the code I used to create this app:\nhttps://github.com/danyaalmohamed/R/blob/master/animatedshiny.r\nThough the code is long, a large proportion of that is for inputs and layouts; a basic animated graph can be created using some more simple code:\nData in reactiveValues() form A plot output A trigger, you could use an actionButton(), or a reactiveTimer()Shiny consists of a ui and a server element. Any visuals (for example a button, a plotoutput()) should be held within the ui, then any functions or calculations which happen behind the scenes should live within the server function.\nI generally find it useful to use a series of small functions in R, and especially in Shiny. Instead of putting all the code inside of an actionButton, calling a function allows scope to implement it with different triggers, such as a reactiveTimer, or looping. It also allows for easier reading, in my opinion.\nWhen initially attempting to create the app, I was taken back by the complexity of Shiny, and faced a steep learning curve. I took a few days to learn the basics, and after some practice I feel I know what is available in the Shiny toolbox, and some ways to work around problems.\nDan Mohamed, Undergraduate Analyst at the NHS Wales Delivery Unit.\nThis blog has been formatted to remove Latin Abbreviations and edited for NHS-R Style.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/the-operator.html",
    "href": "blog/the-operator.html",
    "title": "The :: operator",
    "section": "",
    "text": "Most of the functionality in R comes from additional packages that you load. Sometimes two packages will have a function with the same name but they will do different things. In a situation where you have multiple packages with functions with the same name loaded, R will use the the function from the package you loaded the latest. As you can imagine, this can sometimes create problems. If you are lucky, you get an error message but if you are unlucky your code runs but with an unexpected result.\nLet me give you an example. I always load the {dplyr} package. Look what happens when I use summarize to calculate the mean sepal length by species.\n\nlibrary(dplyr)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.2\n#&gt;\n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt;\n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt;\n#&gt;     intersect, setdiff, setequal, union\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(sepal_length_mean = mean(Sepal.Length))\n#&gt; # A tibble: 3 × 2\n#&gt;   Species    sepal_length_mean\n#&gt;   &lt;fct&gt;                  &lt;dbl&gt;\n#&gt; 1 setosa                  5.01\n#&gt; 2 versicolor              5.94\n#&gt; 3 virginica               6.59\n\nCreated on 2024-02-21 with reprex v2.1.0\nSay that I then realise that I need the {Hmisc} package and load it. Look what happens when I rerun the same code as above.\n\nlibrary(Hmisc)\n#&gt; Warning: package 'Hmisc' was built under R version 4.3.2\n#&gt;\n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt;\n#&gt;     format.pval, units\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(sepal_length_mean = mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nR is now using the summarize function from the {Hmisc} package and I get an error because the syntax is wrong. The best way to solve this problem is to use the :: operator. Writing packagename::functionName tells R which package to get the function from.\n\niris3 &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarize(sepal_length_mean = mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nThis blog has been formatted to follow NHS-R Style"
  },
  {
    "objectID": "blog/the-operator.html#namespace-issues",
    "href": "blog/the-operator.html#namespace-issues",
    "title": "The :: operator",
    "section": "",
    "text": "Most of the functionality in R comes from additional packages that you load. Sometimes two packages will have a function with the same name but they will do different things. In a situation where you have multiple packages with functions with the same name loaded, R will use the the function from the package you loaded the latest. As you can imagine, this can sometimes create problems. If you are lucky, you get an error message but if you are unlucky your code runs but with an unexpected result.\nLet me give you an example. I always load the {dplyr} package. Look what happens when I use summarize to calculate the mean sepal length by species.\n\nlibrary(dplyr)\n#&gt; Warning: package 'dplyr' was built under R version 4.3.2\n#&gt;\n#&gt; Attaching package: 'dplyr'\n#&gt; The following objects are masked from 'package:stats':\n#&gt;\n#&gt;     filter, lag\n#&gt; The following objects are masked from 'package:base':\n#&gt;\n#&gt;     intersect, setdiff, setequal, union\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(sepal_length_mean = mean(Sepal.Length))\n#&gt; # A tibble: 3 × 2\n#&gt;   Species    sepal_length_mean\n#&gt;   &lt;fct&gt;                  &lt;dbl&gt;\n#&gt; 1 setosa                  5.01\n#&gt; 2 versicolor              5.94\n#&gt; 3 virginica               6.59\n\nCreated on 2024-02-21 with reprex v2.1.0\nSay that I then realise that I need the {Hmisc} package and load it. Look what happens when I rerun the same code as above.\n\nlibrary(Hmisc)\n#&gt; Warning: package 'Hmisc' was built under R version 4.3.2\n#&gt;\n#&gt; Attaching package: 'Hmisc'\n#&gt; The following objects are masked from 'package:base':\n#&gt;\n#&gt;     format.pval, units\n\niris %&gt;%\n  group_by(Species) %&gt;%\n  summarize(sepal_length_mean = mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nR is now using the summarize function from the {Hmisc} package and I get an error because the syntax is wrong. The best way to solve this problem is to use the :: operator. Writing packagename::functionName tells R which package to get the function from.\n\niris3 &lt;- iris %&gt;%\n  group_by(Species) %&gt;%\n  dplyr::summarize(sepal_length_mean = mean(Sepal.Length))\n#&gt; Error in iris %&gt;% group_by(Species) %&gt;% dplyr::summarize(sepal_length_mean = mean(Sepal.Length)): could not find function \"%&gt;%\"\n\nCreated on 2024-02-21 with reprex v2.1.0\nThis blog has been formatted to follow NHS-R Style"
  },
  {
    "objectID": "blog/nhsrdatasets-meets-runcharter.html",
    "href": "blog/nhsrdatasets-meets-runcharter.html",
    "title": "NHSRdatasets meets runcharter",
    "section": "",
    "text": "Background\nThe {NHSRdatasets} package made it to CRAN recently, and as it is designed for use by NHS data analysts, and I am an NHS data analyst, let’s take a look at it. Thanks to Chris Mainey and Tom Jemmett for getting this together.\nLoad packages and data\nAs above let’s load what we need for this session. The {runcharter} package is built using {data.table}, but I’m using {dplyr} in this main section to show that you don’t need to know {data.table} to use it.\n\n\n\n\n\n\nInstalling from GitHub\n\n\n\n\n\nSome packages, like {runcharter} are not on CRAN and can be installed using another package, in this case {remotes} also needs to be installed.\n\n\n\n\nlibrary(NHSRdatasets)\nlibrary(runcharter) # remotes::install_github(\"johnmackintosh/runcharter\")\nlibrary(dplyr)\nlibrary(skimr)\n\nHowever- seriously, do take a look at {data.table}. It’s not as hard to understand as some might have you believe. A little bit of effort pays off. You can load the {runcharter} package from github using the {remotes} package. (I’ve managed to install it on Windows and Ubuntu. Mac OS? No idea, I have no experience of that).\n\nae &lt;- data(\"ae_attendances\") # a promise\nae &lt;- ae_attendances #  a string\nrm(ae_attendances) # just typing 'ae' brings it to life in the environment\n\nThat felt a bit glitchy. There has to be a sleeker way to load and assign a built in dataset but I couldn’t find one. Cursory google to Stackoverflow.\nLet’s have a look at the data:\n\nglimpse(ae)\n\nRows: 12,765\nColumns: 6\n$ period      &lt;date&gt; 2017-03-01, 2017-03-01, 2017-03-01, 2017-03-01, 2017-03-0…\n$ org_code    &lt;fct&gt; RF4, RF4, RF4, R1H, R1H, R1H, AD913, RYX, RQM, RQM, RJ6, R…\n$ type        &lt;fct&gt; 1, 2, other, 1, 2, other, other, other, 1, other, 1, other…\n$ attendances &lt;dbl&gt; 21289, 813, 2850, 30210, 807, 11352, 4381, 19562, 17414, 7…\n$ breaches    &lt;dbl&gt; 2879, 22, 6, 5902, 11, 136, 2, 258, 2030, 86, 1322, 140, 0…\n$ admissions  &lt;dbl&gt; 5060, 0, 0, 6943, 0, 0, 0, 0, 3597, 0, 2202, 0, 0, 0, 3360…\n\n\nLot’s of factors. I’m actually very grateful for this package, as it caused me major issues when I first tried to plot this data using an earlier version of {runcharter.} I hadn’t considered factors as a possible grouping variable, which was a pretty big miss, as all the facets were out of order. All sorted now.\nThere’s way too much data for my tiny laptop screen, so I will filter the data for type 1 departments – the package help gives us a great link to explain what this means\n\ntype1 &lt;- ae %&gt;%\n  filter(type == 1) %&gt;%\n  arrange(period)\n\n# plot attendances\np &lt;- runcharter(type1,\n  med_rows = 13, # median of first 13 points\n  runlength = 9, # find a run of 9 consecutive points\n  direction = \"above\", # find run above the original median\n  datecol = period,\n  grpvar = org_code,\n  yval = attendances\n)\n\nThe runcharter function returns both a plot, and a data.table/ data.frame showing a summary of any runs in the desired direction (I’m assuming folk reading this have a passing knowledge of run charts, but if not, you can look at the package vignette, which is the cause of most of my commits!!)\nDon’t try loading the plot right now, because it is huge, and takes ages. If we look at the summary dataframe, we can see 210 rows, a fairly decent portion of which relate to significant increases above the original median value\n\np$sustained\n\n     org_code median start_date   end_date  extend_to  run_type\n        &lt;ord&gt;  &lt;num&gt;     &lt;Date&gt;     &lt;Date&gt;     &lt;Date&gt;    &lt;char&gt;\n  1:      R0A  21430 2017-10-01 2018-10-01 2019-03-01  baseline\n  2:      R1F   3477 2016-04-01 2017-04-01 2017-05-01  baseline\n  3:      R1H  28843 2016-04-01 2017-04-01 2019-03-01  baseline\n  4:      R1K  11733 2016-04-01 2017-04-01 2019-03-01  baseline\n  5:      RA2   5854 2016-04-01 2017-04-01 2018-03-01  baseline\n ---                                                           \n206:      RGN  12473 2018-05-01 2019-01-01 2019-03-01 sustained\n207:      RLT   6977 2018-03-01 2018-11-01 2019-03-01 sustained\n208:      RQ8   8456 2018-03-01 2018-11-01 2019-03-01 sustained\n209:      RTE  12610 2018-05-01 2019-01-01 2019-03-01 sustained\n210:      RVV  14582 2018-03-01 2018-11-01 2019-03-01 sustained\n\n\nLet’s use {skimr} to get a sense of this\n\nskimr::skim(p$sustained)\n\n\nData summary\n\n\nName\np$sustained\n\n\nNumber of rows\n210\n\n\nNumber of columns\n6\n\n\nKey\nNULL\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nDate\n3\n\n\nfactor\n1\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\nrun_type\n0\n1\n8\n9\n0\n2\n0\n\n\nVariable type: Date\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nmedian\nn_unique\n\n\n\nstart_date\n0\n1\n2016-04-01\n2018-07-01\n2016-04-01\n9\n\n\nend_date\n0\n1\n2017-04-01\n2019-03-01\n2017-04-01\n9\n\n\nextend_to\n0\n1\n2017-05-01\n2019-03-01\n2019-03-01\n7\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\norg_code\n0\n1\nTRUE\n139\nRA4: 3, RDD: 3, RDE: 3, RGN: 3\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\nmedian\n0\n1\n9389.8\n4317.54\n3477\n6468.25\n8413\n11311.25\n29102\n▇▅▁▁▁\n\n\n\n\nTo keep this manageable, I’m going to filter out for areas that have median admissions &gt; 10000 (based on the first 13 data points)\n\nhigh_admits &lt;- p$sustained %&gt;%\n  filter(median &gt; 10000 & run_type == \"sustained\") %&gt;%\n  select(org_code)\n\nThen I change the org_code from factor to character, and pull out unique values. I’m sure there is a slicker way of doing this, but it’s getting late, and I don’t get paid for this..\nI use the result to create a smaller data frame\n\nhigh_admits$org_code &lt;- as.character(high_admits$org_code)\n\ntype1_high &lt;- type1 %&gt;%\n  filter(org_code %in% high_admits$org_code)\n\nAnd now I can produce a plot that fits on screen. I’ve made the individual scales free along the y axis, and added titles and so on\n\np2 &lt;- runcharter(type1_high,\n  med_rows = 13, # median of first 13 points as before\n  runlength = 9, # find a run of 9 consecutive points\n  direction = \"above\",\n  datecol = period,\n  grpvar = org_code,\n  yval = attendances,\n  facet_scales = \"free_y\",\n  facet_cols = 4,\n  chart_title = \"Increased attendances in selected Type 1 AE depts\",\n  chart_subtitle = \"Data covers 2016/17 to 2018/19\",\n  chart_caption = \"Source : NHSRDatasets\",\n  chart_breaks = \"6 months\"\n)\n\nLet’s look at the sustained dataframe\n\np2$sustained\n\n    org_code median start_date   end_date  extend_to  run_type\n       &lt;ord&gt;  &lt;num&gt;     &lt;Date&gt;     &lt;Date&gt;     &lt;Date&gt;    &lt;char&gt;\n 1:      RCB   9121 2016-04-01 2017-04-01 2018-03-01  baseline\n 2:      RDD  11249 2016-04-01 2017-04-01 2017-05-01  baseline\n 3:      RDE   7234 2016-04-01 2017-04-01 2017-05-01  baseline\n 4:      RGN   7912 2016-04-01 2017-04-01 2017-05-01  baseline\n 5:      RJ1  12240 2016-04-01 2017-04-01 2018-03-01  baseline\n 6:      RJE  14568 2016-04-01 2017-04-01 2018-05-01  baseline\n 7:      RJL  11262 2016-04-01 2017-04-01 2018-03-01  baseline\n 8:      RQM  16478 2016-04-01 2017-04-01 2018-03-01  baseline\n 9:      RRK   9584 2016-04-01 2017-04-01 2018-03-01  baseline\n10:      RTE  11303 2016-04-01 2017-04-01 2017-05-01  baseline\n11:      RTG  11344 2016-04-01 2017-04-01 2018-07-01  baseline\n12:      RTR  10362 2016-04-01 2017-04-01 2018-03-01  baseline\n13:      RVV  12700 2016-04-01 2017-04-01 2017-05-01  baseline\n14:      RW6  22114 2016-04-01 2017-04-01 2017-05-01  baseline\n15:      RWE  12275 2016-04-01 2017-04-01 2017-05-01  baseline\n16:      RWF  11939 2016-04-01 2017-04-01 2018-03-01  baseline\n17:      RWP   9976 2016-04-01 2017-04-01 2018-03-01  baseline\n18:      RXC   9396 2016-04-01 2017-04-01 2018-03-01  baseline\n19:      RXH  12494 2016-04-01 2017-04-01 2018-03-01  baseline\n20:      RXP  10727 2016-04-01 2017-04-01 2017-05-01  baseline\n21:      RYR  11578 2016-04-01 2017-04-01 2018-03-01  baseline\n22:      RCB  10062 2018-03-01 2018-11-01 2019-03-01 sustained\n23:      RDD  12093 2017-05-01 2018-01-01 2018-03-01 sustained\n24:      RDE   7637 2017-05-01 2018-01-01 2018-03-01 sustained\n25:      RGN  11896 2017-05-01 2018-01-01 2018-05-01 sustained\n26:      RJ1  13570 2018-03-01 2018-11-01 2019-03-01 sustained\n27:      RJE  15183 2018-05-01 2019-01-01 2019-03-01 sustained\n28:      RJL  11972 2018-03-01 2018-11-01 2019-03-01 sustained\n29:      RQM  18560 2018-03-01 2018-11-01 2019-03-01 sustained\n30:      RRK  29102 2018-03-01 2018-11-01 2019-03-01 sustained\n31:      RTE  11772 2017-05-01 2018-01-01 2018-05-01 sustained\n32:      RTG  17169 2018-07-01 2019-03-01 2019-03-01 sustained\n33:      RTR  10832 2018-03-01 2018-11-01 2019-03-01 sustained\n34:      RVV  13295 2017-05-01 2018-01-01 2018-03-01 sustained\n35:      RW6  22845 2017-05-01 2018-01-01 2019-03-01 sustained\n36:      RWE  18173 2017-05-01 2018-01-01 2019-03-01 sustained\n37:      RWF  12793 2018-03-01 2018-11-01 2019-03-01 sustained\n38:      RWP  10358 2018-03-01 2018-11-01 2019-03-01 sustained\n39:      RXC  10279 2018-03-01 2018-11-01 2019-03-01 sustained\n40:      RXH  13158 2018-03-01 2018-11-01 2019-03-01 sustained\n41:      RXP  11314 2017-05-01 2018-01-01 2019-03-01 sustained\n42:      RYR  11970 2018-03-01 2018-11-01 2019-03-01 sustained\n43:      RDD  12776 2018-03-01 2018-11-01 2019-03-01 sustained\n44:      RDE  15322 2018-03-01 2018-11-01 2019-03-01 sustained\n45:      RGN  12473 2018-05-01 2019-01-01 2019-03-01 sustained\n46:      RTE  12610 2018-05-01 2019-01-01 2019-03-01 sustained\n47:      RVV  14582 2018-03-01 2018-11-01 2019-03-01 sustained\n    org_code median start_date   end_date  extend_to  run_type\n\n\nAnd of course, the plot itself\n\np2$runchart\n\n\n\n\n\n\n\nI haven’t looked into the actual data too much, but there are some interesting little facets here – what’s the story with RDE, RRK and RTG for example? I don’t know which Trusts these codes represent, but they show a marked increase. Of course, generally, all areas show an increase at some point.\nThe RGN (top right) and RVV (mid left) show the reason why I worked on this package – we can see that there has been more than one run above the median. . Performing this analysis in Excel is not much fun after a while.\nThere is a lot more I can look at with this package, and we in the NHS-R community are always happy to receive more datasets for inclusion, so please contribute if you can.\nThis post was originally published on johnmackintosh.net but has kindly been re-posted to the NHS-R community blog.\nIt has also been formatted to remove Latin Abbreviations, edited for NHS-R Style and to ensure running of code in Quarto.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/sql-server-database-connections-in-r.html",
    "href": "blog/sql-server-database-connections-in-r.html",
    "title": "SQL Server Database connections in R",
    "section": "",
    "text": "One of the things I found most difficult when learning R was getting data from our SQL Servers into R to analyse. It is easy to load csv files, or pull example datasets from packages, but a little more knowledge is required to connect to external databases. I think this is a common problem for my NHS colleagues when learning R and probably for others too. This post is a brief introduction to the two main ways to pull data in to R using {RODBC} and using {dplyr}‘s ’DBI-compliant’ database connections. I’ll be talking about connections with Microsoft SQL Server (over a local network), but this can also extend to other types of database by using different drivers, or other network set-ups using the right authentication."
  },
  {
    "objectID": "blog/sql-server-database-connections-in-r.html#rodbc",
    "href": "blog/sql-server-database-connections-in-r.html#rodbc",
    "title": "SQL Server Database connections in R",
    "section": "RODBC",
    "text": "RODBC\nThis is the simpler of the two interfaces, and uses slightly older code. It can be used to connect to anything that uses Open Database Connectivity (ODBC). I’ll define a connection string to a database server, a database, and a table called ‘MyTable’ that has some dummy data in it. If you haven’t got any of the packages used in this post, you can install them with: install.packages(\"RODBC\") for example.\n\nlibrary(\"RODBC\") \n\n#Connection string \nRODBC_connection &lt;- odbcDriverConnect(paste('driver={SQL \n                                      Server};server=',                                       \n                                      Sys.getenv(\"SERVER\"),                                       \n                                      ';database=',                                       \n                                      Sys.getenv(\"DATABASE\"),                                       \n                                      ';trusted_connection=true', sep = \"\")) \n\n# for example with a server called \"Cliff\" and a database called \"Richard\" your string would be: \n# driver = {SQL Server}; server = Cliff; database=Richard; trusted_connection = true')                            \n\ndt1 &lt;- sqlFetch(channel=RODBC_connection, sqtable = \"MyTable\") \n\n# Load data from SQL query \ndt2 &lt;- sqlQuery(channel = RODBC_connection, query = \"select TOP 100 * from MyTable\") \n\nQuite straightforward to use! In the example above, I specified trusted_connection = true. In a windows environment, this passes your windows credentials to the server. Since we use these for access permissions on our SQL Servers, we can use this method with no issues. You can, instead, specify a username (uid) and a password (pwd): see the help files for more details, using: ?odbcDriverConnect.\nYou can also use {RODBC} to write back to database tables, choosing whether or not to append your results using the append and safer arguments. Not appending means you will overwrite the table:\n\nsqlSave(channel = RODBC_connection, \n        dat = dt2, \n        tablename = \"Mytable_version2\", \n        append = FALSE, \n        safer = FALSE\n        ) \n\nThere are lots of other functions included with {RODBC} to allow you to see structures and so on. The package vignette is a very helpful place to go for this, along with the help files.\nRemember to disconnect at the end of your session:\n\nodbcClose(RODBC_connection) \n\nIf you do this a lot, you might find Gary Hutson’s recent post, showing how to wrap some of this into a function, a useful addition. Check it out here: http://hutsons-hacks.info/rodbc-helper-function."
  },
  {
    "objectID": "blog/sql-server-database-connections-in-r.html#dbi-dplyr",
    "href": "blog/sql-server-database-connections-in-r.html#dbi-dplyr",
    "title": "SQL Server Database connections in R",
    "section": "DBI dplyr",
    "text": "DBI dplyr\nThe {RODBC} interface was simple, quick, and you may not need to consider another approach, but I prefer to use the {tidyverse} functions linked to {dplyr}. These functions are maturing in the last couple of years, and have a few major advantages:\n\nWork with {tidyverse} functions, including {dplyr} verbs and the pipe %&gt;%\n\nFaster than RODBC to import data\n\nCan be used to work with data in the database, without importing it into R.\n\nThe connection string is slightly different, and we require a few more packages to make this work. You need to make sure you have the following installed:\n\n{dplyr} – to make the tbl and use it, we’ll work with dplyr syntax. = {DBI} – a common Database Interface engine for use in S and R (see: https://cran.r-project.org/web/packages/DBI/vignettes/DBI-1.html)\n{dbplyr} – this add-on package allows translation from {dplyr} to SQL.\n{odbc} – provides the odbc drivers, but you could use the functions below with other drivers instead.\n\n\nlibrary(dplyr) \nlibrary(dbplyr) \nlibrary(odbc) \nlibrary(DBI) \n\nDBI_Connection &lt;- dbConnect(odbc(),                        \n                      driver = \"SQL Server\",                       \n                      server = Sys.getenv(\"SERVER\"),                                           \n                      database = Sys.getenv(\"DATABASE\") \n) \n\nNow we can define a table as if it was part of our R workspace, using the connection object and the names of the table in the database. We can then interact with it directly using {dplyr}. glimpse() is a useful function that shows you the column names, datatypes and top few entries:\n\nMyTable &lt;- tbl(DBI_Connection, \"MyTable\")\n\nglimpse(MyTable)\n## Observations: ?? \n## Variables: 7 \n## Database: Microsoft SQL Server  \n## $ id         &lt;int&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16,... \n## $ Org        &lt;chr&gt; \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"G\", \"H\", \"I\", \"J\", \"... \n## $ year       &lt;int&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2015, 201... \n## $ month      &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, ... \n## $ Category_1 &lt;dbl&gt; 35395, 21126, 9248, 4049, 5323, 16867, 9916, 12404,... \n## $ Category_2 &lt;dbl&gt; 39293, 24860, 11031, 5812, 6876, 18475, 12976, 1657... \n## $ events     &lt;int&gt; 1986, 429, 460, 301, 289, 1172, 446, 756, 663, 874,... \n\nMyTable %&gt;%   \n  filter(year ==2015) %&gt;%   \n  group_by(month) %&gt;%   \n  summarise(AvgEvents = mean(events),             \n            MaxEvents = max(events),             \n            N = n()) %&gt;%   \n  arrange(month) \n## # Source:     lazy query [?? x 4] \n## # Database:   Microsoft SQL Server \n## # Ordered by: month ##   month AvgEvents MaxEvents     N \n##   &lt;int&gt;     &lt;int&gt;     &lt;int&gt; &lt;int&gt; \n## 1     4       624      1986    25 \n## 2     5       658      1948    25 \n## 3     6       671      2068    25 \n## 4     7       669      1879    25 \n## 5     8       630      1981    25 \n## 6     9       649      2011    25 \n\n{dplyr} can then be used to do fairly complex things in just a few lines. The example below is not very well thought-out, but it takes data from the database into a summary plot in just a few lines. I’m filtering the data for 2015 and passing it directly into {ggplot2}. I’ve set a few options for a box plot, but it’s quite minimal, and our data has remained in the database and not been imported to R.\n\nlibrary(ggplot2)\n\nMyTable %&gt;%\n  filter(year == 2015) %&gt;%\n  ggplot(aes(y = events, x = factor(month), group = factor(month))) +\n  geom_boxplot(fill = \"dodgerblue2\", alpha = 0.6, ) +\n  labs(title = \"Monthly Distribution of Events\", x = \"Month\", y = \"Events\")\n\n\nYou can, of course, write an SQL query directly using the dbSendQuery() function. This executes the query on the server-side only, so if you want the results to be returned back to R, you need to use dbFetch() as well. You might need this approach if you are doing fancy queries, or things that are specific to a database environment that don’t yet have translators in {dplyr}.\n\nSomeRecords &lt;- dbFetch(dbSendQuery(DBI_Connection, \"Select TOP 100 * from MyTable\"))\n\n# or\n\nSomeRecords &lt;- dbSendQuery(DBI_Connection, \"Select TOP 100 * from MyTable\") %&gt;%\n  dbFetch()\n\nYou may not need to write a custom query for everyday use, but you are still likely to need to pull the data from the server into memory in R sometimes. For me, this is often to build models on it, as that isn’t supported in-database. You can use the collect() function for this. For example, using part of the query from earlier as an example:\n\nMyTable_local&lt;- MyTable %&gt;%   \n  filter(year ==2015) %&gt;%   \n  group_by(month) %&gt;%   \n  summarise(AvgEvents = mean(events),             \n            MaxEvents = max(events),             \n            N = n()) %&gt;%   \n  arrange(month) %&gt;%   \n  collect() \n\nprint(MyTable_local)  \n## # A tibble: 6 x 4 \n##   month AvgEvents MaxEvents     N \n##   &lt;int&gt;     &lt;int&gt;     &lt;int&gt; &lt;int&gt; \n## 1     4       624      1986    25 \n## 2     5       658      1948    25 \n## 3     6       671      2068    25 \n## 4     7       669      1879    25 \n## 5     8       630      1981    25 \n## 6     9       649      2011    25   \n\nYou can also write back to a database using the dbWriteTable() function. The following code snippet writes a new table to my current connection, called NewDatabaseTable using the local data.frame called MyTable_local (that we created in the last section). The append option indicates whether to add to an existing table or not, and overwrite is just what it sounds like:\n\ndbWriteTable(DBI_Connection, \"NewDatabaseTable\", MyTable_local, overwrite = TRUE)"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html",
    "href": "blog/building-the-ons-mortality-dataset.html",
    "title": "Building the ONS mortality dataset",
    "section": "",
    "text": "This blog was originally a vignette for the NHSRdatasets package that I wrote when I had only been using R for about a year and I found myself contributing to a package! I produced this vignette which detailed all the laborious steps I took to take the wide data sets for provisionally recorded deaths up to 2019 and made them long form.\nI had been using these spreadsheets regularly for a mortality report in the Trust I worked in, but this data became incredibly important from 2020 (as well as changed format yet again) and so I’m sure there are better way to clean it and would be much easier to find than it was back before 2020. For the sake of transparency and showing how code changes over time I’m going to share it in its original state here and perhaps it can be its own training challenge as you free to improve or correct things that inevitably go out of date.\nThe data output stored in NHSRdatasets won’t be changed and instead will have vignettes built to use the data and help explain and explore it."
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#the-original-vignette",
    "href": "blog/building-the-ons-mortality-dataset.html#the-original-vignette",
    "title": "Building the ONS mortality dataset",
    "section": "The original vignette",
    "text": "The original vignette\nThis vignette details why and how the ons_mortality dataset was created.\nThe data were pulled together from the weekly xls spreadsheets provided by the Office of National Statistics (ONS) from 2010 to 2019 for training purposes and is a static data set.\nData is released every Friday and includes provisionally registered deaths for England and Wales for:\n\nTotal, all deaths\nregions of usual residence\nage bands (persons and for males/females)\nby underlying cause ‘All respiratory diseases’ (the methodology has changed over the years, see the subsection for more information)\n\nFurther notes taken from the spreadsheet:\n1 This average is based on the actual number of death registrations recorded for each corresponding week over the previous five years. Moveable public holidays, when register offices are closed, affect the number of registrations made in the published weeks and in the corresponding weeks in previous years.\n2 Counts of deaths by underlying cause exclude deaths at age under 28 days.\n3 Coding of deaths by underlying cause for the latest week is not yet complete.\n4 Does not include deaths where age is either missing or not yet fully coded. For this reason counts of ‘Persons’, ‘Males’ and ‘Females’ may not sum to ‘Total Deaths, all ages’.\n5 Does not include deaths of those resident outside England and Wales or those records where the place of residence is either missing or not yet fully coded. For this reason counts for “Deaths by Region of usual residence” may not sum to “Total deaths, all ages”.\nLater confirmation of deaths\nAggregate numbers of deaths are later confirmed and released in by ONS monthly. Numbers are released at a lower geographical level to the weekly data."
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#preparation-of-data",
    "href": "blog/building-the-ons-mortality-dataset.html#preparation-of-data",
    "title": "Building the ONS mortality dataset",
    "section": "Preparation of data",
    "text": "Preparation of data\nIn order to prepare this data for analysis the worksheet for weekly figures was extracted and the data was moved to long form and merged together over the available years. This was extracted in April 2020 when full previous years’ data was available.\nThe dataset contains:\n\n\ncategory_1: character, containing the names of the groups for counts, e.g. “Total deaths”, “all ages”.\n\ncategory_2: character, subcategory of names of groups where necessary, e.g. details of region: “East”, details of age bands “15-44”.\n\ncounts: numeric, numbers of deaths in whole numbers and average numbers with decimal points. To retain the integrity of the format this column data is left as character.\n\ndate: date, format is yyyy-mm-dd; all dates are a Friday.\n\nweek_no: integer, each week in a year is numbered sequentially."
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#creating-the-dataset",
    "href": "blog/building-the-ons-mortality-dataset.html#creating-the-dataset",
    "title": "Building the ONS mortality dataset",
    "section": "Creating the dataset",
    "text": "Creating the dataset\nSpreadsheets are in xls format from 2010 to 2019.\nEach week is released with a new file name listing the week number and all weeks for that year are included in the spreadsheet. Counts for weeks ahead are missing and listed as NA.\nWhen a year has passed it is renamed to the year name and does not change.\n\n# 2019\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2019/publishedweek522019.xls\",\n  destfile = \"2019Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2018\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2018/publishedweek522018withupdatedrespiratoryrow.xls\",\n  destfile = \"2018Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2017\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2017/publishedweek522017.xls\",\n  destfile = \"2017Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2016\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2016/publishedweek522016.xls\",\n  destfile = \"2016Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2015\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2015/publishedweek2015.xls\",\n  destfile = \"2015Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2014\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2014/publishedweek2014.xls\",\n  destfile = \"2014Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2013\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2013/publishedweek2013.xls\",\n  destfile = \"2013Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2012\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2012/publishedweek2012.xls\",\n  destfile = \"2012Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2011\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2011/publishedweek2011.xls\",\n  destfile = \"2011Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")\n\n# 2010\ndownload.file(\n  \"https://www.ons.gov.uk/file?uri=%2fpeoplepopulationandcommunity%2fbirthsdeathsandmarriages%2fdeaths%2fdatasets%2fweeklyprovisionalfiguresondeathsregisteredinenglandandwales%2f2010/publishedweek2010.xls\",\n  destfile = \"2010Mortality.xls\",\n  method = \"wininet\",\n  mode = \"wb\")"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#extracting-worksheets-to-csv",
    "href": "blog/building-the-ons-mortality-dataset.html#extracting-worksheets-to-csv",
    "title": "Building the ONS mortality dataset",
    "section": "Extracting worksheets to csv",
    "text": "Extracting worksheets to csv\nThe data can be found in the worksheet called Weekly figures “date/year”. Other worksheets in spreadsheets 2010 to 2019 are:\n\nContents\nInformation\nTerms and conditions\nWeekly figures \n\nRelated publications\n\nThe following code finds the tab name and then creates a csv file for each tab/worksheet.\n\nlibrary(readxl)\nlibrary(dplyr)\n\nfiles_list &lt;- list.files(path = \"working_files\",\n                         pattern = \"*.xls\",\n                         full.names = TRUE)\n\n\nread_then_csv &lt;- function(sheet, path) {\n  pathbase &lt;- path %&gt;%\n    basename() %&gt;%\n    tools::file_path_sans_ext()\n  path %&gt;%\n    read_excel(sheet = sheet) %&gt;%\n    write_csv(paste0(pathbase, \"-\", sheet, \".csv\"))\n}\n\n\nfor(j in 1:length(files_list)){\n\n  path &lt;- paste0(files_list[j])\n\n  path %&gt;%\n    excel_sheets() %&gt;%\n    set_names() %&gt;%\n    map(read_then_csv, path = path)\n}"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#loading-the-weekly-figure-worksheets",
    "href": "blog/building-the-ons-mortality-dataset.html#loading-the-weekly-figure-worksheets",
    "title": "Building the ONS mortality dataset",
    "section": "Loading the weekly figure worksheets",
    "text": "Loading the weekly figure worksheets\nFrom 2010 to 2015 the tab name was Weekly Figures then it changed capitalisation to Weekly figures.\n\nfiles_list_sheets &lt;- list.files(path = \"working_files\",\n                         pattern = \"Weekly\",\n                         full.names = TRUE\n                         )\n\nfor(i in files_list_sheets) {\n\n  x &lt;- read_csv((i), col_types = cols(.default = col_character()))\n\n  assign(i, x)\n}"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#format-data-functions",
    "href": "blog/building-the-ons-mortality-dataset.html#format-data-functions",
    "title": "Building the ONS mortality dataset",
    "section": "Format data functions",
    "text": "Format data functions\nTwo functions were required to format the spreadsheets as there were changes to the layout from 2016.\nPackages used:\n\n\njanitor: a specific package for cleaning data from Excel used here to remove the blank lines and columns, convert Excel serial numbers to dates and to format all column headers by removing spaces in those header names and removing capitalisation.\n\ndplyr: specifically dplyr for data manipulation (already loaded earlier).\n\ntidyr: for pivoting data from wide to long.\n\nstringi: a package to works with strings and was used to find certain characters or words.\n\nlubridate: a package to work with dates and used here to format dates to the same format.\n\n2010 - 2015\n\nlibrary(janitor)\nlibrary(tidyr)\nlibrary(stringi)\nlibrary(lubridate)\n\n# Column names that are not related to data points to be removed. This is the same for all years' spreadsheets.\n# Note that single quotes are used for the categories as one sentence includes '' in the text (4). \n\nremove_lookup &lt;- c('week over the previous five years1',\n  'Deaths by underlying cause2,3',\n  'Footnotes',\n  '1 This average is based on the actual number of death registrations recorded for each corresponding week over the previous five years. Moveable public holidays, when register offices are closed, affect the number of registrations made in the published weeks and in the corresponding weeks in previous years.',\n  '2 Counts of deaths by underlying cause exclude deaths at age under 28 days.',\n  '3 Coding of deaths by underlying cause for the latest week is not yet complete.',\n  \"4Does not include deaths where age is either missing or not yet fully coded. For this reason counts of 'Persons', 'Males' and 'Females' may not sum to 'Total Deaths, all ages'.\",\n  '5 Does not include deaths of those resident outside England and Wales or those records where the place of residence is either missing or not yet fully coded. For this reason counts for \"Deaths by Region of usual residence\" may not sum to \"Total deaths, all ages\".',\n  'Source: Office for National Statistics',\n  'Deaths by age group'\n)\n\nformatFunction &lt;- function(file){\n\nONS &lt;- file %&gt;%\n  clean_names %&gt;%\n  remove_empty(c(\"rows\",\"cols\")) %&gt;%\n  filter(!contents %in% remove_lookup) %&gt;%\n  \n  # useful categories are found in the column contents but also include the footnote number\n  mutate(Category = case_when(is.na(x2) & str_detect(contents, \"Region\") ~ \"Region\",\n                              is.na(x2) & str_detect(contents, \"Persons\") ~ \"Persons\",\n                              is.na(x2) & str_detect(contents, \"Females\") ~ \"Females\",\n                              is.na(x2) & str_detect(contents, \"Males\") ~ \"Males\")\n  ) %&gt;%\n  select(contents, Category, everything()) %&gt;%\n  \n  # to ensure data like Persons, Males and Females \n  fill(Category) %&gt;%\n  \n  # categories with Persons, Males and Females in the original column do not correspond directly to data points (wide form data) so are removed by referring to str_detect to find the word\n  filter(!str_detect(contents, \"Persons\"),\n         !str_detect(contents, \"Males\"),\n         !str_detect(contents, \"Females\")) %&gt;%\n  \n  # the two columns for Category and contents are merged to Categories to bring the Category column first. Some categories don't have subcategories and these are preceded by NA_ with this merge\n  unite(\"Categories\", Category, contents) %&gt;%\n  filter(Categories != \"Region_Deaths by Region of usual residence 5\") %&gt;%\n  \n  # the NA_ is removed from some of the category names\n  mutate(Categories = case_when(str_detect(Categories, \"NA_\") ~ str_replace(Categories, \"NA_\", \"\"),\n                                TRUE ~ Categories))\n\n  # Push date row to column names\nonsFormattedJanitor &lt;- row_to_names(ONS, 3)\n\n  # move data from wide to long form using pivot_longer\nx &lt;- onsFormattedJanitor %&gt;%\n  pivot_longer(cols = -`Week ended`,\n               names_to = \"allDates\",\n               values_to = \"counts\") %&gt;%\n  \n  # some spreadsheets import with Excel serial numbers for dates and others as dates, janitor is used to correct this\n  mutate(realDate = dmy(allDates),\n         ExcelSerialDate = case_when(stri_length(allDates) == 5 ~ excel_numeric_to_date(as.numeric(allDates), date_system = \"modern\")),\n         date = case_when(is.na(realDate) ~ ExcelSerialDate,\n                          TRUE ~ realDate)) %&gt;%\n  group_by(`Week ended`) %&gt;%\n  \n  # the week number is replaced as this was lost with the moving of dates to the column headers\n  mutate(week_no = row_number()) %&gt;%\n  ungroup() %&gt;%\n  \n  # Category is a staging name as this is followed by a splitting of the column into category_1 and category_2\n  rename(Category = `Week ended`) %&gt;%\n  \n  # to split the columns there are various characters used as a split point \",\", \"-\", and \":\" in the respiratory category the version is denoted by \"v\"\n  mutate(category_1 = case_when(str_detect(Category, \",\") ~\n                                  substr(Category,1,str_locate(Category, \",\") -1),\n                                str_detect(Category, \":\") ~\n                                  substr(Category,1,str_locate(Category, \":\") -1),\n                                str_detect(Category, \"_\") ~\n                                  substr(Category,1,str_locate(Category, \"_\") -1),\n                                str_detect(Category, \"respiratory\")  ~\n                                  \"All respiratory diseases (ICD-10 J00-J99) ICD-10\"),\n         category_2 = case_when(str_detect(Category, \",\") ~\n                                  substr(Category,str_locate(Category, \", \") +2, str_length(Category)),\n                                str_detect(Category, \":\") ~\n                                  substr(Category,str_locate(Category, \": \") +2, str_length(Category)),\n                                str_detect(Category, \"_\") ~\n                                  substr(Category,str_locate(Category, \"_\") +1, str_length(Category)),\n                                str_detect(Category, \"respiratory\")  ~\n                                  substr(Category,str_locate(Category, \"v\"), str_length(Category)) ),\n         \n         # the data for Total deaths: average of corresponding week over the previous 5 years is split over two cells in the spreasheet\n         category_2 = recode(category_2,\n                             \"average of corresponding\" = \"average of same week over 5 years\")) %&gt;%\n  select(category_1,\n         category_2,\n         counts,\n         date,\n         week_no\n         ) %&gt;%\n  \n  # 2011 requires this code to remove rows where there are no counts and because there are 2 rows relating to respiratory death numbers (see the Respiratory section) the previous methodology has been included in the spreadsheet with : for counts. This code does not affect other years' data/\n  filter(!is.na(counts),\n         counts != \":\") %&gt;%\n  fill(category_1)\n\nreturn(x)\n\n}\n\nMortality2010 &lt;- formatFunction(`working_files/Weekly/2010Mortality-Weekly Figures 2010.csv`)\n\n# 2011 has two lines relating to respiratory, v 2001 only has one data point and the rest of the year is 2010\nMortality2011 &lt;- formatFunction(`working_files/Weekly/2011Mortality-Weekly Figures 2011.csv`) %&gt;%\n  mutate(category_2 = case_when(is.na(category_2) & category_1 == \"All respiratory diseases (ICD-10 J00-J99) ICD-10\" ~ \"v 2010\",\n                                TRUE ~ category_2))\n\nMortality2012 &lt;- formatFunction(`working_files/Weekly/2012Mortality-Weekly Figures 2012.csv`)\nMortality2013 &lt;- formatFunction(`working_files/Weekly/2013Mortality-Weekly Figures 2013.csv`)\nMortality2014 &lt;- formatFunction(`working_files/Weekly/2014Mortality-Weekly Figures 2014.csv`)\nMortality2015 &lt;- formatFunction(`working_files/Weekly/2015Mortality-Weekly Figures 2015.csv`)\n\nFormat data 2016 - 2019\n\nformatFunction2016 &lt;- function(file){\n\n  ONS &lt;- file %&gt;%\n    clean_names %&gt;%\n    \n    # An extra column has been added for region codes (not included in the dataset) meaning contents are now found in the janitor generated column name x2\n    mutate(x2 = case_when(is.na(x2) ~ contents,\n                              TRUE ~ x2)) %&gt;%\n    remove_empty(c(\"rows\",\"cols\")) %&gt;%\n    select(-contents) %&gt;%\n    filter(!x2 %in% remove_lookup) %&gt;%\n    \n    # Region has changed to region\n    mutate(Category = case_when(is.na(x3) & str_detect(x2, \"region\") ~ \"Region\",\n                                is.na(x3) & str_detect(x2, \"Persons\") ~ \"Persons\",\n                                is.na(x3) & str_detect(x2, \"Females\") ~ \"Females\",\n                                is.na(x3) & str_detect(x2, \"Males\") ~ \"Males\",\n                                TRUE ~ NA_character_)\n    ) %&gt;%\n    select(x2, Category, everything()) %&gt;%\n    fill(Category) %&gt;%\n    filter(!str_detect(x2, 'Persons'),\n           !str_detect(x2, 'Males'),\n           !str_detect(x2, 'Females')) %&gt;%\n    unite(\"Categories\", Category, x2) %&gt;%\n    filter(Categories != 'Region_Deaths by Region of usual residence 5') %&gt;%\n    mutate(Categories = case_when(str_detect(Categories, \"NA_\") ~ str_replace(Categories, \"NA_\", \"\"),\n                                  TRUE ~ Categories))\n\n  # Push date row to column names\n  onsFormattedJanitor &lt;- row_to_names(ONS, 3)\n\n  # move data from wide to long form using pivot_longer\n  x &lt;- onsFormattedJanitor %&gt;%\n    pivot_longer(cols = -`Week ended`,\n                 names_to = \"allDates\",\n                 values_to = \"counts\") %&gt;%\n    mutate(realDate = dmy(allDates),\n           ExcelSerialDate = case_when(stri_length(allDates) == 5 ~ excel_numeric_to_date(as.numeric(allDates), date_system = \"modern\")),\n           date = case_when(is.na(realDate) ~ ExcelSerialDate,\n                            TRUE ~ realDate)) %&gt;%\n    group_by(`Week ended`) %&gt;%\n    mutate(week_no = row_number()) %&gt;%\n    ungroup() %&gt;%\n    rename(Category = `Week ended`) %&gt;%\n    mutate(category_1 = case_when(str_detect(Category, \",\") ~\n                                  substr(Category,1,str_locate(Category, \",\") -1),\n                                str_detect(Category, \":\") ~\n                                  substr(Category,1,str_locate(Category, \":\") -1),\n                                str_detect(Category, \"_\") ~\n                                  substr(Category,1,str_locate(Category, \"_\") -1),\n                                str_detect(Category, \"respiratory\")  ~\n                                  \"All respiratory diseases (ICD-10 J00-J99) ICD-10\"),\n         category_2 = case_when(str_detect(Category, \",\") ~\n                                  substr(Category,str_locate(Category, \", \") +2, str_length(Category)),\n                                str_detect(Category, \":\") ~\n                                  substr(Category,str_locate(Category, \": \") +2, str_length(Category)),\n                                str_detect(Category, \"_\") ~\n                                  substr(Category,str_locate(Category, \"_\") +1, str_length(Category)),\n                                str_detect(Category, \"respiratory\")  ~\n                                  substr(Category,str_locate(Category, \"v\"), str_length(Category)) ),\n         category_2 = recode(category_2,\n                             \"average of corresponding\" = \"average of same week over 5 years\")) %&gt;%\n    select(category_1,\n           category_2,\n           counts,\n           date,\n           week_no\n    ) %&gt;%\n    filter(!is.na(counts)) \n\n  return(x)\n\n}\n\n\nMortality2016 &lt;- formatFunction2016(`working_files/Weekly/2016Mortality-Weekly figures 2016.csv`)\nMortality2017 &lt;- formatFunction2016(`working_files/Weekly/2017Mortality-Weekly figures 2017.csv`)\nMortality2018 &lt;- formatFunction2016(`working_files/Weekly/2018Mortality-Weekly figures 2018.csv`)\nMortality2019 &lt;- formatFunction2016(`working_files/Weekly/2019Mortality-Weekly figures 2019.csv`)"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#bind-together",
    "href": "blog/building-the-ons-mortality-dataset.html#bind-together",
    "title": "Building the ONS mortality dataset",
    "section": "Bind together",
    "text": "Bind together\n\nons_mortality &lt;- do.call(\"rbind\", list(\n                      Mortality2010,\n                      Mortality2011,\n                      Mortality2012,\n                      Mortality2013,\n                      Mortality2014,\n                      Mortality2015,\n                      Mortality2016,\n                      Mortality2017,\n                      Mortality2018,\n                      Mortality2019))"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#load-the-data",
    "href": "blog/building-the-ons-mortality-dataset.html#load-the-data",
    "title": "Building the ONS mortality dataset",
    "section": "Load the data",
    "text": "Load the data\n\nlibrary(NHSRdatasets)\nlibrary(dplyr)\n\ndata(\"ons_mortality\")"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#respiratory",
    "href": "blog/building-the-ons-mortality-dataset.html#respiratory",
    "title": "Building the ONS mortality dataset",
    "section": "Respiratory",
    "text": "Respiratory\nDeaths by underlying cause of respiratory diseases All respiratory diseases (ICD-10 J00-J99) appear as:\n\nons_mortality %&gt;% \n  mutate(year = year(date)) %&gt;% \n  select(year, category_1, category_2) %&gt;% \n  group_by(year, category_1, category_2) %&gt;% \n  filter(category_1 == 'All respiratory diseases (ICD-10 J00-J99) ICD-10') %&gt;% \n  slice(1)\n\nNotes from the spreadsheets in relation to these methodology changes:\n2011\n\nRespiratory deaths for 2011 are coded to the new version of ICD-10 while for 2010 they are coded to the previous version. Week 1 2011 has been coded to both versions to give an indication of the impact of the change.\n2014\n\nFor deaths registered from January 2014, cause of death is coded to the ICD-10 classification using IRIS software. Further information about the implementation of the software is available on the ONS website.\nAll deaths registered in week 2 were dual-coded using both ICD-10 v2010 (NCHS) and ICD-10 v2013 (IRIS). An information note providing the preliminary findings on the impact of the implementation of IRIS software for ICD-10 cause of death coding on mortality statistics in England and Wales is available on the ONS website.\n2015\n\nFor deaths registered from January 2014, cause of death is coded to the ICD-10 classification using IRIS software. Further information about the implementation of the software is available on the ONS website."
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#age-bands",
    "href": "blog/building-the-ons-mortality-dataset.html#age-bands",
    "title": "Building the ONS mortality dataset",
    "section": "Age bands",
    "text": "Age bands\nNumbers are provided for categories: Persons, Females and Males.\nAs detailed in the notes, if the age is missing or not coded at the time of release then the counts of ‘Persons’, ‘Males’ and ‘Females’ may not sum to ‘Total Deaths, all ages’.\nNote that these age bands were changed in 2020 releases.\n\nons_mortality %&gt;% \n  select(category_1, category_2) %&gt;% \n  group_by(category_1, category_2) %&gt;% \n  filter(category_1 %in% c('Persons','Females','Males')) %&gt;% \n  slice(1)"
  },
  {
    "objectID": "blog/building-the-ons-mortality-dataset.html#remove-working_files-folder",
    "href": "blog/building-the-ons-mortality-dataset.html#remove-working_files-folder",
    "title": "Building the ONS mortality dataset",
    "section": "Remove working_files folder",
    "text": "Remove working_files folder\n\nunlink(\"working_files\", recursive = TRUE)"
  },
  {
    "objectID": "blog/exact-matching-in-r.html",
    "href": "blog/exact-matching-in-r.html",
    "title": "Exact Matching in R",
    "section": "",
    "text": "I’ve been working with a group of analysts in East London who are interested in joined-up health and social care data. They’ve created a powerful, unique dataset that shows how each resident of the London Borough of Barking & Dagenham interacts with NHS and council services. The possibilities are enormous, both in terms of understanding the way that people use services across a whole system, and for more general public health research.\nYou can download a sample of the data https://nhsrcommunity.com/wp-content/uploads/2019/07/sample_lbbd_dataset.csv\nToday we’re interested in whether social isolation is related to healthcare costs, and we’re going to use exact matching to explore this issue. Our theory is that people who live alone have higher healthcare costs because they have less support from family members.\nHere’s an extract from the data. ‘Total cost’ means the cost of health and social care in one year. The actual dataset has many more variables!\n\n\n\n\n\n\nID\nAge.group\nSex\nNumber.of.long-term.conditions\nLives.alone\nTotal.cost\n\n\n\n1\n50-64\nMale\n0\nNo\n£93\n\n\n2\n65-74\nFemale\n1\nYes\n£0\n\n\n3\n50-64\nMale\n4\nNo\n£1065\n\n\n4\n85+\nFemale\n5\nYes\n£7210\n\n\n\n\n\n\nFor the purposes of this blog, we’ll use a sample of 5000 (out of 114,000) individuals, with some changes to the values to ensure anonymity.\nDescriptive stats\nThe first thing we’ll look at is whether people who live alone are different. I find descriptive tables fiddly so often use a function that can be re-used across a number of variables:\n\n\n{data.table} imports the 5000 rows very quickly but note that the object created, d, is a data.table and will act differently to a data.frame or tibble.\n\n# change your working directory to the place you've saved it and build the path to it using the {here} package which locates the RProj file of a project rather than requiring `setwd()` which can often be difficult to use with network drives when the start `\\\\`\n\nlibrary(data.table) # useful for efficient reading and summarising of data\n\nd &lt;- fread(paste0(here::here(), \"/blog/data/sample_lbbd_dataset.csv\"))\n\n# describe age, sex and number of long-term conditions\ndescribe.by.isolation &lt;- function(variable) {\n  a &lt;- table(unlist(d[, variable, with = F]), d$livesalone)\n  p &lt;- round(prop.table(a, 2) * 100, 0)\n  matrix(paste0(a, \" (\", p, \")\"), ncol = 2, dimnames = list(row.names(a), c(\"Live with others\", \"Lives alone\")))\n}\n\nlapply(c(\"age_grp\", \"sex\", \"LTC_grp\"), describe.by.isolation)\n\n[[1]]\n      Live with others Lives alone\n50-64 \"2514 (65)\"      \"425 (37)\" \n65-74 \"825 (21)\"       \"277 (24)\" \n75-84 \"370 (10)\"       \"255 (22)\" \n85+   \"150 (4)\"        \"184 (16)\" \n\n[[2]]\n       Live with others Lives alone\nFemale \"1946 (50)\"      \"663 (58)\" \nMale   \"1913 (50)\"      \"478 (42)\" \n\n[[3]]\n   Live with others Lives alone\n0  \"1368 (35)\"      \"305 (27)\" \n1  \"1159 (30)\"      \"329 (29)\" \n2  \"727 (19)\"       \"228 (20)\" \n3+ \"605 (16)\"       \"279 (24)\" \n\n# describe healthcare costs\nd[, .(mean_cost = mean(cost), sd = sd(cost), med_cost = median(cost), iqr = IQR(cost)), livesalone]\n\n   livesalone mean_cost        sd med_cost      iqr\n       &lt;lgcl&gt;     &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:      FALSE  2367.962  7951.524  469.500 1272.878\n2:       TRUE  4715.150 11527.056  780.989 2899.131\n\npar(mfrow = c(2, 1), mar = c(3, 3, 2, 0))\n\nhist(log(d$cost[d$livesalone] + 1), main = \"Lives alone\", xlab = \"Log cost + 1\", col = \"red\", xlim = c(0, 14))\n\nhist(log(d$cost[!d$livesalone] + 1), main = \"Does not live alone\", xlab = \"Log cost + 1\", col = \"green\", xlim = c(0, 14))\n\n\n\n\n\n\n\nPeople who live alone are older, more likely to be female and have more long-term health problems. Their mean healthcare costs are £2,347 higher. The difference in healthcare costs is visible on the histograms, which we have displayed on a log scale because some values are extremely high. There’s some ‘zero inflation’ in both groups – people with no healthcare costs who do not fit into the otherwise lognormal-ish distribution. So far not too surprising – but are the increased healthcare costs explained by the differences in age, sex and health?\nRegression\nOne approach would be to use regression. We could fit a linear model – this is actually not a great fit for patients with higher healthcare costs, but we won’t go into that here.\n\nlinear_model &lt;- lm(cost ~ livesalone + age_grp + sex + LTC_grp, d)\n\nplot(linear_model) # diagnostics show that a linear model is not a great fit. You might have to press return to see all the plots before you can continue.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nsummary(linear_model)\n\n\nCall:\nlm(formula = cost ~ livesalone + age_grp + sex + LTC_grp, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-12101  -2540   -847    -97 167419 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       621.0      257.2   2.415  0.01579 *  \nlivesaloneTRUE    891.7      300.8   2.965  0.00305 ** \nage_grp65-74      287.6      307.5   0.935  0.34962    \nage_grp75-84     1928.9      395.0   4.884 1.07e-06 ***\nage_grp85+       5809.5      518.4  11.208  &lt; 2e-16 ***\nsexMale          -424.8      242.5  -1.752  0.07985 .  \nLTC_grp1          733.6      305.2   2.403  0.01629 *  \nLTC_grp2         2572.9      350.6   7.339 2.51e-13 ***\nLTC_grp3+        4977.0      372.0  13.381  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8522 on 4991 degrees of freedom\nMultiple R-squared:  0.09449,   Adjusted R-squared:  0.09304 \nF-statistic:  65.1 on 8 and 4991 DF,  p-value: &lt; 2.2e-16\n\n\nThe results suggest that health and care costs for people who live alone are £892 more than those who do not live alone, on average. Clearly the variables we added to the model are important confounders, as this is much lower than the ‘crude’ difference of £2,347. Two limitations of this approach are that we will have to think quite carefully about the fit of the model to the data, and that we can’t describe how living alone affects the distribution of costs.\nExact matching\nWe therefore used exact matching, in which each individual who lives alone is matched to an individual who does not live alone, based on specified variables such as age group and sex. We developed a function that does this by stratifying the data and then matching ‘cases’ and ‘controls’ randomly. Unmatched individuals are deleted from the dataset, leaving a dataset that is balanced in terms of the variables you specify. Let me know here if there’s any other functionality you want and we can try and incorporate it.\nLet’s try matching on age group, sex and the grouped number of long-term conditions:\n\n#' @param data dataset containing:\n#' @param treat treatment/exposure variable 'mvar' (a string specifying variable name).\n#' @param mvar matching variable 'mvar' (a string specifying variable name). If you want to match on multiple variables, concatenate them first.\n#' @param ratio controls (an integer &gt; 0)\n#' @param seed for fixing random selection of cases/controls (an integer; default NULL means no seed). Choice of seed is arbitrary.\n#'\n#' @return data.table of matched observations, with additional variable 'id' for use in paired/grouped analyses\n\nsmatch &lt;- function(data, treat, mvar, ratio = 1, seed = NULL) {\n  setnames(data, mvar, \".mvar\")\n  targ &lt;- data[, .(case = sum(get(treat)), control = sum(!get(treat))), .mvar]\n  targ[, cst := floor(pmin(control / ratio, case))]\n  targ[, cnt := cst * ratio]\n  targ &lt;- targ[cst &gt; 0]\n  l2 &lt;- cumsum(targ$cst)\n  ids &lt;- mapply(\":\", c(0, l2[-nrow(targ)]), l2 - 1)\n  names(ids) &lt;- targ$.mvar\n  case &lt;- NULL\n  control &lt;- NULL\n  \n  x &lt;- withr::with_preserve_seed(runif(seed))\n  \n  for (i in targ$.mvar) {\n    case[[i]] &lt;- data[get(treat) == T & .mvar == i][sample(.N, targ$cst[targ$.mvar == i])]\n    case[[i]][, id := ids[[i]]]\n    control[[i]] &lt;- data[get(treat) == F & .mvar == i][sample(.N, targ$cnt[targ$.mvar == i])]\n    control[[i]][, id := rep(ids[[i]], each = ratio)]\n  }\n  rbindlist(c(case, control))\n}\n\n# create a single variable summarising matching variables\nd$mvar &lt;- do.call(\"paste0\", d[, c(\"age_grp\", \"sex\", \"LTC_grp\")])\n\n# create 1:1 matched dataset.\nmatched_data &lt;- smatch(d, treat = \"livesalone\", mvar = \"mvar\", ratio = 1, seed = 74)\n\n# check balance: same number of individuals in each group\ndcast(matched_data, age_grp + sex + LTC_grp ~ livesalone, value.var = \"id\", fun.aggregate = length)\n\nKey: &lt;age_grp, sex, LTC_grp&gt;\n    age_grp    sex LTC_grp FALSE  TRUE\n     &lt;char&gt; &lt;char&gt;  &lt;char&gt; &lt;int&gt; &lt;int&gt;\n 1:   50-64 Female       0    62    62\n 2:   50-64 Female       1    65    65\n 3:   50-64 Female       2    31    31\n 4:   50-64 Female      3+    24    24\n 5:   50-64   Male       0   100   100\n 6:   50-64   Male       1    76    76\n 7:   50-64   Male       2    40    40\n 8:   50-64   Male      3+    27    27\n 9:   65-74 Female       0    39    39\n10:   65-74 Female       1    46    46\n11:   65-74 Female       2    31    31\n12:   65-74 Female      3+    41    41\n13:   65-74   Male       0    32    32\n14:   65-74   Male       1    34    34\n15:   65-74   Male       2    22    22\n16:   65-74   Male      3+    32    32\n17:   75-84 Female       0    30    30\n18:   75-84 Female       1    45    45\n19:   75-84 Female       2    38    38\n20:   75-84 Female      3+    58    58\n21:   75-84   Male       0    10    10\n22:   75-84   Male       1    24    24\n23:   75-84   Male       2    15    15\n24:   75-84   Male      3+    27    27\n25:     85+ Female       0    11    11\n26:     85+ Female       1    19    19\n27:     85+ Female       2    11    11\n28:     85+ Female      3+    23    23\n29:     85+   Male       0     5     5\n30:     85+   Male       1     8     8\n31:     85+   Male       2    13    13\n32:     85+   Male      3+    13    13\n    age_grp    sex LTC_grp FALSE  TRUE\n\n\nNow we have a dataset that is balanced in terms of age, sex and the count of long-term conditions. Let’s see how healthcare costs compare:\n\nmatched_data[, .(mean_cost = mean(cost), sd = sd(cost), med_cost = median(cost), iqr = IQR(cost)), livesalone]\n\n   livesalone mean_cost        sd med_cost      iqr\n       &lt;lgcl&gt;     &lt;num&gt;     &lt;num&gt;    &lt;num&gt;    &lt;num&gt;\n1:       TRUE  4248.037 10804.742 730.1007 2469.241\n2:      FALSE  3383.810  9362.913 669.2357 1967.224\n\n# histograms of cost\npar(mfrow = c(2, 1), mar = c(3, 3, 2, 0))\n\nhist(log(matched_data$cost[d$livesalone] + 1), main = \"Lives alone\", xlab = \"Log cost + 1\", col = \"red\", xlim = c(0, 14))\n\nhist(log(matched_data$cost[!d$livesalone] + 1), main = \"Does not live alone\", xlab = \"Log cost + 1\", col = \"green\", xlim = c(0, 14))\n\n\n\n\n\n\n# t-test (in reality you might want a paired test, and to check whether a t-test is appropriate)\nt.test(cost ~ livesalone, matched_data) # notice how wide the confidence intervals are for this reduced dataset\n\n\n    Welch Two Sample t-test\n\ndata:  cost by livesalone\nt = -1.9606, df = 2060.3, p-value = 0.05006\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -1728.6837721     0.2281226\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           3383.810            4248.037 \n\n# proportion with costs over £10000\nmatched_data[, .(over10k = sum(cost &gt; 10000) / .N), livesalone]\n\n   livesalone    over10k\n       &lt;lgcl&gt;      &lt;num&gt;\n1:       TRUE 0.10551331\n2:      FALSE 0.08555133\n\n\nThe mean difference is £803. When we used the whole dataset, this value was even closer to the coefficient from linear regression. It’s now difficult to see a difference in the histograms, but you can easily create any description of the distribution that you like – for example the proportion of patients that have costs over £10,000.\nWho got matched and who didn’t?\nThe point of matching was to create a comparison group of people who don’t live alone who were in some ways similar to the group who do. We probably had to delete lots of people who don’t live alone in a systematic way (for example men and younger people who do not live alone were more likely to be deleted). We might also have deleted some of the group who do live alone, which could be more problematic if we want to generalise our results to the population. Let’s see who got deleted…\n\nd[, matched := ifelse(PID %in% matched_data$PID, \"matched\", \"unmatched\")]\n\n# just looking at age group for now\ncompare_matched &lt;- dcast(d, age_grp ~ livesalone + matched, value.var = \"PID\")\ncompare_matched[, \"TRUE_total\" := TRUE_matched + TRUE_unmatched]\ncompare_matched[, lapply(.SD, function(x) x / sum(x) * 100), .SDcols = 2:6]\n\n   FALSE_matched FALSE_unmatched TRUE_matched TRUE_unmatched TRUE_total\n           &lt;num&gt;           &lt;num&gt;        &lt;num&gt;          &lt;num&gt;      &lt;num&gt;\n1:     40.399240       74.421090    40.399240       0.000000   37.24803\n2:     26.330798       19.522622    26.330798       0.000000   24.27695\n3:     23.479087        4.381902    23.479087       8.988764   22.34882\n4:      9.790875        1.674385     9.790875      91.011236   16.12621\n\n\nYou can see that some of the ‘lives alone’ group got deleted (TRUE_unmatched), and they were all in the older age groups. The difference between everyone who lives alone (TRUE_total) and the matched group (TRUE_matched) is diluted, because the oldest groups are a relatively small part of the data. Nonetheless, I would say this is a fairly important difference. If you are concerned about generalisability to the population, you might want to restrict the analysis to people aged under 85. In the full dataset this was not be a problem (as there were lots more potential ‘controls’), but you might encounter similar issues if you match on more detailed variables.\nFinal point! We’ve focused on a technical solution to exact matching. We haven’t really thought about which variables we should be matching on. This is a matter of judgement, and needs a bit of thinking before we dive into the matching process. Just like covariates in regression, the matching variables are confounders – they’re alternative explanations for an association between living alone and healthcare costs. Age and sex are clearly important. Health status is more complex. Do you want to think about long-term conditions as something that cause people to live alone, or the product of living alone? If people are sicker because they live alone, matching on the number of long term conditions might falsely reduce the association between living alone and healthcare costs.\nThis blog was written by Dan Lewer, NIHR Doctoral Research Fellow / SpR Public Health. Department of Epidemiology & Public Health, UCL\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-meets-r.html",
    "href": "blog/nhs-meets-r.html",
    "title": "NHS meets R",
    "section": "",
    "text": "Hello and welcome to our nascent NHS-R Community; a community dedicated to promoting the learning, application and utilisation of R in the National Health Service (NHS) in the United Kingdom. Like any community, NHS-R relies on the vibrancy of its participants to be relevant and productive – and fun.\n\n\nThe NHS is one of the best healthcare systems in the world[1]. It was launched in 1948 with the guiding principle of being free at the point of delivery – a kind of crowd funded open-source freeware equivalent of healthcare. More than half (52%) of the public say the NHS is what makes them most proud to be British, placing it above the armed forces (47%), the Royal Family (33%), Team GB (26%) and the BBC (22%)1.\nThe NHS in England deals with about 1 million people every 36 hours[2] and is continually generating vast amounts of data about the health and care of people[3]. This data is one of the most precious, yet under tapped, resources in the NHS. “Data is the new oil of the digital economy”[4] and drilling and mining NHS data could improve the NHS[5]. But mining these mountains of data is a colossal task.\nThis is where R comes in. R was conceived in 1992[6] as a free open-source statistical programming environment, which is now widely used in industry[7] (Google, Microsoft, Airbnb, New York Times, Lloyds of London, and so on) and academia, and is now ranked amongst the most popular (sixth as of 2017) programming languages[8]. But its use in the NHS is almost non-existent. Whilst there are several reasons for this, the absence of R at scale in the NHS, means that the NHS is unable to take advantage of the huge benefits of R, including cutting-edge visualisation and statistical tools, and a worldwide R community, which freely shares learning and resources.\nSo, our aim is to promote the use of R in the NHS, and help to make the NHS better.\nTo kick-start the NHS-R Community, we have developed a website [www.nhsrcommunity.com] and are offering four free workshops (3 days each, repeated in Yorkshire and Wales). Workshop (1) will be an introduction to R for healthcare analysts. Subsequent workshops will focus on the following problems:- (2) understanding and reporting hospital mortality statistics, (3) predicting urgent demand for hospital care and (4) evaluation of interventions using matched retrospective controls. Each workshop will be led by experts in both the problem domain and R, and captured electronically for wider dissemination. Registration for the workshops is now open*.\nHowever, anyone can contribute to the NHS-R Community, so why not share your experience (novice, beginner, or otherwise) of using R in the healthcare setting? Write a blog, share R tips, do an on-line R tutorial, suggest topics for ongoing development and support, and share ideas on how to embed R into the NHS.\nFrom the NHS-R Team [Posted: 19 March 2018]\nThe NHS-R Community project is funded by The Health Foundation.\n*NB: To be eligible for the workshops you must have a working NHS email address. Places are limited.\n\nhttps://www.newscientist.com/article/2140698-us-ranked-worst-healthcare-system-while-the-nhs-is-the-best/ Accessed 20240221\nhttps://www.nhs.uk/NHSEngland/thenhs/about/Pages/overview.aspx Accessed 20240221\nhttps://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Accessed 20240221\nhttp://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html Accessed 20240221\nhttp://makemeanalyst.com/companies-using-r/ Accessed 20240221\nhttps://spectrum.ieee.org/computing/software/the-2017-top-programming-languages Accessed 20240221\n\nThis blog has been formatted to remove Latin Abbreviations."
  },
  {
    "objectID": "blog/nhs-meets-r.html#so-why-get-involved",
    "href": "blog/nhs-meets-r.html#so-why-get-involved",
    "title": "NHS meets R",
    "section": "",
    "text": "The NHS is one of the best healthcare systems in the world[1]. It was launched in 1948 with the guiding principle of being free at the point of delivery – a kind of crowd funded open-source freeware equivalent of healthcare. More than half (52%) of the public say the NHS is what makes them most proud to be British, placing it above the armed forces (47%), the Royal Family (33%), Team GB (26%) and the BBC (22%)1.\nThe NHS in England deals with about 1 million people every 36 hours[2] and is continually generating vast amounts of data about the health and care of people[3]. This data is one of the most precious, yet under tapped, resources in the NHS. “Data is the new oil of the digital economy”[4] and drilling and mining NHS data could improve the NHS[5]. But mining these mountains of data is a colossal task.\nThis is where R comes in. R was conceived in 1992[6] as a free open-source statistical programming environment, which is now widely used in industry[7] (Google, Microsoft, Airbnb, New York Times, Lloyds of London, and so on) and academia, and is now ranked amongst the most popular (sixth as of 2017) programming languages[8]. But its use in the NHS is almost non-existent. Whilst there are several reasons for this, the absence of R at scale in the NHS, means that the NHS is unable to take advantage of the huge benefits of R, including cutting-edge visualisation and statistical tools, and a worldwide R community, which freely shares learning and resources.\nSo, our aim is to promote the use of R in the NHS, and help to make the NHS better.\nTo kick-start the NHS-R Community, we have developed a website [www.nhsrcommunity.com] and are offering four free workshops (3 days each, repeated in Yorkshire and Wales). Workshop (1) will be an introduction to R for healthcare analysts. Subsequent workshops will focus on the following problems:- (2) understanding and reporting hospital mortality statistics, (3) predicting urgent demand for hospital care and (4) evaluation of interventions using matched retrospective controls. Each workshop will be led by experts in both the problem domain and R, and captured electronically for wider dissemination. Registration for the workshops is now open*.\nHowever, anyone can contribute to the NHS-R Community, so why not share your experience (novice, beginner, or otherwise) of using R in the healthcare setting? Write a blog, share R tips, do an on-line R tutorial, suggest topics for ongoing development and support, and share ideas on how to embed R into the NHS.\nFrom the NHS-R Team [Posted: 19 March 2018]\nThe NHS-R Community project is funded by The Health Foundation.\n*NB: To be eligible for the workshops you must have a working NHS email address. Places are limited.\n\nhttps://www.newscientist.com/article/2140698-us-ranked-worst-healthcare-system-while-the-nhs-is-the-best/ Accessed 20240221\nhttps://www.nhs.uk/NHSEngland/thenhs/about/Pages/overview.aspx Accessed 20240221\nhttps://www.wired.com/insights/2014/07/data-new-oil-digital-economy/ Accessed 20240221\nhttp://blog.revolutionanalytics.com/2017/10/updated-history-of-r.html Accessed 20240221\nhttp://makemeanalyst.com/companies-using-r/ Accessed 20240221\nhttps://spectrum.ieee.org/computing/software/the-2017-top-programming-languages Accessed 20240221\n\nThis blog has been formatted to remove Latin Abbreviations."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html",
    "href": "blog/a-dbplyr-based-address-matching-package.html",
    "title": "A dbplyr-based Address Matching Package",
    "section": "",
    "text": "Matching address records from one table to another is a common and often repeated task. This is easy when address strings can be matched exactly, although not so easy when they cannot be matched exactly. An overarching issue is that an address string may be spelt (or misspelt) in multiple ways across multiple records. Despite this, we may want to know which records are likely to be same address in another table, even though these addresses do not share the exact same spelling."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#the-challenge",
    "href": "blog/a-dbplyr-based-address-matching-package.html#the-challenge",
    "title": "A dbplyr-based Address Matching Package",
    "section": "",
    "text": "Matching address records from one table to another is a common and often repeated task. This is easy when address strings can be matched exactly, although not so easy when they cannot be matched exactly. An overarching issue is that an address string may be spelt (or misspelt) in multiple ways across multiple records. Despite this, we may want to know which records are likely to be same address in another table, even though these addresses do not share the exact same spelling."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#the-solution",
    "href": "blog/a-dbplyr-based-address-matching-package.html#the-solution",
    "title": "A dbplyr-based Address Matching Package",
    "section": "The solution",
    "text": "The solution\nTo this end, the NHSBSA Data Science team have created an address matching package called {addressMatchR}. Using this package, we can now standardise all our address matching activities and save time using the same functions for a variety of use cases. With the code and functions openly available, we hope that other NHS organisations can benefit from this package as well."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#the-details",
    "href": "blog/a-dbplyr-based-address-matching-package.html#the-details",
    "title": "A dbplyr-based Address Matching Package",
    "section": "The details",
    "text": "The details\nThis package enables two database tables to be matched against each other, with the only requirement being each table has a postcode and address field. The output will be a matched dataset where matches are categorised as being Exact or Non-Exact, with Non-Exact matches being scored, so that the quality of these Non-Exact matches can be considered. You can download the package using the following code:\n\ninstall.packages(\"devtools\")\n\ndevtools::install_github(\"nhsbsa-data-analytics/addressMatchR\")\n\nThis package has been created and configured to work with database tables, as people or teams often need or want to match addresses in bulk. This could be tens or even hundreds of million records, which may not be feasible within a local R environment. For that reason, all of the functions have been configured to work with {dbplyr}, so the data being matched never ‘leaves’ the database.\nFor those encountering {dbplyr} for the first time, it is a package which enables users to use remote database tables as if they are in-memory data frames by automatically converting dplyr code into SQL. The advantage of this is that dplyr functions can be used to query a database and process the output using succinct and easy-to-read code. A disadvantage is that dbplyr code sometimes needs to be structured in a way to optimise how it is converted into SQL. More information on {dbplyr} can be found here: A dplyr backend for databases • dbplyr (tidyverse.org)"
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#connect-to-a-database",
    "href": "blog/a-dbplyr-based-address-matching-package.html#connect-to-a-database",
    "title": "A dbplyr-based Address Matching Package",
    "section": "Connect to a database",
    "text": "Connect to a database\nThe first thing to do when using {dbplyr} is to connect to a database, after which tables within a user’s schema can be queried. These objects are referred to as ‘lazyframes’ rather than dataframes, with the tables still being remote. We first need to establish a database connection, using our database connection details and authentication.\n\ncon &lt;- DBI::dbConnect(\n  odbc::odbc(),\n  Driver = \"XXXXXX\",\n  DBQ = \"XXXXXX\",\n  UID = \"XXXXXX\",\n  PWD = \"XXXXXX\"\n)"
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#generate-dummy-data",
    "href": "blog/a-dbplyr-based-address-matching-package.html#generate-dummy-data",
    "title": "A dbplyr-based Address Matching Package",
    "section": "Generate dummy data",
    "text": "Generate dummy data\nWe will generate two dummy address data sets rather than use genuine address data. We will match these two datasets against each other.\n\ndata_one &lt;- data.frame(\n  ADDRESS_ONE = c(\n    \"10 KINGS ROAD\", \"11 KINGS ROAD\", \"12 KINGS RD\",\n    \"13 KONGS ROAD\", \"14 KING ROD\", \"15A KINGS ROADD\",\n    \"15B KINGS RD\", \"THE SHOP KINGS RD\", \"THE SHIP KINGS ROAD\"\n  ),\n  POSTCODE_ONE = c(\n    rep(\"ABCD 333\", 2), rep(\"ABCD 123\", 7)\n  )\n)\n\ndata_two &lt;- data.frame(\n  ADDRESS_TWO = c(\n    \"10 KINGS ROAD\", \"11 KINGS ROAD\", \"12, kings, road\",\n    \"13, kings, rd\", \"14, king, road\", \"15A, kings road\",\n    \"15B, kings road\", \"the shop, king-rd\", \"the ship kings,,road\"\n  ),\n  POSTCODE_TWO = c(\n    \"abcd123\", \"abcd123\", \"ABCD-123\", \"ABCD-123\", \"ABCD.123\",\n    \"ABCD  123\", \"ABCD 123\", \"ABCD123###\", \"ABCD 123\"\n  )\n)\n\nIn practice, {dbplyr} needs to connect to tables existing within a database. If we save these two datasets as tables within our schema, we can connect to them afterwards.\n\nDBI::dbWriteTable(con, name = \"TEST_ADDRESS_DATA_ONE\", value = data_one)\nDBI::dbWriteTable(con, name = \"TEST_ADDRESS_DATA_TWO\", value = data_two)"
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#clean-the-data",
    "href": "blog/a-dbplyr-based-address-matching-package.html#clean-the-data",
    "title": "A dbplyr-based Address Matching Package",
    "section": "Clean the data",
    "text": "Clean the data\nWe can now connect to our two database address tables and clean the address and postcode fields. The functions within the package used for address and postcode cleaning are addressMatchR::tidy_postcode() and addressMatchR::tidy_single_line_address(). The tidy_postcode() functions does exactly as you would imagine and trims whitespace and removes spaces and non-alphanumeric characters. The tidy_single_line_address() does the same, but also splits house numbers with alphanumeric strings (e.g. 24A -&gt; 24, A), which helps the matching algorithm work more effectively.\n\ndata_one &lt;- dplyr::tbl(con, \"TEST_ADDRESS_DATA_ONE\") %&gt;%\n  addressMatchR::tidy_postcode(df = ., col = POSTCODE_ONE) %&gt;%\n  addressMatchR::tidy_single_line_address(df = ., col = ADDRESS_ONE)\n\ndata_two &lt;- dplyr::tbl(con, \"TEST_ADDRESS_DATA_TWO\") %&gt;%\n  addressMatchR::tidy_postcode(df = ., col = POSTCODE_TWO) %&gt;%\n  addressMatchR::tidy_single_line_address(df = ., col = ADDRESS_TWO)\n\nWe can now match the two cleaned address database tables against each other. To explain the format of the output, it is necessary to explain how the matching function itself works.\nWhat happens under the hood?\nThe key features of the matching algorithm are listed below:\n\nThe function identifies two match types, Exact and Non-Exact matches\nExact matches are when the address and postcode are identical across datasets\nAll records not Exact matched are considered for a Non-Exact match\nA Non-Exact match was to identify address strings deemed similar yet not identical\nIf an address could not be Non-Exact matched it was excluded from the output\nNon-Exact matching is conducted on a postcode level. For example, an address from postcode ‘NE1 5DL’ would only be matched against other addresses that shared the same postcode.\nThe matching algorithm generates a matching score for each lookup-address that an address is matched against. For example, if 30 lookup-addresses share the same postcode, each of these 30 addresses would be scored against the address.\nThe scoring process splits an address into tokens (words) and then scores every token of an address against all the tokens of the addresses with a shared postcode.\nToken-level scoring uses the Jaro-Winkler string similarity algorithm.\nHowever, numerical tokens don’t use Jaro-Winkler and are scored slightly differently and given a higher weighting.\nAll of the token-level scores are aggregated to give a final score, for every lookup-address an address was matched against.\nThe best scoring Non-Exact match is then selected. If multiple properties have the same best match score, they are all included as joint best Non-Exact matches.\n\nFor the purposes of this blog, the above process has been heavily simplified. For a far more detailed and thorough explanation of the matching process, read section four of this RPubs article, which also describes a use case of the address matching function within an analysis of care home prescribing."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#match-the-data",
    "href": "blog/a-dbplyr-based-address-matching-package.html#match-the-data",
    "title": "A dbplyr-based Address Matching Package",
    "section": "Match the data",
    "text": "Match the data\nThe matching function only requires each of the lazyframes to be specified, along with their postcode and address column names. We can store the matched output in a new lazyframe.\n\nresults_db &lt;- addressMatchR::calc_match_addresses &lt;- function(\n    data_one,\n    ADDRESS_ONE,\n    POSTCODE_ONE,\n    data_two,\n    ADDRESS_TWO,\n    POSTCODE_TWO) {}\n\nWe can then inspect this output. The cleaned rather than initial address fields are displayed here:\nADDRESS_ONE         POSTCODE_ONE ADDRESS_TWO         SCORE MATCH_TYPE\n&lt;chr&gt;               &lt;chr&gt;        &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;     \n1 THE SHIP KINGS ROAD ABCD123      THE SHIP KINGS ROAD 1     EXACT     \n2 12 KINGS RD         ABCD123      12 KINGS ROAD       0.833 NON-EXACT \n3 13 KONGS ROAD       ABCD123      13 KINGS RD         0.813 NON-EXACT \n4 14 KING ROD         ABCD123      14 KING ROAD        0.989 NON-EXACT \n5 15 A KINGS ROADD    ABCD123      15 A KINGS ROAD     0.994 NON-EXACT \n6 15 B KINGS RD       ABCD123      15 B KINGS ROAD     0.857 NON-EXACT \n7 THE SHOP KINGS RD   ABCD123      THE SHOP KING-RD    0.719 NON-EXACT\nThe output shows the following has happened:\n\nThe 2 records with postcode ABCD333 from data_one weren’t matched, as no records from data_two shared the same postcode.\nOnly 1 record from data_one had an Exact match and the remaining records were considered for a Non-Exact match.\nThe highest scoring match from data_two was attributed to each data_one record. In this instance, each Non-Exact match correctly identified the appropriate address.\n\nIf address tables in a database are too big to be matched locally, it is possible the results may be too big to be collected into a dataframe. That said, if required the output could be collected locally by:\n\nresults_df &lt;- results_db %&gt;%\n  dplyr::collect()\n\nIn contrast, the following would write the results back to the database:\n\nresults_db %&gt;%\n  dplyr::compute(\n    name = \"TEST_ADDRESS_MATCH\",\n    temporary = FALSE\n  )\n\nUsing this second method, at no point during the workflow will any of the data have been ‘at rest’ within our R environment. As mentioned, this means such a workflow can work with far larger volumes of data, as is often the case with address matching. Finally, we then need to remember to disconnect from the database and delete our dummy address data.\n\nDBI::dbRemoveTable(con, \"TEST_ADDRESS_DATA_ONE\")\nDBI::dbRemoveTable(con, \"TEST_ADDRESS_DATA_TWO\")\nDBI::dbDisconnect(con)"
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#other-package-functions",
    "href": "blog/a-dbplyr-based-address-matching-package.html#other-package-functions",
    "title": "A dbplyr-based Address Matching Package",
    "section": "Other package functions",
    "text": "Other package functions\nOne of our own internal use cases for using this package was matching a selection of address records against Ordnance Survey (OS) AddressBase. The package also includes functions to help upload OS AddressBase into R, and how to create two versions of a single line address from AddressBase Plus (which has address information across multiple fields).\n\naddressMatchR::upload_addressbase_plus_to_oracle()\naddressMatchR::calc_addressbase_plus_dpa_single_line_address()\naddressMatchR::calc_addressbase_plus_geo_single_line_address()\n\nThe idea is that AddressBase address information can be used as a comprehensive set of lookup addresses, to validate a set of addresses against. Access to AddressBase is free for all NHS organisations under the Public Sector Geospatial Agreement (PSGA) and worth looking at if you need a comprehensive list of UK addresses to match against. More information on Ordnance Survey AddressBase and the PSGA can be found here: The Public Sector Geospatial Agreement | Ordnance Survey."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#what-do-i-need-to-be-aware-of",
    "href": "blog/a-dbplyr-based-address-matching-package.html#what-do-i-need-to-be-aware-of",
    "title": "A dbplyr-based Address Matching Package",
    "section": "What do I need to be aware of?",
    "text": "What do I need to be aware of?\nThere are many ways to match address information with none being perfect. Some caveats around the approach used within the package are outlined below:\n\nThe code has been configured to work with an Oracle database, so some functions may require tweaking for different databases. Please get in touch if you require any assistance or advice about this\nIf a postcode is incorrect, an address will attempt to be matched against the ‘wrong street’.\nAddress records with no postcode or an invalid postcode will not be able to be matched.\nThe user is required to manually deal with non-exact matches that share the same top score.\nThe user is recommended to manually validate a selection of non-exact matches to see if a match score threshold is required (very much use case dependent).\nThe matching and address cleaning functions expect an address within a single cell."
  },
  {
    "objectID": "blog/a-dbplyr-based-address-matching-package.html#further-information",
    "href": "blog/a-dbplyr-based-address-matching-package.html#further-information",
    "title": "A dbplyr-based Address Matching Package",
    "section": "Further information",
    "text": "Further information\nHopefully this quick overview and explanation of the {addressMatchR} package is enough to get you started with matching large volumes of address records across database tables. Feel free to have a look at the underlying code on our GitHub page if you want to see how the functions work in more detail.\nIf you would like to see the output from one of our use cases of using the package, please have a read of our interactive analysis of care home prescribing in older age patients.\nIf you need to get in touch with a question about the package and/or code, or about how to process the output related to your own use case, please get in touch with us at: nhsbsa.datalab@nhs.net\nThanks!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions\n\n\n\nPredictive Analytics\n\nMachine Learning\n\n\n\n\n\n\nJun 14, 2025\n\n\nJoseph Mosley\n\n13 min\n\n2,410 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe importance of community in NHS data science\n\n\n\nNHS-R\n\n\n\n\n\n\nMar 31, 2025\n\n\nClaire Welsh\n\n4 min\n\n645 words\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Analytics within healthcare - Random Forest models for predicting length of stay\n\n\n\nPredictive Analytics\n\nMachine Learning\n\ndplyr\n\nSQL\n\n\n\n\n\n\nFeb 27, 2025\n\n\nJoseph Mosley\n\n7 min\n\n1,374 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community GitHub Actions - spelling\n\n\n\nGitHub Action\n\n\n\n\n\n\nFeb 14, 2025\n\n\nZoë Turner\n\n7 min\n\n1,258 words\n\n\n\n\n\n\n\n\n\n\n\n\nCreating synthetic data using the synthpop package\n\n\n\nData\n\n\n\n\n\n\nOct 1, 2024\n\n3 min\n\n423 words\n\n\n\n\n\n\n\n\n\n\n\n\nCQC directory data vignette\n\n\n\nData\n\n\n\n\n\n\nSep 29, 2024\n\n\nMartine Wauben\n\n4 min\n\n788 words\n\n\n\n\n\n\n\n\n\n\n\n\nHow to ask for help\n\n\n\nNHS-R\n\nR tips\n\n\n\n\n\n\nJul 31, 2024\n\n\nLyn Howard\n\n2 min\n\n397 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R/NHS.Pycom conference 2023 my experience of the event\n\n\n\nConference\n\n\n\n\n\n\nJun 4, 2024\n\n\nPablo Leon-Rodenas\n\n6 min\n\n1,099 words\n\n\n\n\n\n\n\n\n\n\n\n\nPublish on GitHub\n\n\n\nGitHub\n\nRMarkdown\n\nQuarto\n\n\n\n\n\n\nJun 3, 2024\n\n\nZoë Turner\n\n2 min\n\n250 words\n\n\n\n\n\n\n\n\n\n\n\n\nShowcasing a function - separate()\n\n\n\nR tips\n\n\n\n\n\n\nMar 12, 2024\n\n\nZoë Turner\n\n1 min\n\n145 words\n\n\n\n\n\n\n\n\n\n\n\n\nRecoding an NA and back again\n\n\n\nR tips\n\n\n\n\n\n\nMar 12, 2024\n\n\nZoë Turner\n\n2 min\n\n304 words\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Quarto website for NHS-R Community\n\n\n\nReflections\n\n\n\n\n\n\nFeb 24, 2024\n\n\nZoë Turner\n\n7 min\n\n1,218 words\n\n\n\n\n\n\n\n\n\n\n\n\nNavigating the Data-Driven Frontier: Insights from an NHS-R Committee Member\n\n\n\nCommittee\n\n\n\n\n\n\nFeb 6, 2024\n\n\nPrajwal Khairnar\n\n3 min\n\n466 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is a “proper” data scientist anyway?\n\n\n\nNHS-R\n\n\n\n\n\n\nJan 8, 2024\n\n\nChris Beeley\n\n3 min\n\n592 words\n\n\n\n\n\n\n\n\n\n\n\n\nCoffee and Code\n\n\n\nNHS-R\n\n\n\n\n\n\nNov 14, 2023\n\n\nSimon Wellesley-Miller\n\n4 min\n\n625 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhat was the “Unconference”?\n\n\n\nConference\n\n\n\n\n\n\nOct 30, 2023\n\n\nBen Murch\n\n5 min\n\n920 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community needs help\n\n\n\nNHS-R\n\n\n\n\n\n\nOct 20, 2023\n\n\nZoë Turner\n\n5 min\n\n927 words\n\n\n\n\n\n\n\n\n\n\n\n\nAphA blog – August 2023\n\n\n\nNHS-R\n\n\n\n\n\n\nSep 5, 2023\n\n\nZoë Turner\n\n2 min\n\n376 words\n\n\n\n\n\n\n\n\n\n\n\n\nJuly Blog\n\n\n\nNHS-R\n\n\n\n\n\n\nJul 19, 2023\n\n\nZoë Turner\n\n7 min\n\n1,234 words\n\n\n\n\n\n\n\n\n\n\n\n\nAphA June Blog\n\n\n\nNHS-R\n\n\n\n\n\n\nJul 18, 2023\n\n\nZoë Turner\n\n5 min\n\n963 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R newscast 20th July 2023\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nJun 15, 2023\n\n\nZoë Turner\n\n3 min\n\n551 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R newscast 25th May 2023\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nJun 15, 2023\n\n\nZoë Turner\n\n2 min\n\n279 words\n\n\n\n\n\n\n\n\n\n\n\n\nAphA blog – May 2023\n\n\n\nNHS-R\n\nSQL\n\n\n\n\n\n\nJun 9, 2023\n\n\nZoë Turner\n\n6 min\n\n1,094 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe data science assembly\n\n\n\nNHS\n\n\n\n\n\n\nJun 9, 2023\n\n\nSami Sultan\n\n3 min\n\n448 words\n\n\n\n\n\n\n\n\n\n\n\n\nRStudio and Git – selecting many files\n\n\n\nRStudio\n\nGit\n\n\n\n\n\n\nMay 23, 2023\n\n\nZoë Turner\n\n3 min\n\n448 words\n\n\n\n\n\n\n\n\n\n\n\n\nAphA April 2023 Blog\n\n\n\nNHS-R\n\n\n\n\n\n\nApr 25, 2023\n\n\nZoë Turner\n\n4 min\n\n709 words\n\n\n\n\n\n\n\n\n\n\n\n\nCode snippets – 2 scale y axes in {ggplot2}\n\n\n\nggplot2\n\nR tips\n\n\n\n\n\n\nMar 28, 2023\n\n\nZoë Turner\n\n4 min\n\n744 words\n\n\n\n\n\n\n\n\n\n\n\n\nCode snippets – first(), last() and nth() {dplyr} functions\n\n\n\ndplyr\n\nR tips\n\n\n\n\n\n\nMar 13, 2023\n\n\nZoë Turner\n\n4 min\n\n739 words\n\n\n\n\n\n\n\n\n\n\n\n\nCode snippets – regular expressions\n\n\n\nStrings\n\nR tips\n\n\n\n\n\n\nFeb 25, 2023\n\n\nZoë Turner\n\n5 min\n\n945 words\n\n\n\n\n\n\n\n\n\n\n\n\nA roundup from the 2022 NHS-R Conference – Part 1\n\n\n\nConference\n\n\n\n\n\n\nFeb 9, 2023\n\n\nLaura Moscoviz\n\n4 min\n\n755 words\n\n\n\n\n\n\n\n\n\n\n\n\nA roundup from the 2022 NHS-R Conference – Part 2\n\n\n\nConference\n\n\n\n\n\n\nFeb 9, 2023\n\n\nLaura Moscoviz\n\n4 min\n\n619 words\n\n\n\n\n\n\n\n\n\n\n\n\nFuzzy joining tables using string distance methods\n\n\n\nStrings\n\ntidyverse\n\n\n\n\n\n\nFeb 3, 2023\n\n\nTom Jemmett\n\n7 min\n\n1,378 words\n\n\n\n\n\n\n\n\n\n\n\n\nGuest blogger – ChatGPT\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nFeb 3, 2023\n\n2 min\n\n295 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R newscast 16th January 2023\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nJan 17, 2023\n\n\nChris Beeley\n\n3 min\n\n438 words\n\n\n\n\n\n\n\n\n\n\n\n\nReflections on the NHS-R Community Conference\n\n\n\nConference\n\n\n\n\n\n\nJan 11, 2023\n\n\nChandan Kaur\n\n2 min\n\n345 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Workshop: Development and validation of clinical prediction models using R\n\n\n\nModelling\n\n\n\n\n\n\nDec 15, 2022\n\n\nEduard Incze\n\n2 min\n\n322 words\n\n\n\n\n\n\n\n\n\n\n\n\nPosit + NHS-R: a Perfect Partnership!\n\n\n\nNHS-R\n\nConference\n\n\n\n\n\n\nNov 29, 2022\n\n4 min\n\n613 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R newscast 28th October 2022\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nOct 30, 2022\n\n\nChris Beeley\n\n1 min\n\n79 words\n\n\n\n\n\n\n\n\n\n\n\n\nA dbplyr-based Address Matching Package\n\n\n\ndbplyr\n\nAddresses\n\n\n\n\n\n\nOct 17, 2022\n\n\nAdnan Shroufi\n\n11 min\n\n2,071 words\n\n\n\n\n\n\n\n\n\n\n\n\nUsing RMD for academic writing\n\n\n\nRMarkdown\n\n\n\n\n\n\nOct 10, 2022\n\n\nDaniel Weiand\n\n6 min\n\n1,092 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R newscast 15th August 2022\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nAug 24, 2022\n\n\nChris Beeley\n\n2 min\n\n219 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R newscast 6th July 2022\n\n\n\nNHS-R\n\nPodcast\n\n\n\n\n\n\nJul 29, 2022\n\n\nChris Beeley\n\n2 min\n\n320 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe ‘nhsnumber’ package and the joy of sharing your niche\n\n\n\nNHS\n\nPackage\n\n\n\n\n\n\nJul 19, 2022\n\n\nMark Sellors\n\n5 min\n\n922 words\n\n\n\n\n\n\n\n\n\n\n\n\nNew year, new laptop, new library location, new version of R!\n\n\n\nR tips\n\n\n\n\n\n\nJun 17, 2022\n\n\nCara Thompson\n\n6 min\n\n1,170 words\n\n\n\n\n\n\n\n\n\n\n\n\nAlignment cheatsheet\n\n\n\nggplot2\n\ntidyverse\n\nVisualisations\n\n\n\n\n\n\nMay 11, 2022\n\n\nCara Thompson\n\n6 min\n\n1,107 words\n\n\n\n\n\n\n\n\n\n\n\n\nSuccess story - Dr Lydia Briggs, Data Scientist, GOSH\n\n\n\nReflections\n\n\n\n\n\n\nMar 21, 2022\n\n\nLydia Briggs\n\n2 min\n\n261 words\n\n\n\n\n\n\n\n\n\n\n\n\nSuccess story - Kate Cheema, Director of Health Intelligence, British Heart Foundation\n\n\n\nReflections\n\n\n\n\n\n\nMar 21, 2022\n\n\nKate Cheema\n\n3 min\n\n550 words\n\n\n\n\n\n\n\n\n\n\n\n\nSuccess story - Dr William Bryant, Senior Data Scientist, GOSH\n\n\n\nReflections\n\n\n\n\n\n\nMar 21, 2022\n\n\nWilliam Bryant\n\n1 min\n\n157 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS Low Income Scheme\n\n\n\ngolem\n\nshiny\n\nhtml\n\nAccessibility\n\nScrollytell\n\n\n\n\n\n\nMar 18, 2022\n\n13 min\n\n2,431 words\n\n\n\n\n\n\n\n\n\n\n\n\nJoin the fun! Join NHS-R\n\n\n\nNHS-R\n\n\n\n\n\n\nMar 2, 2022\n\n\nMohammed Amin Mohammed\n\n4 min\n\n788 words\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R to create column charts featuring 95% confidence intervals\n\n\n\nStatistics\n\ntidyverse\n\nggplot2\n\nPublic Health\n\n\n\n\n\n\nJan 5, 2022\n\n\nDaniel Weiand\n\n8 min\n\n1,493 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhat a NHS-R Community Conference it was – simply wow!\n\n\n\nConference\n\n\n\n\n\n\nNov 16, 2021\n\n\nGary Hutson\n\n4 min\n\n734 words\n\n\n\n\n\n\n\n\n\n\n\n\nDesigning my first Shiny dashboard\n\n\n\nShiny\n\nFunctions\n\ntidyverse\n\nbase R\n\nGIS\n\nzoo\n\nplotly\n\n\n\n\n\n\nOct 14, 2021\n\n\nPablo Leon-Rodenas\n\n14 min\n\n2,702 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community Conference: view from the The Chartered Society of Physiotherapy (CSP)\n\n\n\nPersonal experience\n\n\n\n\n\n\nSep 10, 2021\n\n\nRobin Hinks\n\n2 min\n\n253 words\n\n\n\n\n\n\n\n\n\n\n\n\nTracking and getting download statistics for your R packages\n\n\n\nPackages\n\nFunctions\n\nLists\n\ntidyverse\n\n\n\n\n\n\nSep 8, 2021\n\n\nGary Hutson\n\n3 min\n\n564 words\n\n\n\n\n\n\n\n\n\n\n\n\nOptimising dplyr\n\n\n\ndplyr\n\ndata.table\n\ntidyverse\n\n\n\n\n\n\nJul 15, 2021\n\n\nTom Jemmett\n\n11 min\n\n2,149 words\n\n\n\n\n\n\n\n\n\n\n\n\nUsing {sf} to calculate catchment areas\n\n\n\nGIS\n\nggplot2\n\n\n\n\n\n\nJun 3, 2021\n\n\nTom Jemmett\n\n10 min\n\n1,996 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe Health Service Modelling Associates (HSMA) Programme – the future of analytics for health, social care and policing?\n\n\n\nDiscrete Event Simulation\n\n\n\n\n\n\nMay 7, 2021\n\n\nDaniel Chalk\n\n9 min\n\n1,616 words\n\n\n\n\n\n\n\n\n\n\n\n\nCan we rely on synthetic data to overcome data governance issue in healthcare?\n\n\n\nSynthetic dataset\n\nData\n\nbase R\n\n\n\n\n\n\nApr 22, 2021\n\n6 min\n\n1,032 words\n\n\n\n\n\n\n\n\n\n\n\n\nSoftware licensing\n\n\n\nNHS-R\n\n\n\n\n\n\nMar 17, 2021\n\n\nChris Beeley\n\n4 min\n\n692 words\n\n\n\n\n\n\n\n\n\n\n\n\nLearned from Community\n\n\n\nNHS-R\n\nDiscrete Event Simulation\n\n\n\n\n\n\nMar 3, 2021\n\n\nNujcharee Haswell\n\n3 min\n\n477 words\n\n\n\n\n\n\n\n\n\n\n\n\nA thank you note to NHS-R Community\n\n\n\nNHS-R\n\n\n\n\n\n\nFeb 2, 2021\n\n\nZoë Turner\n\n2 min\n\n398 words\n\n\n\n\n\n\n\n\n\n\n\n\nTwo-Way Business Intelligence – Partnering Shiny and SQL to Capture Insights in Performance Reporting\n\n\n\nSPC charts\n\nStatistics\n\nShiny\n\nSQL\n\n\n\n\n\n\nJan 20, 2021\n\n\nChristopher Reading-Skilton\n\n4 min\n\n765 words\n\n\n\n\n\n\n\n\n\n\n\n\nHexitime is shortlisted as a finalist for the HSJ Partnership Awards 2021\n\n\n\nNHS-R\n\n\n\n\n\n\nDec 14, 2020\n\n\nBeth Taylor\n\n3 min\n\n502 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe NHS-R Community and Hexitime – Our 1st month of collaborating\n\n\n\nNHS-R\n\n\n\n\n\n\nDec 11, 2020\n\n\nNHS-R Community Team\n\n3 min\n\n422 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community Conference – organisers’ perspective\n\n\n\nConference\n\n\n\n\n\n\nDec 8, 2020\n\n\nNHS-R Community Team\n\n4 min\n\n695 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R 2020 Week Long Conference – so much great content, so little time to catch it all…\n\n\n\nConference\n\n\n\n\n\n\nNov 19, 2020\n\n\nGary Hutson\n\n5 min\n\n941 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHSr – Rguably the Best Conference in the World\n\n\n\nConference\n\n\n\n\n\n\nNov 17, 2020\n\n\nEugene Hickey\n\n4 min\n\n623 words\n\n\n\n\n\n\n\n\n\n\n\n\nAnnouncing a new NHS-R + Hexitime partnership\n\n\n\nNHS-R\n\n\n\n\n\n\nOct 26, 2020\n\n\nMohammed A Mohammed\n\n3 min\n\n441 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Book Club\n\n\n\nNHS-R\n\n\n\n\n\n\nOct 20, 2020\n\n\nNujcharee Haswell\n\n2 min\n\n333 words\n\n\n\n\n\n\n\n\n\n\n\n\nMember profile - Robin Hinks\n\n\n\nPersonal experience\n\n\n\n\n\n\nOct 8, 2020\n\n\nRobin Hinks\n\n2 min\n\n353 words\n\n\n\n\n\n\n\n\n\n\n\n\nAnnotating SPC plots using annotate with ggplot\n\n\n\nSPC charts\n\nStatistics\n\n\n\n\n\n\nAug 13, 2020\n\n\nChristopher Reading-Skilton\n\n3 min\n\n542 words\n\n\n\n\n\n\n\n\n\n\n\n\nWe need you: share your COVID-19 work on NHSE/I’s regular mini-huddle series\n\n\n\nCovid-19\n\n\n\n\n\n\nMay 21, 2020\n\n\nSophie Hodges\n\n2 min\n\n293 words\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting R (Virtual) Workshop – the pupil’s perspective\n\n\n\nForecasting\n\n\n\n\n\n\nMay 21, 2020\n\n\nPaul Bullard\n\n4 min\n\n733 words\n\n\n\n\n\n\n\n\n\n\n\n\n{phsmethods}: an R package for Public Health Scotland\n\n\n\nPackages\n\n\n\n\n\n\nApr 30, 2020\n\n\nJack Hannah\n\n9 min\n\n1,634 words\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding the ONS mortality dataset\n\n\n\nData\n\nPackages\n\nTraining\n\ndplyr\n\nFunctions\n\nLoops\n\nWeb scraping\n\n\n\n\n\n\nApr 23, 2020\n\n\nZoë Turner\n\n24 min\n\n4,689 words\n\n\n\n\n\n\n\n\n\n\n\n\nSPC Charting in R\n\n\n\nSPC charts\n\nStatistics\n\n\n\n\n\n\nMar 13, 2020\n\n\nChristopher Reading-Skilton\n\n8 min\n\n1,549 words\n\n\n\n\n\n\n\n\n\n\n\n\nOur first ever NHS-R webinar!\n\n\n\nNHS-R\n\n\n\n\n\n\nFeb 21, 2020\n\n\nNHS-R\n\n4 min\n\n696 words\n\n\n\n\n\n\n\n\n\n\n\n\nTowards open health analytics: our guide to sharing code safely on GitHub\n\n\n\nGitHub\n\n\n\n\n\n\nFeb 14, 2020\n\n\nFiona Grimm\n\n12 min\n\n2,206 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHSRdatasets meets runcharter\n\n\n\nData\n\nPackages\n\nRun charts\n\nStatistics\n\n\n\n\n\n\nFeb 12, 2020\n\n\nJohn MacKintosh\n\n5 min\n\n844 words\n\n\n\n\n\n\n\n\n\n\n\n\nDon’t Repeat Yourself!\n\n\n\nFunctions\n\nData\n\n\n\n\n\n\nJan 30, 2020\n\n\nTom Jemmett\n\n8 min\n\n1,545 words\n\n\n\n\n\n\n\n\n\n\n\n\nHow NHS-R Community do The Apprentice…\n\n\n\nPersonal experience\n\nTraining\n\n\n\n\n\n\nJan 29, 2020\n\n\nZoë Turner\n\n5 min\n\n905 words\n\n\n\n\n\n\n\n\n\n\n\n\nForecasting R\n\n\n\nForecasting\n\n\n\n\n\n\nJan 8, 2020\n\n\nBahman Rostami-Tobar\n\n6 min\n\n1,180 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe first ever train the trainer class 2019\n\n\n\nTraining\n\n\n\n\n\n\nJan 3, 2020\n\n\nMohammed A Mohammed\n\n3 min\n\n401 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community Conference II\n\n\n\nPersonal experience\n\n\n\n\n\n\nDec 20, 2019\n\n\nZoë Turner\n\n3 min\n\n577 words\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Leeds\n\n\n\nPersonal experience\n\n\n\n\n\n\nDec 11, 2019\n\n\nLouise Hick\n\n3 min\n\n550 words\n\n\n\n\n\n\n\n\n\n\n\n\nA new kid on the NHS-R block\n\n\n\nPersonal experience\n\n\n\n\n\n\nNov 22, 2019\n\n\nNighat Khan\n\n5 min\n\n864 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS Number Validation\n\n\n\nNHS\n\nPackages\n\n\n\n\n\n\nNov 20, 2019\n\n\nZoë Turner\n\n11 min\n\n2,003 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe NHS-R Conference 2019\n\n\n\nConference\n\n\n\n\n\n\nNov 15, 2019\n\n\nMohammed Amin Mohammed\n\n5 min\n\n817 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community datasets package released\n\n\n\nPackages\n\nData\n\n\n\n\n\n\nOct 10, 2019\n\n\nChris Mainey\n\n3 min\n\n512 words\n\n\n\n\n\n\n\n\n\n\n\n\nHow R changed me as an analyst\n\n\n\nPersonal experience\n\n\n\n\n\n\nSep 11, 2019\n\n\nZoë Turner\n\n6 min\n\n1,124 words\n\n\n\n\n\n\n\n\n\n\n\n\nDygraphs\n\n\n\ndygraphs\n\ndplyr\n\nbase R\n\n\n\n\n\n\nSep 3, 2019\n\n\nZoë Turner\n\n7 min\n\n1,318 words\n\n\n\n\n\n\n\n\n\n\n\n\nExact Matching in R\n\n\n\nR tips\n\nStatistics\n\ndata.table\n\nData\n\n\n\n\n\n\nJul 30, 2019\n\n\nDan Lewer\n\n9 min\n\n1,643 words\n\n\n\n\n\n\n\n\n\n\n\n\nSQL Server Database connections in R\n\n\n\nSQL\n\n\n\n\n\n\nJul 18, 2019\n\n\nChris Mainey\n\n14 min\n\n2,747 words\n\n\n\n\n\n\n\n\n\n\n\n\nCount of working days function\n\n\n\nFunctions\n\n\n\n\n\n\nJul 16, 2019\n\n\nZoë Turner\n\n7 min\n\n1,212 words\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to Funnel Plots\n\n\n\nStatistics\n\nggplot2\n\nbase R\n\n\n\n\n\n\nMay 22, 2019\n\n\nChris Mainey\n\n10 min\n\n1,870 words\n\n\n\n\n\n\n\n\n\n\n\n\nrforhealthcare.org – A free online resource for all your R related healthcare needs\n\n\n\nResources\n\n\n\n\n\n\nApr 18, 2019\n\n\nSean Manzi\n\n3 min\n\n582 words\n\n\n\n\n\n\n\n\n\n\n\n\nMoving on with the plan… And using R to do it! My personal experience…\n\n\n\nPersonal experience\n\n\n\n\n\n\nApr 11, 2019\n\n\nVicki Cruze\n\n4 min\n\n685 words\n\n\n\n\n\n\n\n\n\n\n\n\nFormat ONS spreadsheet\n\n\n\nR tips\n\ndplyr\n\n\n\n\n\n\nMar 15, 2019\n\n\nZoë Turner\n\n8 min\n\n1,571 words\n\n\n\n\n\n\n\n\n\n\n\n\nUsing R to track NHS winter pressures\n\n\n\nNHS\n\nggplot2\n\ndplyr\n\nbase R\n\nData\n\n\n\n\n\n\nMar 14, 2019\n\n\nFiona Grimm\n\n38 min\n\n7,434 words\n\n\n\n\n\n\n\n\n\n\n\n\nAnimated Population pyramids in R: part 1\n\n\n\nbase R\n\nVisualisations\n\ntidyverse\n\nggplot2\n\nPopulation\n\n\n\n\n\n\nMar 12, 2019\n\n\nAnastasiia Zharinova\n\n10 min\n\n1,869 words\n\n\n\n\n\n\n\n\n\n\n\n\nAnimating a Graph Over Time in Shiny\n\n\n\nR tips\n\nshiny\n\n\n\n\n\n\nMar 1, 2019\n\n\nDan Mohamed\n\n4 min\n\n638 words\n\n\n\n\n\n\n\n\n\n\n\n\nA run chart is not a run chart is not a run chart\n\n\n\nRun charts\n\nStatistics\n\n\n\n\n\n\nFeb 26, 2019\n\n\nJacob Anhøj\n\n18 min\n\n3,532 words\n\n\n\n\n\n\n\n\n\n\n\n\nA simple function to create nice correlation plots\n\n\n\nFunctions\n\nStatistics\n\n\n\n\n\n\nJan 31, 2019\n\n\nGary Hutson\n\n11 min\n\n2,116 words\n\n\n\n\n\n\n\n\n\n\n\n\nFrom script-based development to function-based development and onwards to Package Based development: part 2\n\n\n\nPackages\n\nFunctions\n\n\n\n\n\n\nJan 7, 2019\n\n\nAndrew Hill\n\n9 min\n\n1,614 words\n\n\n\n\n\n\n\n\n\n\n\n\nRoadmap to collaborative working using R in the NHS: Part I- Workflows\n\n\n\nWorkflow\n\n\n\n\n\n\nJan 7, 2019\n\n\nSebastian Zeki\n\n11 min\n\n2,007 words\n\n\n\n\n\n\n\n\n\n\n\n\nBut this worked the last time I ran it!\n\n\n\nPackages\n\nDebugging\n\nPublic Health\n\n\n\n\n\n\nDec 20, 2018\n\n\nZoë Turner\n\n5 min\n\n989 words\n\n\n\n\n\n\n\n\n\n\n\n\nLocal Public Health joins the paRty\n\n\n\nReflections\n\nPublic Health\n\n\n\n\n\n\nNov 22, 2018\n\n\nAndy Evans\n\n3 min\n\n525 words\n\n\n\n\n\n\n\n\n\n\n\n\nThoughts on the NHS-R conference\n\n\n\nConference\n\n\n\n\n\n\nNov 15, 2018\n\n\nJohn MacKintosh\n\n3 min\n\n457 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Conference: was it worth it?\n\n\n\nConference\n\n\n\n\n\n\nOct 26, 2018\n\n\nZoë Turner\n\n4 min\n\n681 words\n\n\n\n\n\n\n\n\n\n\n\n\nText Mining – Term Frequency analysis and Word Cloud creation in R\n\n\n\nText Mining\n\nConference\n\n\n\n\n\n\nOct 22, 2018\n\n\nGary Hutson\n\n7 min\n\n1,300 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Community Conference: My experience of the day\n\n\n\nConference\n\n\n\n\n\n\nOct 16, 2018\n\n\nNeil Pettinger\n\n4 min\n\n742 words\n\n\n\n\n\n\n\n\n\n\n\n\nFrom script-based development to function-based development and onwards to Package Based development\n\n\n\nFunctions\n\n\n\n\n\n\nOct 15, 2018\n\n\nAndrew Hill\n\n6 min\n\n1,175 words\n\n\n\n\n\n\n\n\n\n\n\n\nA simple function to install and load packages in R\n\n\n\nPackages\n\n\n\n\n\n\nAug 17, 2018\n\n\nGary Hutson\n\n2 min\n\n388 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS open source public datasets – creating realistic synthetic datasets\n\n\n\nSynthetic dataset\n\nData\n\n\n\n\n\n\nAug 16, 2018\n\n\nSebastian Zeki\n\n3 min\n\n588 words\n\n\n\n\n\n\n\n\n\n\n\n\nHow to extrapolate data from data\n\n\n\nStrings\n\n\n\n\n\n\nJul 23, 2018\n\n\nSebastian Zeki\n\n3 min\n\n555 words\n\n\n\n\n\n\n\n\n\n\n\n\nEvolution of the R user\n\n\n\nReflections\n\n\n\n\n\n\nJul 13, 2018\n\n\nSeb Fox, Julian Flowers and Georgina Anderson, Public Health England.\n\n6 min\n\n1,118 words\n\n\n\n\n\n\n\n\n\n\n\n\nEven Simpler SQL\n\n\n\nR tips\n\nSQL\n\ndplyr\n\nPatient Flow\n\n\n\n\n\n\nJun 27, 2018\n\n\nJohn MacKintosh\n\n4 min\n\n744 words\n\n\n\n\n\n\n\n\n\n\n\n\nSimpler SQL with {dplyr}\n\n\n\nR tips\n\nSQL\n\ndplyr\n\nPatient Flow\n\n\n\n\n\n\nJun 7, 2018\n\n\nJohn MacKintosh\n\n6 min\n\n1,153 words\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram with auto binning in ggplot2\n\n\n\nR tips\n\nggplot2\n\nbase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n3 min\n\n497 words\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Bar Charts – Plotting Variance with ggplot2\n\n\n\nR tips\n\nggplot2\n\nbase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n5 min\n\n938 words\n\n\n\n\n\n\n\n\n\n\n\n\nDiverging Dot Plot and Lollipop Charts – Plotting Variance with ggplot2\n\n\n\nR tips\n\nggplot2\n\nbase R\n\n\n\n\n\n\nMay 24, 2018\n\n\nGary Hutson\n\n4 min\n\n669 words\n\n\n\n\n\n\n\n\n\n\n\n\nWhy Government needs sustainable software too\n\n\n\nRAP\n\nNHS\n\n\n\n\n\n\nMay 24, 2018\n\n\nMatthew Upson\n\n10 min\n\n1,914 words\n\n\n\n\n\n\n\n\n\n\n\n\nR studio shortcuts\n\n\n\nR tips\n\n\n\n\n\n\nMay 21, 2018\n\n\nEmma Vestesson\n\n1 min\n\n129 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe :: operator\n\n\n\nR tips\n\n\n\n\n\n\nMay 21, 2018\n\n\nEmma Vestesson\n\n3 min\n\n412 words\n\n\n\n\n\n\n\n\n\n\n\n\nImporting and exporting Data\n\n\n\nR tips\n\n\n\n\n\n\nMay 15, 2018\n\n\nS Zeki\n\n2 min\n\n226 words\n\n\n\n\n\n\n\n\n\n\n\n\nThe joy of R\n\n\n\nReflections\n\nPublic Health\n\n\n\n\n\n\nApr 9, 2018\n\n\nJulian Flowers\n\n5 min\n\n830 words\n\n\n\n\n\n\n\n\n\n\n\n\nPareto Chart in ggplot2\n\n\n\nR tips\n\nggplot2\n\nbase R\n\n\n\n\n\n\nMar 24, 2018\n\n\nGary Hutson\n\n5 min\n\n853 words\n\n\n\n\n\n\n\n\n\n\n\n\nAiming for a wrangle-free (or reduced) world\n\n\n\nReflections\n\n\n\n\n\n\nMar 23, 2018\n\n\nSebastian Fox\n\n6 min\n\n1,193 words\n\n\n\n\n\n\n\n\n\n\n\n\nNHS meets R\n\n\n\nNHS-R\n\n\n\n\n\n\nMar 19, 2018\n\n3 min\n\n533 words\n\n\n\n\n\nNo matching items\n\n  \n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/roadmap-to-collaborative-working-using-r-in-the-nhs-part-i-workflows.html",
    "href": "blog/roadmap-to-collaborative-working-using-r-in-the-nhs-part-i-workflows.html",
    "title": "Roadmap to collaborative working using R in the NHS: Part I- Workflows",
    "section": "",
    "text": "We finally have a tool that is data independent. R allows the scripting of a data science methodology that we can share and develop and which does not necessarily depend on carrying patient data with it. This means that the door to working collaboratively on solutions is now wide open. But before this happens in a truly robust way there are several hurdles that need to be overcome. This is the first part of a series of articles in which I hope to outline the roadmap to networking our solutions in R so we can work collaboratively between Trusts to accelerate data science solutions for healthcare data.\n\nStandardising R Workflows in the NHS\nIn the throws of a new(ish) tool such as R, everybody has a way of working which is slightly different to one another. This is how best practice evolves, but in the end the best practice has to be standardised so that developers can work together. This is an article outlining my best practice for R script organisation and is an invitation for comment to see if there is any interest in developing a NHS standard for working and organising our solutions.\n\n\nPrinciples of R script development\nAs the R development environment is so methods based, it makes sense to have a standardised way to develop scripts so that different developers understand the basic workflow of data and can focus on the specific methodology for the specific problem rather than disentangle endless subsets of data and how each is cleaned and merged and so on. I use various principles when developing a script and useful approach to R script development.\n\n\na) An R script should focus on a specific problem.\nA solution is only as good as the question. Questions come in a variety of scopes and shapes and there is an art to asking a solvable question which is beyond the limits of this article.\nHaving defined a question, a script should set out to be the solution to that question and not be a generic answer. Generic answers belong as functions or collections of functions called packages. An R script should tackle specific problems such as ‘How many endoscopies did we perform last year’ and you find that this kind of question is asked a lot (’How many x did we perform last y years) then the script might become a function and a collection of functions might become a package.\n\n\nb) The R script should access the minimal dataset needed and avoid intermediate datasets.\nThere is a real danger with data analysis that the data set used is huge but you only need a part of it. With R you can have the ability to specify the data used from within the script so that you should use the minimum dataset that is pertinent to the question. In fact the whole script should be specific to the question being asked. The data access should, as far as possible also be from the data repository rather than an intermediate dataset. For example you can specify a SQL query from within R to an electronic patient record (EPR) rather than get a data dump from the EPR into, for example, an Excel spreadsheet, and then import the Excel spreadsheet. It’s just more secure and avoids versioning issues with the data dump.\n\n\nc) An R script should be organised according to a standardised template\nAll analysis I perform adheres to a specific workflow for each script so that the script is separated into specific subsections that perform types of actions on the data. This also incorporates the important aspect of commenting on each part of the workflow. This is important so that developers can understand the code further down the line. The workflow I use is as follows:\n## Title ##\n\n## Aim ##\n\n## Libraries ##\n\n## Data access ##\n\n## Data cleaning & Data merging ##\n\n## Data mutating (creating columns from pre-existing data) ##\n\n## Data forking (filtering and subsetting) ##\n\n## Documenting the sets (usually creation of consort type diagrams with diagrammeR##\n\n## Documenting the code with CodeDepends ##\n\n## Data analysis ##\n\n\n\nData access\nThe title of the script including the author and date is written at the top. The aim of the script is then stated along with any explanation (why am I doing this and so on). The workflow makes sure that all libraries are loaded at the beginning. Data access is also maintained at the top so anyone can immediately see the starting point for the analysis. Data access should specify the minimal dataset needed to answer the specific question of the script as explained above. For example there is no point using a dataset of all endoscopies between 2001 and 2011 when your script is only looking at colonoscopy specifically. I also try to avoid functions such as file.choose() as I like to keep the path to the source file documented, whether it is a local or remote repository.\n\n\nData cleaning & Data merging\nThe difficult task of data cleaning and merging with other datasets is then performed. One of my data principles is that when working with datasets you should start with the smallest dataset that answers all of your questions and then filter down for each question rather than build up a dataset throughout a script, so I like to merge external datasets early when possible. This could be called early binding technically but given the data access part of the code specifies a data set that is question-driven, I am early binding to a late-bound set (if that makes sense).\n\n\nData mutating\nOnce cleaning and merging is performed, subsets of data for specific subsections of the question can be done and then the statistical analysis performed on each subset as needed.\nI find it very useful to keep track of the subsets of data being used. This allows for sanity checking but also enables a visual overview of any data that may have been ‘lost’ along the way. I routinely use {diagrammeR} for this purpose which gives me consort type diagrams of my dataset flows.\nThe other aspect is to examine the code documentation and for this I use {codeDepends} which allows me to create a flow diagram of my code (rather than the data sets). Using {diagrammeR} and {codeDepends} allows me to get an overview of my script rather than trying to debug line by line.\n\n\n{codeDepends} is no longer on CRAN but this may be the original repository https://github.com/duncantl/CodeDepends\n\nR scripts should exist with a standardised folder structure for each script\n\nR scripts often exist within a project. You may be outputting image files you want access to later, as well as needing other files. R studio maintains R scripts as a project and creates a file system around each script. There are several packages that will also create a file dependency system for a script so that at the very least the organisation around the R script is easy to navigate. There are several ways to do this and some packages exist that will set this up for you.\n\n\ne) R files should have a standard naming convention.\nThis is the most frustrating problem when developing R scripts. I have a few scripts that extract text from medical reports. I also have a few scripts that do time series analysis on patients coming to have endoscopies. And again a few that draw Circos plots of patient flows through the department. In the end that is a lot of scripts. There is a danger of creating a load of folders with names like ‘endoscopyScripts’ and ‘timeSeries’ that don’t categorise my scripts according to any particular system. The inevitable result is lost scripts and repeated development. Categorization and labelling systems are so important so you can prevent re-inventing the same script. As the entire thrust of what I do with R is in the end to develop open source packages, I choose to name scripts and their folders according to the questions I am asking. The naming convention I use is as follows\nTop level folder: Name according to question domains (defined by generic dataset)\nScript name: Defined by question in the script dataset_FinalAnalysis\nThe R developer will soon come to realise the question domains that are most frequently asked and I would suggest that this is used as the naming convention for top-level folders. I would avoid categorising files according to the method of analysis. As an example, I develop a lot of scripts for the extraction of data from endoscopies. In general I either do time series analysis on them or I do quality analysis. The questions I ask of the data are things like: ‘How many colonoscopies did we do last year’ or ‘How are all the endoscopists performing when measured by their diagnostic detection rates for colonoscopy’. I could name the files ‘endoscopyTimeSeries’ or ‘endoscopyQualityAssess’ but this mechanistic labelling doesn’t tell me much. By using question based labelling I can start to see patterns when looking over my files. According to my naming convention I should create a folder called ‘Endoscopy’ and then the script names should be ‘Colonoscopies_DxRate and ’Colonoscopies_ByYear’. The next time I want to analyse a diagnostic rate, maybe for a different data set like gastroscopies, I can look through my scripts and see I have done a similar thing already and re-use it.\nIn the end, the role of categorizing scripts in this way allows me to see a common pattern of questions. The re-usability of already answered questions is really the whole point of scripting solutions. Furthermore it allows the deeply satisfying creation of generic solutions which can be compiled into functions and then into packages. This has already been expanded upon here.\n\n\nf) Always use a versioning system for your R scripts\nR scripts need to be versioned as scripts may change over time. A versioning system is essential to any serious attempt at providing solutions. There are two aspects to versioning. Firstly the scripts may change and secondly the packages may change. Dependency versioning can be dealt with by using checkpoints within the scripts. This essentially freezes the dependency version so that the package that worked with the current version of the script can be used. I have found this very useful for the avoidance of script errors that are out of my control.\nThe other issue is that of versioning between developers. I routinely use Github as my versioning system. This is not always available from within Trusts but there are other versioning systems that can be used in house only. Whichever is used, versioning to keep track of the latest workable scripts is essentially to prevent chaos and insanity from descending into the development environment. A further plea for open source versioning is the greater aspiration of opening up R scripts in healthcare to the wider public so that everyone can have a go at developing solutions and perhaps we can start to network solutions between trusts.\n\n\nConclusion:\nThere is a greater aim here, which I hope to expand on in a later article, which is the development of a solutions network. In all healthcare trusts, even outside of the UK, we have similar questions, albeit with different datasets. The building blocks I have outlined above are really a way of standardising in-house development using R so that we can start to share solutions between trusts and understand each other’s solutions. A major step forward in sharing solutions would be to develop a way of sharing (and by necessity categorising) the questions we have in each of our departments. Such a network of solution sharing in the NHS (and beyond) would require a CRAN (or rOpenSci) type pathway of open source building, and validation as well as standardised documentation but once this is done it would represent a step change in analytics in the NHS. The steps I have shared above certainly help me in developing solutions and certainly help in the re-use of what I have already built rather than re-creating solutions from scratch.\nThis blog was written by Sebastian Zeki, Consultant Gastroenterologist at Guy’s and St Thomas’ NHS Foundation Trust.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-low-income-scheme.html",
    "href": "blog/nhs-low-income-scheme.html",
    "title": "NHS Low Income Scheme",
    "section": "",
    "text": "The NHSBSA Data Science team recently published an article investigating the take-up of the NHS Low Income Scheme in England. The NHS Low Income Scheme is a service administered by the NHS Business Services Authority, which provides help to people who are not already entitled to help with health costs.\nWe wanted to try something new to report the findings and tell the story behind the data; and not just create a “usual” dashboard or report. We took inspiration from a scrollytelling article developed by ONS called Exploring local income deprivation. They developed it using Svelte, Layer Cake and Mapbox GL and it inspired our team to try something similar in R, as this is the language our team is familiar with.\nIf you want to develop a scrollytell report using Javascript, you can use the ONS template. In this blog, we outline how we developed a scrollytell article in {shiny}using the {golem} framework. We also discuss the challenges we faced to ensure accessibility and how we overcame them by adapting the NHS.UK frontend toolkit."
  },
  {
    "objectID": "blog/nhs-low-income-scheme.html#golem",
    "href": "blog/nhs-low-income-scheme.html#golem",
    "title": "NHS Low Income Scheme",
    "section": "Golem",
    "text": "Golem\nAs Data Scientists, we look to employ best practice wherever we can. For this complex shiny app, this meant modularising our code by following the {golem} framework, in order to package the app as an R package. We achieved this by keeping each scrollytell section separate and packaging up reusable components into functions and modules. Otherwise, it would be a huge monolithic application that would be horrible to debug and maintain.\nOur team regularly checks the latest news and R tips on the NHSR community website, Slack, and Twitter and we stumbled across Clinical Development Unit Data Science Team’s {experiencesdashboard}. The open sourcing of this dashboard was vital in the development of our own product. We used it as a reference guide and also reused some of the ideas in the code.\nWe have published the data-raw directory (similar to CDU’s repository) but not the data directory. The data_raw directory contains mostly {dbplyr} code that was used to transform and aggregate data from a table containing information on NHS Low Income Scheme applicants / applications. The resulting datasets have statistical disclosure control applied to them to prevent accidental disclosure of sensitive information.\nModularised shiny code is much easier to debug and in my opinion, one of the most useful elements of modularised shiny was its reusability across many different modules. We will demonstrate some examples using the NHS.UK frontend toolkit later in the blog."
  },
  {
    "objectID": "blog/nhs-low-income-scheme.html#scrollytell-in-r",
    "href": "blog/nhs-low-income-scheme.html#scrollytell-in-r",
    "title": "NHS Low Income Scheme",
    "section": "Scrollytell in R",
    "text": "Scrollytell in R\nWe used the {scrollytell} R package to develop the article in {shiny}.\nBelow is the code snippet for the scrollytell container inside app_ui.R. A scrollytell container is made up of a fixed scrolly graph and multiple scrolly sections.\nThe public version of this report does not make full use of the fixed scrolly graph. We use it to differentiate the sections when scrolling down the article. However, please keep your eyes peeled as we are planning to open source more code where you can see how this works.\nscrollytell::scrolly_container(\n              outputId = \"scrolly\",\n              scrollytell::scrolly_graph(),\n              scrollytell::scrolly_sections(\n                scrollytell::scrolly_section(\n                  id = \"02_what_is_lis\",\n                  mod_02_what_is_lis_ui(\"02_what_is_lis_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"03_who_applies_to_lis\",  \n                  mod_03_who_applies_to_lis_ui(\"03_who_applies_to_lis_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"04_applications_over_time\",\n                  mod_04_applications_over_time_ui(\"04_applications_over_time_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"05_what_help_does_lis_provide\",\n                  mod_05_what_help_does_lis_provide_ui(\"05_what_help_does_lis_provide_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"06_take_up_region\",\n                  mod_06_take_up_region_ui(\"06_take_up_region_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"07_take_up_la\",\n                  mod_07_take_up_la_ui(\"07_take_up_la_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"08_spotlight_students\",                 \n                  mod_08_spotlight_students_ui(\"08_spotlight_students_ui_1\")\n                ),\n                scrollytell::scrolly_section(\n                  id = \"09_final_thoughts\",\n                  mod_09_final_thoughts_ui(\"09_final_thoughts_ui_1\")\n                )\n              )\n           )"
  },
  {
    "objectID": "blog/nhs-low-income-scheme.html#accessibility",
    "href": "blog/nhs-low-income-scheme.html#accessibility",
    "title": "NHS Low Income Scheme",
    "section": "Accessibility",
    "text": "Accessibility\nOur app needed to meet the NHS accessibility standard. Accessibility testing was new to us and was not an easy task. And it seemed that we had less control over the shiny app.\nWe were recommended to test using three accessibility tools; WAVE, Lighthouse and Axe Devtools. These are all freely available and easily installed through google extension. The idea was to remove any serious or critical errors from these tests. Some of the errors were easy to fix. For example, we needed to declare page language for accessibility reasons. To fix that issue, we just needed to add one html tag lang = \"en\".\nHowever, some of the errors were hard to fix. For example, web page content should be able to resize to 400% without loss of content or functionality. We initially failed the test as some contents overlapped when we zoomed. To solve the more complicated accessibility issues, we used the NHS.UK frontend site. We downloaded the zip file, modified it slightly (see this norem issue), and added it to the inst/app/www directory.\nNHS.UK frontend toolkit was a great resource as it contained most of the CSS classes we needed. The benefit of using this was that once we applied the proper CSS style our shiny app looked so much like an NHS website! We also referenced this site as well to modify shiny code if necessary. (Examples – NHS.UK frontend design system components). We can show a few examples of how we modified them.\n\nSelectInput: we wanted to have our selectInput similar to the NHS style. We inspected html code from the frontend example page (Select – NHS.UK frontend design system components) and modified the CSS to look like an NHS select Input (rather than default shiny style).\n\nOur shiny code:\n#' selectInput Function\n#'\n#' @importFrom shiny tagList\nnhs_selectInput &lt;- function(inputId,\n                            label,\n                            choices,\n                            selected = NULL,\n                            full_width) {\n  \n  # Create select input\n  nhs_selectInput &lt;- shiny::selectInput(\n    inputId = inputId,\n    label = label,\n    choices = choices,\n    selected = selected,\n    selectize = FALSE\n  )\n  \n  # Hack the CSS to look like an NHS select input\n  nhs_selectInput$attribs$class &lt;- \"nhsuk-form-group\"\n  nhs_selectInput$children[[1]]$attribs$class &lt;- \"nhsuk-label\"\n  \n  if (full_width) {\n    nhs_selectInput$children[[2]]$children[[1]]$attribs$class &lt;- \"nhsuk-select form-control\"\n    nhs_selectInput$children[[2]]$children[[1]]$attribs$style &lt;- \"border-radius: 0;\"\n  } else {\n    nhs_selectInput$children[[2]]$children[[1]]$attribs$class &lt;- \"nhsuk-select\"\n  }\n  tagList(\n    nhs_selectInput\n  )\n}\nNHS toolkit select html code\n&lt;div class=\"nhsuk-form-group\"&gt;\n  &lt;label class=\"nhsuk-label\" for=\"select-1\"&gt;\n    Label text goes here\n  &lt;/label&gt;\n  &lt;select class=\"nhsuk-select\" id=\"select-1\" name=\"select-1\"&gt;\n    &lt;option value=\"1\"&gt;NHS.UK frontend option 1&lt;/option&gt;\n    &lt;option value=\"2\" selected&gt;NHS.UK frontend option 2&lt;/option&gt;\n    &lt;option value=\"3\" disabled&gt;NHS.UK frontend option 3&lt;/option&gt;\n  &lt;/select&gt;\n&lt;/div&gt;\nOutput SelectInput\n\nAction button for download: we wanted to have our download button similar to the NHS style. We inspected html code from the frontend example page (Action Link – NHS.UK frontend design system components) and modified the CSS to look like an NHS action link.\n\nOur shiny code\nmod_nhs_download_ui &lt;- function(id) {\n  ns &lt;- NS(id)\n  tagList(\n    tags$div(\n      style = \"position:relative; height:55px;\",\n      tags$div(\n        class = \"nhsuk-action-link\",\n        style = \"position:absolute; bottom:0; right:0; margin-bottom:0;\",\n        shiny::downloadLink(\n          outputId = ns(\"download\"),\n          class = \"nhsuk-action-link__link\",\n          tags$svg(\n            class = \"nhsuk-icon nhsuk-icon__arrow-right-circle\",\n            ,\n            viewBox = \"0 0 24 24\",\n            `aria-hidden` = \"true\",\n            width = \"36\",\n            height = \"36\",\n            tags$path(\n              d = \"M0 0h24v24H0z\",\n              fill = \"none\"\n            ),\n            tags$path(\n              d = \"M12 2a10 10 0 0 0-9.95 9h11.64L9.74 7.05a1 1 0 0 1 1.41-1.41l5.66 \n              5.65a11 0 0 1 0 1.42l-5.66 5.65a1 1 0 0 1-1.41 0 1 1 0 0 1 \n              0-1.41L13.69 13H2.05A10 10 0 1 0 12 2z\"\n            )\n          ),\n          tags$span(\n            class = \"nhsuk-action-link__text\",\n            \"Download Data\"\n          )\n        )\n      )\n    )\n  )\n}\nNHS toolkit action link html code\n&lt;div class=\"nhsuk-action-link\"&gt;\n  &lt;a class=\"nhsuk-action-link__link\" \n        href=\"https://www.nhs.uk/service-search/minor-injuries-unit/locationsearch/551\"&gt;\n    &lt;svg class=\"nhsuk-icon nhsuk-icon__arrow-right-circle\"  \n         viewBox=\"0 0 24 24\" aria-hidden=\"true\" width=\"36\" height=\"36\"&gt;\n      &lt;path d=\"M0 0h24v24H0z\" fill=\"none\"&gt;&lt;/path&gt;\n      &lt;path d=\"M12 2a10 10 0 0 0-9.95 9h11.64L9.74 7.05a1 1 0 0 1 1.41-1.41l5.66\n      5.65a1 1 0 0 1 0 1.42l-5.66 5.65a1 1 0 0 1-1.41 0 1 1 0 0 1 \n      0-1.41L13.69 13H2.05A10 10 0 1 0 12 2z\"&gt;&lt;/path&gt;\n    &lt;/svg&gt;\n    &lt;span class=\"nhsuk-action-link__text\"&gt;Find a minor injuries unit&lt;/span&gt;\n  &lt;/a&gt;\n&lt;/div&gt;\nOutput download button\nOnce we applied the NHS.UK frontend toolkit we successfully passed the accessibility test.\nPlease take a look at our shiny scrollytell article and check out the code on GitHub. We also created {golem} shinyR template.\nWe were grateful to the NHS-R community as it always inspires us to improve our code and we hope we can contribute back to the community by sharing our code. Thank you for taking the time to read this.\nBlog post from:\nKayoung Goffe (Data Scientist, NHS Business Services Authority)\nAdam Ivison (Data Scientist, NHS Business Services Authority -&gt; Principal Data Scientist, UK Health Security Agency)"
  },
  {
    "objectID": "blog/the-data-science-assembly.html",
    "href": "blog/the-data-science-assembly.html",
    "title": "The data science assembly",
    "section": "",
    "text": "By Sami Sultan, Junior Data Scientist, NHS England\nWho am I?\nIn June 2022, I joined Health Education England as a junior data scientist, armed with a background in R. One year later, I find myself embedded within the NHS with an aim to contribute to both the NHS R community and the data science assembly.\nI completed my PhD in regenerative medicine early 2022. Like many within the field of biology and medicine, I would generate and analyse vast amounts of quantitative datasets. The analysis would include tests for normality and scedasticity, before conducting the most appropriate mono- or multi-variable statistical test. It became quickly apparent that by utilising the power of R, I could create automated statistical pipelines to increase both productivity and reliability.\nDuring my PhD, I focused on hypothesis testing and power calculations. Building on these experiences, I now focus more on predictive modelling within the workforce modelling team, using both R and python.\nMy introduction to the NHS R community\nIf you are enthusiastic about R, it is inevitable that you will know about Zoë Turner and Chris Beeley, pioneers in the NHS R community space. Zoë and Chris introduced me to the NHS R website which includes blogs and event postings, podcasts, YouTube channel, GitHub, and Slack. The latter containing a wealth of information and updates regarding the NHS R community, and a space to connect with other R users for help and advice. More recently, I have also been introduced to the AphA blog which “is a new look for the updates from NHS-R Community”. If there is one thing to take away from this blog post, it is the substantial depth of resources regarding all things R!\nWhat is the data science assembly?\nThe data science assembly is run by Sarah Culkin (Deputy Director, Analytics and Data Science), attended by data scientists who are working within NHSE. This meeting is in place to ensure there is coherent movement of information between the relevant data science communities. Facilitated by the sub-group leads, topic areas such as training, GitHub, publications, and data infrastructure are discussed, providing updates and insights to promote progression and growth.\nMy mission\nThe most significant sub-group within the data science assembly for this blog post are the external communities, including the NHS R community! At the time of joining there were no active leads for the NHS R community. Therefore, I put my name forward with the ambition of bringing the data science assembly and NHS R community closer together. With the help of the limitless resources including the AphA blog and Slack, I will keep the data since assembly up to date with everything R and ensure that the R community has a voice to be heard.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/a-simple-function-to-install-and-load-packages-in-r.html",
    "href": "blog/a-simple-function-to-install-and-load-packages-in-r.html",
    "title": "A simple function to install and load packages in R",
    "section": "",
    "text": "I was starting to despair at the amount of packages I end up using during the task of transforming, cleaning, modelling and validating some of my models. I thought there must be a simple approach to dealing with this?\nSo, I started to ponder if I could create a function that would just install and then load the packages straight into R in one go. I found the solution and it can be applied to all your projects – all you have to do is supply the list of packages to the function and “hey presto!”\nRun the below function and I will explain what this does:\n\ninstall_or_load_pack &lt;- function(pack) {\n  create.pkg &lt;- pack[!(pack %in% installed.packages()[, \"Package\"])]\n\n  if (length(create.pkg)) {\n    install.packages(create.pkg, dependencies = TRUE)\n  }\n\n  sapply(pack, require, character.only = TRUE)\n\n  # I know I should be using purr here, but this is before the Tidyverse is loaded. I know you Tidyverse trend setters will have me here.\n}\n\nThis creates a function, in which, you can pass a vector of packages you want to load. The sweet point with this function is that if the packages are not installed, this function will do that and then load them, so the next time you come to using the function it will just load them into your project – instead of installing them. I said it was cool, or I thought you might find it cooler than I expected, either way I still think it’s cool.\nThe last step is to make a call to the function and specify the list of packages you need:\n\npackages &lt;- c(\"ggplot2\", \"plotly\", \"data.table\", \"tidyverse\", \"caret\")\n\ninstall_or_load_pack(packages)\n\nIf the packages are not installed then this will show an installation series in the console window, otherwise it will just flag a Boolean value to show that they are now active in the project:\n\n   ggplot2     plotly data.table  tidyverse      caret \n      TRUE       TRUE       TRUE       TRUE       TRUE \n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutson Hacks.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/new-year-new-laptop-new-library-location-new-version-of-r.html",
    "href": "blog/new-year-new-laptop-new-library-location-new-version-of-r.html",
    "title": "New year, new laptop, new library location, new version of R!",
    "section": "",
    "text": "Eek? Starting again wasn’t nearly as painful as it used to be, thanks to {installr} and the generosity of people posting solutions online.\nNew year – time to set up the new laptop! Eek? Well, it wasn’t actually that bad! But first, a bit of context.\nI had been working on an old laptop that struggled with resource-intensive tasks. My way around that had been to switch from the laptop to my less-readily-accessible desktop for stuff that needed more oomph. Because two devices means two different package libraries, I had thought I was being smart by having my R library in Dropbox: point R to the right location, and tada!, the packages are always stable across the two machines. Plus, in case of catastrophic failure, it’s easy to start again. Right? Wrong! So, so very wrong!\nKeeping packages in Dropbox is a very bad idea\nIt turns out, this had plenty of disadvantages! From needing to pause Dropbox sync when installing new packages to avoid a LOCK error, to file conflicts between the two machines leading to duplicates that are poorly labelled, Dropbox created way more problems than it solved. To add to the headache, my code was all under version control with Git, but not necessarily all up on GitHub (I didn’t need it because of Dropbox, right?). I soon discovered that what I thought was a clever hack really didn’t have the main advantage I was looking for.\nThe rollercoaster of trusting in a poor system\nI booted up the new laptop, pointed R to the Dropbox library and experienced that fun thing when you open up RStudio and crashes: it had no idea what to do because it couldn’t find the packages it needs in order to launch. Hmm, but they were all there… Or were they?\nIt soon transpired that most of the files were of size 0KB, which isn’t a good sign. Something bad had happened during the Dropbox sync, possibly due to having initially popped the Dropbox folder straight under the C drive — where, fun fact, you don’t really have all the permissions you think you have on your own personal laptop — and then moving it. Not to worry, I could uninstall Dropbox and reinstall it again and all would be well… But it was going to take over 24 hours to sync all the files, and I was getting more and more anxious in the meantime about all the files that looked like they had been corrupted in the process – had they? What state would they be in on the Desktop if I fired that up? Major Eek! So much for that quick fix in case of catastrophic failure!\nThe wiser path\nHaving established this was not the way to go, I took steps to remedy the situation and make my R setup more orthodox. Enter {installr}, a package designed “to make updating R (on windows) as easy as running a function”.\nStep 1: Move the library to a standard location\nTurns out, if you do things in a more standard way, there are good solutions to help you keep up with best practice in a standard way!\nThe first step was to relocate the library so that I could start using packages and functions which were designed precisely for this type of task. I knew I was about to upgrade my version of R, so I would need to reinstall the packages as part of that. So, long story short, I knew I was OK with updating packages rather than sticking to the versions I’d used to build projects. My #Dataviz work doesn’t need to be regenerated, and the stuff I’m doing for clients has been tested in different package environments, so I knew that was safe too.\nFollowing the instructions on this Stackoverflow thread, I typed:\n\nto_install &lt;- unname(installed.packages(lib.loc = .libPaths())[, \"Package\"])\n\ninstall.packages(pkgs = to_install, lib=\"C:/Program Files/R/R-4.0.3/library\")\n\nThose two simple lines installed all my existing packages in a more standard location, dependencies and all! It was much quicker than I thought it would be! We’re talking minutes. Definitely a lot faster than waiting for Dropbox to work its magic! No looking back there! Just count the exclamation marks to get a measure of my surprise.\nStep 2: Update R!\nTo do this, I used {installr} and ran updateR() with the defaults:\n\ninstall.packages(\"installr\")\ninstallr::updateR()\n\nAt this point, I got a warning that it was best to do this in the R Gui rather than in RStudio. Fair enough, close RStudio, open R Gui and run that command again.\nIt then informed me that I had over 450 packages to copy / update. Yikes! But again that was remarkably quick! The process didn’t copy over my .RProfile code chunks which tweak a few things on launching RStudio, but that’s easy enough to copy across manually.\nI checked the .libPaths were back to normal, and they were. This will make future updates way more straightforward! Then I checked for any packages that needed to be updated; unsurprisingly, there were none. This had really worked as well as it says on the tin!\nWell, nearly.\nStep 3: Sort out the graphics devices\nI tested out my new setup on a few known projects, where I knew that I could test the output against what I expected, and encountered the following error message:\n# Error in f(...) : Graphics API version mismatch\nThat looks fun!\nI narrowed it down to the first ggsave() call of the project. I confirmed that it was simply the ggsave() command that was failing and nothing to do with the specific plot by creating a basic plot and trying to save it. A bit of Googling later, I found the solution on the Posit Community Forum: updateR isn’t perfect when it comes to packages that provide graphics devices.\nReinstalling {ragg} did the trick:\n\ninstall.packages(\"ragg\")\n\nMy aim at the time of writing the original post was to not lose time reinstalling individual packages while deadlines were looming. But there’s a lot of wisdom in starting with a clean slate every now and then. Thanks too for pointing out to me the wisdom of the clean slate!. It’s highly unlikely that I’m still using all 450+ of those packages. Some I will have installed to try something and decided not to use, others I will have used in past projects but won’t use again. If I were to do this forever, I’d end up with 1000s of packages, most of which would be taking up space that they shouldn’t be taking up.\nSome recommend starting afresh with every major R upgrade (for example moving from 3.x to 4.x); others prefer a package purge with every R update. There are no hard rules when it comes to this — just make sure you give yourself an opportunity to revisit which packages you actually need every now and then!\nFootnotes\n\nThanks to Chris Beeley for pointing out to me the wisdom of the clean slate!\n\nCitation\nFor attribution, please cite this work as\nThompson (2022, Jan. 11). Building stories with data: New year, new laptop, new library location, new version of R!. Retrieved from https://www.cararthompson.com/posts/2022-01-11-new-year-new-laptop-new-library-location-new-version-of-r/\nThis blog has been edited for NHS-R Style, formatted to remove Latin Abbreviations.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/rforhealthcare.html",
    "href": "blog/rforhealthcare.html",
    "title": "rforhealthcare.org – A free online resource for all your R related healthcare needs",
    "section": "",
    "text": "Down in the South West of England we at the PenCHORD team of the NIHR CLAHRC South West Peninsula have been busy developing some online treats for all of you interested in using the statistical programming language R. The https://rforhealthcare.org/ website is a free resource full of R guides and tutorials designed to get you started with R and RStudio. But we do not stop at simply introducing you to the R language, oh no, we have begun to produce tutorials on more complex topics such as linear regression, performing t-tests, statistical process control and principle component analyses.\nThe rationale behind developing this site has been to create an easily accessible resource for those working in and around the healthcare sector to support you in using R on a day-to-day basis as an alternative to Excel and SQL. The training topics have been selected so they are relevant to the types of tasks carried out by healthcare analysts while explaining things in such a way that you do not need a PhD in computer science to understand what is going on!\nThe website has been designed to be simple and user friendly. If you are completely new to R there is an introductory presentation that you can download on the homepage. This describes what R and RStudio are, how to install them and some basic information about how to go about using R.\nOnce you have R and RStudio installed then it is time to start learning about R functionality. There are currently three main sections to the tutorials: Basic functionality, Plotting and Statistics. The ‘Basic functionality’ section covers everything from introducing data types and data structures to transforming and manipulating data through to writing your own functions. The plotting section introduces you to creating all of the basic types of graphs that you need from histograms to box plots. The statistics section then introduces some useful statistical techniques such as t-tests and linear regression.\nA good way to get started using R in the healthcare environment is to take a simple but repetitive task that you currently do in Excel or SQL and translate it into a script that can be used time and time again. This might be something such as data cleaning and transformation or producing a series of graphs and tables. The list of tutorials will keep growing as we find out which topics you would like to learn to implement in R. So please do get in touch through the website or twitter (@PenCLAHRC) and let us know if there is tutorial you would like us to write for you.\n\n\nHSMA website is now https://arc-swp.nihr.ac.uk/training-type/health-service-modelling-associates-programme-hsma/\nPython healthcare link removed as this now returns a page from DDoS-Guard, a Russian infrastructure company.\nOver the last 18 months we have also created a large amount of training materials on a variety of topics for use on our innovative Health Service Modelling Associates programme (www.health-modelling.org/). One example is the  website which is a free resource for using the programming language python for healthcare related applications. There are topics to help you go from a complete beginner to advanced level programmer in no time! Over the coming months we will be making all of our training materials available for free online in the areas of: introducing simulation modelling in healthcare, problem structuring for simulation modelling, system dynamics modelling, discrete event simulation, network analysis and machine learning, so watch this space!\nThis blog was written by Dr Sean Manzi, Research Fellow at NIHR CLAHRC South West Peninsula.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/evolution-of-the-r-user.html",
    "href": "blog/evolution-of-the-r-user.html",
    "title": "Evolution of the R user",
    "section": "",
    "text": "Image adapted from source: https://www.r-project.org/logo/"
  },
  {
    "objectID": "blog/evolution-of-the-r-user.html#projects",
    "href": "blog/evolution-of-the-r-user.html#projects",
    "title": "Evolution of the R user",
    "section": "Projects",
    "text": "Projects\nExcitement factor: 2 (they aren’t exciting but they are sooooo useful)\n\n\nScreenshot of RStudio Project wizard\n\nUseful link: https://support.rstudio.com/hc/en-us/articles/200526207-Using-Projects\nIt is difficult to share RMarkdown documents so other people can “knit” them. There are issues around working directories and imported files that are saved in a location other people can’t access. Here are some normal conversation starters for people in this situation:\n“Save these files in the same folder as the markdown document…”\nOr\n“Set your working directory to here and save this file here…”\nUsers may have heard of projects at this point but never reaped their benefits. Projects solve many of these problems.\nThere are the added benefits of being able to work on multiple projects at the same time and being able to pick up a project where a user has left off."
  },
  {
    "objectID": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html",
    "href": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html",
    "title": "Towards open health analytics: our guide to sharing code safely on GitHub",
    "section": "",
    "text": "In September 2019, our team at the Health Foundation took a big step towards making our analytics more transparent and reproducible: we launched our GitHub Page and by default, made all our code repositories public. Since then, we’ve uploaded more code, welcomed contributions from external collaborators, and added new projects in progress. And another team at the Health Foundation, the Improvement Analytics Unit, has joined us by opening up their GitHub profile too. Along the journey we’ve learned a great deal and so, we’d like to share our experiences to inspire others.\nLaunching our GitHub page is part of a wider strategy to make our work more open and collaborative, with the twin aims of tackling bigger issues in analytics and sharing learning. We hope this will help accelerate innovation in analytics and data-driven technologies across the health and care ecosystem."
  },
  {
    "objectID": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html#keep-server-architecture-confidential-dont-release-absolute-file-paths",
    "href": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html#keep-server-architecture-confidential-dont-release-absolute-file-paths",
    "title": "Towards open health analytics: our guide to sharing code safely on GitHub",
    "section": "1. Keep server architecture confidential: don’t release absolute file paths",
    "text": "1. Keep server architecture confidential: don’t release absolute file paths\nTo minimise any security risks to our secure data server, we make sure our code does not contain any absolute file paths, for example to access raw data. First, we figured these wouldn’t be useful to anyone else and second, it meant we could avoid disclosing the layout of our secure server.\nIn practice, we had to find a convenient way to refer to file locations in our scripts without using explicit paths. Unfortunately, this wasn’t as easy as setting a default working directory through an R Studio projects (although we would recommend using them anyway), as raw data is often stored in a separate location. We are now using a combination of two approaches:\n\nThe here package ? to refer to files within the project directory. This is preferable to using relative file paths, as it works from within subdirectories and because the file paths will be generated correctly on other operating systems.\nA separate script containing absolute paths to locations outside the project directory, which can be sourced but is never released from our secure server."
  },
  {
    "objectID": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html#keep-code-clean-dont-refer-to-raw-data-or-statistical-results",
    "href": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html#keep-code-clean-dont-refer-to-raw-data-or-statistical-results",
    "title": "Towards open health analytics: our guide to sharing code safely on GitHub",
    "section": "2. Keep code clean: don’t refer to raw data or statistical results",
    "text": "2. Keep code clean: don’t refer to raw data or statistical results\nPotentially the biggest confidentiality risk when releasing code is referring to either the raw data or aggregate results in the comments. Although generally considered bad practice, it is easy to fall into this bad habit and to copy-paste any outputs into a comment while working on a particularly tricky bit of analysis. Once disclosive comments are in the code, it’s easy to overlook them later.\nWe believe that sticking to good habits right from the start is the best way to manage this risk. Knowing that the code will definitely go through Statistical Disclosure Control has helped us to think twice about what we put in the comments. This, in turn, helps our output checkers as they don’t have to spend as much time going through the code.\nAs a result, it’s also motivated us to write more concise, readable and self-explanatory code. A tool that we can highly recommend here is the tidylog package ?. Simply loading the package will print feedback on dplyr and tidyr operations in the R console. In this way, it often eliminates the need for hard-coded checks of these operations."
  },
  {
    "objectID": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html#be-kind-to-colleagues-minimise-friction-during-disclosure-control",
    "href": "blog/towards-open-health-analytics-our-guide-to-sharing-code-safely-on-github.html#be-kind-to-colleagues-minimise-friction-during-disclosure-control",
    "title": "Towards open health analytics: our guide to sharing code safely on GitHub",
    "section": "3. Be kind to colleagues: minimise friction during disclosure control",
    "text": "3. Be kind to colleagues: minimise friction during disclosure control\nReleasing code on a regular basis could increase the time our colleagues spend on Statistical Disclosure Control and their overall workload. To minimise this risk, we make reading and checking our code as straightforward as possible. This, of course, starts with the point we made in the last section (no disclosive comments!), but there are several other things we do:\n\nA consistent and logical structure for individual scripts and for projects; including splitting analysis workflows into smaller portions, rather than working in one enormous script.\nIncreasing readability through consistent formatting and indentation. We found the tidyverse style guide very useful, as well as the Collin Gillespie’s guide to RStudio features that can automatically reformat and indent code. We are also experimenting with tools such as the styler package ? that can flexibly re-format whole scripts or projects.\n\nWe are still working on finding the right balance between the added value of openness, by frequently releasing and sharing code in active development, and the workload this creates for our output checkers. At the very least, we’d like to partially automate this process in the future, and we’d welcome any suggestions for existing tools that could help us."
  },
  {
    "objectID": "blog/predictive-analytics.html",
    "href": "blog/predictive-analytics.html",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "",
    "text": "Hi! This is my first blog post here and I’m excited to have the chance to share my thoughts with you on this topic, as it is something that has seemingly exploded in popularity over the last few years. For context, I currently work as an “Information Development Analyst” within an acute NHS trust. So naturally, I’m constantly exploring new ways of working within healthcare analytics and business intelligence, as well as aspects of data and technology that I might just think are interesting! This post specifically focuses on predictive analytics within R and my journey developing a prediction model within my organisation.\nMachine learning for predictive analytics is becoming increasingly popular (and essential) throughout healthcare, particularly within the NHS. Machine learning models within healthcare can be successfully deployed for a variety of use cases. This area of healthcare analytics is constantly growing as new techniques become more prevalent and machine learning resources become more accessible to information professionals based within NHS organisations. While some of these solutions are deployed on a large scale and require the investment of an abundance of time and money, smaller scale models can often be developed in-house, taking advantage of the wide array of tools that R has on offer for machine learning. In this example, I will show how I developed a random forest model to predict whether or not a patient’s length of stay was going to be less than (&lt;) or greater-than or equal-to (&gt;=) 7 days. The data used to train such a model would be available for most data analysts within the NHS through their trust’s internal electronic patient record systems.\nThe code shown for this particular machine learning task is specific to the context and model discussed in this blog post. This is not a comprehensive guide on how to approach a machine learning task within R or a complete overview of all the relevant considerations necessary. While I hope that readers find this post interesting, anyone considering undertaking something similar should seek the relevant guidance to do so.\n\n\n\n\n\n\nThe objective of this model is to classify whether or not a patient’s length of stay will be &lt; or &gt;= 7 days, rather than predicting the exact number of days!"
  },
  {
    "objectID": "blog/predictive-analytics.html#introduction",
    "href": "blog/predictive-analytics.html#introduction",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "",
    "text": "Hi! This is my first blog post here and I’m excited to have the chance to share my thoughts with you on this topic, as it is something that has seemingly exploded in popularity over the last few years. For context, I currently work as an “Information Development Analyst” within an acute NHS trust. So naturally, I’m constantly exploring new ways of working within healthcare analytics and business intelligence, as well as aspects of data and technology that I might just think are interesting! This post specifically focuses on predictive analytics within R and my journey developing a prediction model within my organisation.\nMachine learning for predictive analytics is becoming increasingly popular (and essential) throughout healthcare, particularly within the NHS. Machine learning models within healthcare can be successfully deployed for a variety of use cases. This area of healthcare analytics is constantly growing as new techniques become more prevalent and machine learning resources become more accessible to information professionals based within NHS organisations. While some of these solutions are deployed on a large scale and require the investment of an abundance of time and money, smaller scale models can often be developed in-house, taking advantage of the wide array of tools that R has on offer for machine learning. In this example, I will show how I developed a random forest model to predict whether or not a patient’s length of stay was going to be less than (&lt;) or greater-than or equal-to (&gt;=) 7 days. The data used to train such a model would be available for most data analysts within the NHS through their trust’s internal electronic patient record systems.\nThe code shown for this particular machine learning task is specific to the context and model discussed in this blog post. This is not a comprehensive guide on how to approach a machine learning task within R or a complete overview of all the relevant considerations necessary. While I hope that readers find this post interesting, anyone considering undertaking something similar should seek the relevant guidance to do so.\n\n\n\n\n\n\nThe objective of this model is to classify whether or not a patient’s length of stay will be &lt; or &gt;= 7 days, rather than predicting the exact number of days!"
  },
  {
    "objectID": "blog/predictive-analytics.html#data-set-preparation-variable-selection",
    "href": "blog/predictive-analytics.html#data-set-preparation-variable-selection",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "Data Set Preparation & Variable Selection",
    "text": "Data Set Preparation & Variable Selection\nThe variables used in the data set to train the model were selected as it was thought they would be useful as indicators for a patient’s length of stay. These included age, elective / non-elective admission, whether the patient was admitted from a nursing home or not, theatre records, the patient’s sepsis status and the presence of particular comorbidities (that is previous diagnosis of dementia, chronic obstructive pulmonary disease, heart failure and diabetes).\nTo allow the model to be trained on this data, all of the categorical variables had to be re-categorised using “one hot encoding”, where the values of each of these variables would then be represented in a separate column identified with a 1 or a 0. This alteration allows the input into the model to be numeric. As only categorical variables needed to be encoded, age did not need to be altered.\n\n\n\n\n\n\nFor example Instead of the comorbidity columns stating “Yes” or “No” for each condition, this would be represented with separate columns. These columns would state whether the patient had each relevant diagnosis with a 1 for true and 0 for false."
  },
  {
    "objectID": "blog/predictive-analytics.html#preparation-of-model",
    "href": "blog/predictive-analytics.html#preparation-of-model",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "Preparation Of Model",
    "text": "Preparation Of Model\nThe R script used to prepare and evaluate this model utilised the following packages: “stats”, “dplyr” and “randomForest”. Initially, the data set was loaded into R from a CSV file and the columns checked (for example formatting and column names). In this example, the CSV file is referred to as “DATASET”. Following this, the data set is split into a testing section and a training section. For this I opted for a split of 70% training and 30% testing. Then, the random forest model is built, specifying that the field “Length.Of.Stay” is the variable I am looking to predict and that I want to make this prediction based on all of the other variables available in the data set. Lastly, the model accuracy is evaluated through a confusion matrix. This model presents an accuracy of 85.11%, when evaluating the predictions made from the testing data set.\n\n\n\n\n\n\nIn this case, set.seed(x) is important in order to make your results reproducible. If a different seed number was used to re-train the model, your final accuracy score may differ, even though the same data set was used for training.\n\n\n\n\n\n\n\n# load packages\nlibrary(stats)\nlibrary(dplyr)\nlibrary(randomForest)\n\n# set seed\nset.seed(10)\n\n# dataset preparation\nResults &lt;- read.csv(\"DATASET\", header = TRUE, stringsAsFactors = TRUE)\n\n# check dataset\nhead(Results)\n\n# split data into testing and training\nindex &lt;- sample(2, nrow(Results), replace = TRUE, prob = c(0.7, 0.3))\n\n# training\nTraining &lt;- Results[index == 1, ]\n\n# testing\nTesting &lt;- Results[index == 2, ]\n\n# build model\nMODEL &lt;- randomForest(Length.Of.Stay ~ ., data = Training)\n\n# evaluate model accuracy through a confusion matrix\nLOS_Pred &lt;- predict(MODEL, Testing)\nTesting$LOS_Pred &lt;- LOS_Pred\n\nCONFUSION_MATRIX &lt;- table(Testing$Length.Of.Stay, Testing$LOS_Pred)\n\nACCURACY &lt;- sum(diag(CONFUSION_MATRIX) / sum(CONFUSION_MATRIX))\nACCURACY"
  },
  {
    "objectID": "blog/predictive-analytics.html#interactivity-for-users",
    "href": "blog/predictive-analytics.html#interactivity-for-users",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "Interactivity For Users",
    "text": "Interactivity For Users\nOnce the model has been created and evaluated for usability, you may want to start thinking about potential methods to make the model accessible within your organisation. Ideally, a user would be able to input values for each variable into the model in as minimal clicks as possible. In this case, I experimented with a shiny application that would likely be the best way for users to input values into the model to receive a prediction.\nHowever, it would not be ideal for the user to have to go through clinical notes and various systems to find the information they need for every variable. The solution arrived at for this issue was that a table would be created in SQL containing all of the relevant information for each variable, for any patients that have been admitted on the trust’s primary electronic patient record system. The relevant information for all of the variables would then be retrieved from various systems. This would be entirely managed within SQL. The data from SQL would be retrieved for use in R with the “odbc” package. In the example shown below, the table that contains a regularly updated feed of daily admissions is referred to as “[Database].[dbo].[Table]”.\n\n# load package\nlibrary(odbc)\n\n# connect to sql via odbc\nmyDSN &lt;- \"LOCAL_DSN_NAME\"\n\n# define query\ncon &lt;- dbConnect(odbc::odbc(), dsn = myDSN)\n\n# define query\nMyQuery &lt;- \"SELECT * FROM [Database].[dbo].[Table]\"\n\n# run sql query\nsql_result &lt;- dbGetQuery(con, MyQuery)\n\nBelow, two examples are shown of test data inputted into the shiny app, one returning a prediction of “0-6 Days” and one returning a prediction of “7(+) Days”.\n0-6 Days\n\n\n\n\n7(+) Days"
  },
  {
    "objectID": "blog/predictive-analytics.html#considerations",
    "href": "blog/predictive-analytics.html#considerations",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "Considerations",
    "text": "Considerations\nHosting for shiny applications is not currently widely available outside of public facing solutions, which would obviously not be appropriate for something such as this (that is information governance concerns).\nOther machine learning model types available in R, such a “logistic regression” or “XGBoost” could have also been applied here for classification, as an alternative to random forest.\nAn NHS organisation may have developed similar machine learning models using alternative methods, such as python rather than R."
  },
  {
    "objectID": "blog/predictive-analytics.html#final-thoughts",
    "href": "blog/predictive-analytics.html#final-thoughts",
    "title": "Predictive Analytics within healthcare - Random Forest models for predicting length of stay",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nIf you are working within business intelligence or data analytics within a healthcare space, you have likely encountered some form of machine learning tool, whether this is something you have developed yourself or made use of from a third-party application. It’s important to acknowledge that there are different scales of how far a machine learning model can be implemented within an organisation. How far an NHS trust integrates such technology may entirely depend on their IT infrastructure, budget and resources. However, the machine learning tools available in R certainly lower the barriers to entry if an organisation did wish to incorporate these methods into their analytics. Lastly, while the technology for projects such as this has certainly become more accessible, the time and consideration regarding variable selection, model testing and performance evaluation is still fundamental for a model to be accurate in its predictions and therefore useful from an operational perspective.\nThank you for taking the time to read this. Hopefully you found this interesting (and potentially even useful)!\nImage from Stock Photos, Royalty Free Laptop with analytics Images | Depositphotos"
  },
  {
    "objectID": "blog/the-nhsnumber-package-and-the-joy-of-sharing-your-niche.html",
    "href": "blog/the-nhsnumber-package-and-the-joy-of-sharing-your-niche.html",
    "title": "The ‘nhsnumber’ package and the joy of sharing your niche",
    "section": "",
    "text": "Being the author of a package with tens of thousands of users must be incredibly rewarding. All those people getting value from your work and using it to do incredible things. Few of us will ever write a package that has that kind of reach though.\nMost of us must be content to give back to our communities in smaller ways.\nIn 2019 I was working for a company building software for Genomics England and the NHS. We had many conversations about NHS numbers and NHS Spine and so on, and over time, I became interested in the numbers themselves.\nAs a manager, I wasn’t directly involved in writing any code, but was still heavily involved in the R community in my free time. So I wrote some code to validate the checksums used by NHS numbers to get a better understanding of how they work and to play with some R.\nNHS numbers use a fairly simple format. The first 9 characters are the actual number and the 10th digit is a checksum. A checksum is some data that is used to verify another bit of data. In the case of NHS numbers, it’s the last digit. That 10th digit is generated by an algorithm that takes the first 9 digits as it’s input. This means you can check the validity of a number by taking the first 9 digits, computing the checksum and comparing that with the provided checksum. If they match, the number is valid.\nTaken together, all 10 digits form the complete NHS number.\n\nlibrary(nhsnumber)\n\n## Warning: package 'nhsnumber' was built under R version\n## 4.1.3\n\n# Take a made up number and generate a checksum\nget_checksum(123456788, full_output = TRUE)\n\n[1] 1234567881\n\n# Take that output and a version of the same input number with an\n# incorrect checksum and test their validity\nis_valid(c(1234567881, 1234567882))\n\n[1]  TRUE FALSE\n\n\nCredit card numbers, and many other numbers found out in the wild, use the same technique, though often with different algorithms (the Luhn algorithm in the case of credit card numbers). This makes it possible for us to validate the legitimacy of numbers before we pass them on to upstream services for final validation and association with the human it was assigned to..\nOf course, in most cases, the algorithms are well known, and there’s nothing to stop people generating fake numbers that pass checksum validation. However, this early stage checksum validation can be used to flag typos and transcription errors in a number, or weed out obvious chancers.\nOnce I had an implementation figured out, I wrapped it up into an R package, dropped it on GitHub, tweeted about it…\n …and then promptly forgot about it.\nOver the years I’ve written a lot of super-niche and one-off R packages , principally for my own entertainment, and this felt like another one of those.\nCut to a year later and I’m working for RStudio, helping out a little at Data Orchard and spending a lot of time thinking about the data science community and ways to give back to the community that’s always been so generous to me. It was at this point that I decided to make the effort to publish the package to CRAN.\nGetting your first package on CRAN can be a nerve wracking experience, but it was a fairly smooth process, with only small changes required by the CRAN team before it could be published.\nWhen you publish any package, you never really know if anyone will use it. You’re pushing your work out into the world to see if it can survive on its own. After nhsnumber was published, I would occasionally check its stats and see low, but consistent numbers of downloads. A couple of times since it was published, actual users have reached out to say thanks or report a bug.\nAs a stats-first language rather than a general purpose one, some might say that R is itself, somewhat niche (though a pretty large niche, it must be said!). Add to that a package that only makes sense in one geographic region and is also specific to those working within and alongside one specific organisation within that region and we’re knee-deep in niches! Clearly this sort of package is never going to be applicable to all R users.\nBut none of that means a package isn’t valuable. If only one user benefits from its existence I’d consider that a success. If your work can serve a community, however small, and help improve their work in some way, I consider that a win. So, if like me, you have ideas for R packages, but you consider them too niche to be worth the time, I’d encourage you to share them in some way anyway. You’ll learn a lot along the way, maybe have a bit of fun thinking about how best to organise and present your package and may, just may, improve someone else’s life along the way.\nHappy coding everyone!\nMark\nPS. Because of the interest shown in the nhsnumber R package by the NHS-R community, I decided it would be fun to port the package to Python too.\nMark Sellors is a technologist working in the data science and technical computing space. He is the author of several niche packages for R as well as the Field Guide To The R Ecosystem. By day he works as a Solutions Engineering Manager at RStudio, is a Non-executive director at Data Orchard and the founder of the R4pi.org project.\nThe other R packages that Mark has published to CRAN are:\n\nenvstat: Configurable reporting for your compute environment\nrlog: Simple, opinionated logging for your R scripts and Shiny apps\n\nYou can find Mark on Twitter and LinkedIn and read his infrequently updated blog.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/july-blog.html",
    "href": "blog/july-blog.html",
    "title": "July Blog",
    "section": "",
    "text": "Back in June I attended a Government Data Science (GDS) Toolshed meetup for Public Sector and cross Government, the name coming from the idea that we have tools that we’ve built in our teams that we’d like to share. Matt Dray skilfully found a metaphor (and corresponding pictures) to explain this idea and I got the chance to showcase NHS-R GitHub repository and invite people to join us in our Toolshed!\nInspired by this way of thinking about the NHS-R Community GitHub I’ll be doing a workshop repeating this message for the upcoming DataConnect23 which is open to Public Sector and Cross Government. My hope is that I can get the message out to as many people as I can about how this is a place where you can get involved as much, or as little, as you like. We have a toolshed where people can practice, play and use tools we’ve created."
  },
  {
    "objectID": "blog/july-blog.html#how-do-you-get-support-and-recognition-for-your-work-building-the-community",
    "href": "blog/july-blog.html#how-do-you-get-support-and-recognition-for-your-work-building-the-community",
    "title": "July Blog",
    "section": "How do you get support and recognition for your work building the community?",
    "text": "How do you get support and recognition for your work building the community?\nI can’t clarify if this was a particularly directed question for me as an individual or more about anyone wishing to get involved with NHS-R (or any community) but I’ll assume it was the latter as that covers me too!\nGetting time allocated to working on what are seemingly side projects, or getting recognition for your efforts can definitely feel strange and I’ve never formally requested or put forward anything like a business case to do this. I’ve been lucky to have a manager who could see the value in the work that NHS-R Community do and how that will consequently benefit my work as well as ultimately contribute to making the NHS better.\nTalking about the NHS-R Github like a Toolshed was a bit of an epiphany for me because the things we make are sometimes highly polished things (the packages {NHSRdatasets} and {NHSRplotthedots} both on CRAN are very professional projects) whilst many other repositories are just set up or very much work in progress. Sometimes things get started and then abandoned and that’s ok as we can archive things in GitHub so they are not lost but are tidied away!\nI, and others, have been able to do things which were not directly related to our work, like teaching and creating websites/books using R, but I’ve certainly gone on to use those skills in my work and I’m sure others have done too. Quantifying and justifying why this is useful will take time in itself and the question is, is that useful when I could be learning through doing?"
  },
  {
    "objectID": "blog/july-blog.html#what-lessons-did-you-learn-in-growing-a-community",
    "href": "blog/july-blog.html#what-lessons-did-you-learn-in-growing-a-community",
    "title": "July Blog",
    "section": "What lessons did you learn in growing a community?",
    "text": "What lessons did you learn in growing a community?\nFor me it’s the realisation that work is fun and rewarding because of people and their kindness. I wouldn’t have known that I needed something like the NHS-R Community before it existed. In fact, I only came to know about it because a colleague mentioned there were some free R courses by NHS-R back in 2018. That could have been the end of my involvement but the work of passionate people to get the community up and running meant more opportunities were created where I could get involved and no one said no I couldn’t.\nSo frequently we have things “dictated” to us in our work but with communities, it’s more equal. Yes, some people will know more things but if you can work combine your skills and experiences we can make an even greater impact."
  },
  {
    "objectID": "blog/july-blog.html#do-you-think-it-was-hindrance-to-name-it-nhs-r-or-did-that-help-build-the-community-at-the-beginning",
    "href": "blog/july-blog.html#do-you-think-it-was-hindrance-to-name-it-nhs-r-or-did-that-help-build-the-community-at-the-beginning",
    "title": "July Blog",
    "section": "Do you think it was hindrance to name it NHS-R? Or did that help build the community at the beginning?",
    "text": "Do you think it was hindrance to name it NHS-R? Or did that help build the community at the beginning?\nYes, it probably helped to rapidly build a trusted “brand” but now we are conscious that the name “NHS” and “R” can feel exclusionary. Whilst there have never been strict boundaries to who can get involved we still need to work hard to tell people we are more than NHS and interested in more than R. Perhaps one day we won’t have to do this, maybe we’ll need a rebrand, but whatever the solution there is nothing like word of mouth to get the message out.\nHow we fit in with other communities is difficult too, not because we don’t get on but because we want to be inclusive and so overlap but we don’t want to take over. There are many people who you’ll find in all three communities: NHS-R, NHS Pycom and Government Data Science (me for example) who get something different from each group. I personally have all three Slack channels open and keep an eye on all the conversations because I’m curious but I’m most active in NHS-R and Government Data Science Slack groups because that’s where I’m most confident in my own knowledge of NHS and R."
  },
  {
    "objectID": "blog/july-blog.html#join-the-slack-groups",
    "href": "blog/july-blog.html#join-the-slack-groups",
    "title": "July Blog",
    "section": "Join the Slack groups",
    "text": "Join the Slack groups\nDetails are in the Open Analytics Resources and feel free to contact NHS-R Community via email at nhs.rcommunity@nhs.net"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html",
    "href": "blog/nhs-r-newscast-6th-july-2022.html",
    "title": "NHS-R newscast 6th July 2022",
    "section": "",
    "text": "Really good 1hr overview video from the Oxford Internet Institute to the Goldacre Morley report, gov initial response, and some good Q&A.\nhttps://twitter.com/jessRmorley/status/1542110796540428289\n\n\n\nCommunity building\n\nDrop in sessions\nRegular updates on GitHub issues and work triage\nMore involvement of and communication with fellows\nRegular, structured communication programme featuring tweets, blogs, and podcasts\n\nExperiential learning\n\nMentoring scheme\n\n\n\nNHS data science accelerator\nRun and co-run NHS-R training\nNHS-R solutions can include a component where training/ supervising/ assisting the team delivering them is included\n\nWriting code, software, and training\n\nOffer an “out of the box” NHS-R solution where software is provided to organisations that request it\nContributing to policy"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html#goldacre-review",
    "href": "blog/nhs-r-newscast-6th-july-2022.html#goldacre-review",
    "title": "NHS-R newscast 6th July 2022",
    "section": "",
    "text": "Really good 1hr overview video from the Oxford Internet Institute to the Goldacre Morley report, gov initial response, and some good Q&A.\nhttps://twitter.com/jessRmorley/status/1542110796540428289\n\n\n\nCommunity building\n\nDrop in sessions\nRegular updates on GitHub issues and work triage\nMore involvement of and communication with fellows\nRegular, structured communication programme featuring tweets, blogs, and podcasts\n\nExperiential learning\n\nMentoring scheme\n\n\n\nNHS data science accelerator\nRun and co-run NHS-R training\nNHS-R solutions can include a component where training/ supervising/ assisting the team delivering them is included\n\nWriting code, software, and training\n\nOffer an “out of the box” NHS-R solution where software is provided to organisations that request it\nContributing to policy"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html#nhsrplotthedots",
    "href": "blog/nhs-r-newscast-6th-july-2022.html#nhsrplotthedots",
    "title": "NHS-R newscast 6th July 2022",
    "section": "NHSRplotthedots",
    "text": "NHSRplotthedots\nGoing to have a bit of a push towards version 0.2 by conference time.\nAny users who have quibbles / potential bugs, now’s the time to talk / raise issues / get them understood and addressed.\nAuto-rebasing. Discussion about pros and cons https://github.com/nhs-r-community/NHSRplotthedots/issues/151\nAny users who have some time to help with writing material for a training workshop?\nhttps://github.com/nhs-r-community/NHSRplotthedots/issues"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html#drop-in-session",
    "href": "blog/nhs-r-newscast-6th-july-2022.html#drop-in-session",
    "title": "NHS-R newscast 6th July 2022",
    "section": "Drop in session",
    "text": "Drop in session\nwas a great success, next one early August TCB for details see #announcements in Slack"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html#screencasts",
    "href": "blog/nhs-r-newscast-6th-july-2022.html#screencasts",
    "title": "NHS-R newscast 6th July 2022",
    "section": "Screencasts",
    "text": "Screencasts\nGreat for learning, this is a curated list of screencasts following #tidytuesday solutions, indexed in various ways so you can find the things that might interest you, or that you want to learn about.\nhttps://twitter.com/OscarBaruffa/status/1526154552201854977\nBig Book of R https://www.bigbookofr.com/"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html#rap-strategy",
    "href": "blog/nhs-r-newscast-6th-july-2022.html#rap-strategy",
    "title": "NHS-R newscast 6th July 2022",
    "section": "RAP strategy",
    "text": "RAP strategy\nhttps://analysisfunction.civilservice.gov.uk/policy-store/reproducible-analytical-pipelines-strategy/"
  },
  {
    "objectID": "blog/nhs-r-newscast-6th-july-2022.html#demand-and-capacity",
    "href": "blog/nhs-r-newscast-6th-july-2022.html#demand-and-capacity",
    "title": "NHS-R newscast 6th July 2022",
    "section": "Demand and capacity",
    "text": "Demand and capacity\nVery early days, some code here https://github.com/nhs-r-community/shinyDemand. Join the #demand-and-capacity channel on Slack to get involved\nSend an email! nhs.rcommunity@nhs.net"
  },
  {
    "objectID": "blog/reflections-on-the-nhs-r-community-conference.html",
    "href": "blog/reflections-on-the-nhs-r-community-conference.html",
    "title": "Reflections on the NHS-R Community Conference",
    "section": "",
    "text": "Hello, I’m Chandan Kaur, an analyst currently working at UHB NHS Trust, looking to employ more advanced analytical techniques into my work, now that the core BI stuff is out the way (it’s been a long road…). \nI first used R during my MSc Statistics course, but not had much opportunity to use it since. \nI’ve finally found a community of like-minded individuals who can help me on my R discovery journey and continue to inspire me.\nI attended this year’s NHS-R conference (last attended conference in November 2019 as a 2-month-old newbie to the NHS, so was a bit lost admittedly).\nMy reflections on my 1st day of NHS-R Community conference, 16th November 2022: \n1) Even more inspired to use R for different use case scenarios; my mind is working overtime.\n2) Totally supportive of the collaborative and engaged spirit of the community.\n3) Being surrounded by super intelligent people is very cool.\n4) Next goal – join NHS-R academy if they will allow me. ??\n5) Python set of sessions going on also.\n6) Biggest take-away, #Be Agile, use whichever tool works best for the task\nI particularly enjoyed the sessions by the following speakers on the 16th November:\n\nSteven Wyatt’s “Activity and capacity modelling for the New Hospital Programme” (I have lots of questions I still need to ask, I will be in touch Steven).\n\n\n\nChristopher Reading-Skilton’s Pythia – RTT Acute Trust Waiting List Modelling using Simmer (I want to build something like that and I was particularly in awe of his self-knitted top, do you take knitting orders Chris?)\n\n\n\nRichard Wilson’s Reproducible Report Production using R markdown: an example using Shiny (I really want to scan that QR code again).\n\nOn the 17th Nov 2022 (Quick flying visit during lunch to meet some of the HEU team; they’re so lovely).\nI watched David Sgorbati’s “R and Python: Integrated workflows” session afterwards on YouTube; his enthusiasm for his field is totally infectious.\nAlso I’m still waiting for Chris Beeley’s promised song compilation as promised on the NHS-R Slack group! ???\nSee videos and presentations from the NHS-R Conference 2022 on YouTube"
  },
  {
    "objectID": "blog/reflections-on-the-nhs-r-community-conference.html#th-and-17th-november-2022",
    "href": "blog/reflections-on-the-nhs-r-community-conference.html#th-and-17th-november-2022",
    "title": "Reflections on the NHS-R Community Conference",
    "section": "",
    "text": "Hello, I’m Chandan Kaur, an analyst currently working at UHB NHS Trust, looking to employ more advanced analytical techniques into my work, now that the core BI stuff is out the way (it’s been a long road…). \nI first used R during my MSc Statistics course, but not had much opportunity to use it since. \nI’ve finally found a community of like-minded individuals who can help me on my R discovery journey and continue to inspire me.\nI attended this year’s NHS-R conference (last attended conference in November 2019 as a 2-month-old newbie to the NHS, so was a bit lost admittedly).\nMy reflections on my 1st day of NHS-R Community conference, 16th November 2022: \n1) Even more inspired to use R for different use case scenarios; my mind is working overtime.\n2) Totally supportive of the collaborative and engaged spirit of the community.\n3) Being surrounded by super intelligent people is very cool.\n4) Next goal – join NHS-R academy if they will allow me. ??\n5) Python set of sessions going on also.\n6) Biggest take-away, #Be Agile, use whichever tool works best for the task\nI particularly enjoyed the sessions by the following speakers on the 16th November:\n\nSteven Wyatt’s “Activity and capacity modelling for the New Hospital Programme” (I have lots of questions I still need to ask, I will be in touch Steven).\n\n\n\nChristopher Reading-Skilton’s Pythia – RTT Acute Trust Waiting List Modelling using Simmer (I want to build something like that and I was particularly in awe of his self-knitted top, do you take knitting orders Chris?)\n\n\n\nRichard Wilson’s Reproducible Report Production using R markdown: an example using Shiny (I really want to scan that QR code again).\n\nOn the 17th Nov 2022 (Quick flying visit during lunch to meet some of the HEU team; they’re so lovely).\nI watched David Sgorbati’s “R and Python: Integrated workflows” session afterwards on YouTube; his enthusiasm for his field is totally infectious.\nAlso I’m still waiting for Chris Beeley’s promised song compilation as promised on the NHS-R Slack group! ???\nSee videos and presentations from the NHS-R Conference 2022 on YouTube"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html",
    "href": "blog/importing-and-exporting-data.html",
    "title": "Importing and exporting Data",
    "section": "",
    "text": "This blog originally appeared in http://gastrodatascience.com\nThere are a large number of file types that are able to store data. R is usually able to import most of them but there are some caveats. Below is a summary of methods I use for data imports using the most common file types.\nIt is worth saying that most datasets will come from excel or csv files. It is unusual to gain direct access to the database and these are the normal export types from most data storage systems."
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-csv-or-text",
    "href": "blog/importing-and-exporting-data.html#import-csv-or-text",
    "title": "Importing and exporting Data",
    "section": "Import csv or text",
    "text": "Import csv or text\n\nread.table(\"mydata.txt\", header = T, stringsAsFactors = F)\n\n# or, and using tab as a delimiter:\n\nread_delim(\"SomeText.txt\", \"\\t\", trim_ws = TRUE)\n\n# Maybe get a csv off the internet:\ntbl &lt;- read.csv(\"http://www.example.com/download/data.csv\")\n\nTo prevent strings being imported as factors, add the parameter stringsAsFactors = F"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-from-excel",
    "href": "blog/importing-and-exporting-data.html#import-from-excel",
    "title": "Importing and exporting Data",
    "section": "Import from excel",
    "text": "Import from excel\n\nlibrary(XLConnect)\nwk &lt;- loadWorkbook(\"~Mydata.xlsx\")\ndfw &lt;- readWorksheet(wk, sheet = \"Sheet3\", header = TRUE)\n\n# Alternative and super friendly way\n# For excel imports using readxl package:\nlibrary(readxl)\nread_excel(\"~Mydata.xlsx\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#import-from-database",
    "href": "blog/importing-and-exporting-data.html#import-from-database",
    "title": "Importing and exporting Data",
    "section": "Import from database",
    "text": "Import from database\n\nlibrary(RODBC)\nchannel &lt;- odbcConnect(\"MyDatabase\", believeNRows = FALSE)\n# Get one of the tables\ntbl_PatientDetails &lt;- sqlFetch(channel, \"tblPtDetails\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#export-to-excel",
    "href": "blog/importing-and-exporting-data.html#export-to-excel",
    "title": "Importing and exporting Data",
    "section": "Export to excel",
    "text": "Export to excel\n\nlibrary(XLConnect)\nexc &lt;- loadWorkbook(\"~Mydata.xls\", create = TRUE)\ncreateSheet(exc, \"Input\")\nsaveWorkbook(exc)\nXLConnect::writeWorksheet(exc, mydata, sheet = \"Input\", startRow = 1, startCol = 2)\n\n# Another way is:\nlibrary(xlsx)\nwrite.xlsx(mydata, \"c:/mydata.xlsx\")"
  },
  {
    "objectID": "blog/importing-and-exporting-data.html#export-to-csv-or-a-tab-delimited-file",
    "href": "blog/importing-and-exporting-data.html#export-to-csv-or-a-tab-delimited-file",
    "title": "Importing and exporting Data",
    "section": "Export to csv or a tab delimited file",
    "text": "Export to csv or a tab delimited file\n\nwrite.csv(mydata, file = \"filename\", row.names = FALSE)\nwrite.table(mydata, \"c:/mydata.txt\", sep = \"\\t\")\n\nThere are also many other file types that can be imported and exported but these are the most common so the most practical."
  },
  {
    "objectID": "blog/using-sf-to-calculate-catchment-areas.html",
    "href": "blog/using-sf-to-calculate-catchment-areas.html",
    "title": "Using {sf} to calculate catchment areas",
    "section": "",
    "text": "R is a highly capable GIS tool, thanks to the sf package. In this post I am going to show you how you can use sf with openly available data to calculate catchment areas and population sizes, and then how to calculate travel time isochrones with osrm.\n\nknitr::opts_chunk$set(echo = TRUE)\n\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(readxl)\n  library(glue, include.only = \"glue_data\")\n  library(scales, exclude = c(\"discard\", \"col_factor\"))\n  library(sf)\n  library(leaflet)\n  library(leaflet.extras)\n  library(httr)\n})\n\nFirstly, I have prepared a dataset containing the 23 A&E departments that are Adult Major Trauma Centres (MTCs) in England and Wales. This dataset is an sf object which contains a special column called geometry - this column contains the locations of the hospitals. The data is sourced from this, the Organisation Data Service (ODS) and the NHS Postcode Database.\nWhen we print this table out we get a bit of information telling us how many features (rows) and fields (columns) there are, along with what the type of the geometry is (in this case, we have points). We also get the “bounding box” (bbox) - this is like a rectangle that encloses all of our geometries. Finally, we get the coordinate reference system (CRS) that is being used.\nBelow this we see a table of data - like any other dataframe in R.\n\nmajor_trauma_centres &lt;- read_sf(\n  paste0(\n    \"https://gist.githubusercontent.com/tomjemmett/\",\n    \"ac111a045e1b0a60854d3d2c0fb0baa8/raw/\",\n    \"a1a0fb359d1bc764353e8d55c9d804f47a95bfe4/\",\n    \"major_trauma_centres.geojson\"\n  ),\n  agr = \"identity\"\n)\nmajor_trauma_centres\n\nSimple feature collection with 23 features and 2 fields\nAttribute-geometry relationships: identity (2)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -4.113684 ymin: 50.41672 xmax: 0.1391145 ymax: 54.98021\nGeodetic CRS:  WGS 84\n# A tibble: 23 × 3\n   name                                         org_id               geometry\n   &lt;chr&gt;                                        &lt;chr&gt;             &lt;POINT [°]&gt;\n 1 ADDENBROOKE'S HOSPITAL                       RGT01    (0.1391145 52.17374)\n 2 DERRIFORD HOSPITAL                           RK950    (-4.113684 50.41672)\n 3 HULL ROYAL INFIRMARY                         RWA01   (-0.3581464 53.74411)\n 4 JAMES COOK UNIVERSITY HOSPITAL               RTDCR    (-1.214805 54.55176)\n 5 JOHN RADCLIFFE HOSPITAL                      RTH08    (-1.219806 51.76387)\n 6 KING'S COLLEGE HOSPITAL (DENMARK HILL)       RJZ01  (-0.09391574 51.46808)\n 7 LEEDS GENERAL INFIRMARY                      RR801    (-1.551744 53.80145)\n 8 NORTHERN GENERAL HOSPITAL                    RHQNG     (-1.455966 53.4098)\n 9 NOTTINGHAM UNIVERSITY HOSPITALS NHS TRUST -… RX1RA     (-1.185957 52.9438)\n10 QUEEN ELIZABETH HOSPITAL                     RRK02    (-1.938476 52.45318)\n# ℹ 13 more rows\n\n\nThis dataframe can be used like any other dataframe in R, for example, we can use dplyr to filter rows out we aren’t interested in, or join to another dataframe to bring in extra information.\nFor example, we can load the LSOA population estimates and join it to the LSOA boundaries file from the ONS geoportal. If you aren’t familiar with Output Areas see the ONS Census Geography documentation.\nFirst, let’s load the population estimate file.\n\n# download the LSOA population estimates from ons website: for some reason they\n# provide this download as an excel file in a zip file, so we need to download\n# the zip then extract the file, but we don't need to keep the zip after. withr\n# handles this temporary like file for us\nif (!file.exists(\"SAPE22DT2-mid-2019-lsoa-syoa-estimates-unformatted.xlsx\")) {\n  withr::local_file(\"lsoa_pop_est.zip\", {\n    download.file(\n      paste0(\n        \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/\",\n        \"populationandmigration/populationestimates/datasets/\",\n        \"lowersuperoutputareamidyearpopulationestimates/mid2019sape22dt2/\",\n        \"sape22dt2mid2019lsoasyoaestimatesunformatted.zip\"\n      ),\n      \"lsoa_pop_est.zip\",\n      mode = \"wb\"\n    )\n    unzip(\"lsoa_pop_est.zip\")\n  })\n}\n\nlsoa_pop_estimates &lt;- read_excel(\n  \"SAPE22DT2-mid-2019-lsoa-syoa-estimates-unformatted.xlsx\",\n  \"Mid-2019 Persons\",\n  skip = 3\n) %&gt;%\n  select(LSOA11CD = `LSOA Code`, pop = `All Ages`)\n\nhead(lsoa_pop_estimates)\n\n# A tibble: 6 × 2\n  LSOA11CD    pop\n  &lt;chr&gt;     &lt;dbl&gt;\n1 E01011949  1954\n2 E01011950  1257\n3 E01011951  1209\n4 E01011952  1740\n5 E01011953  2033\n6 E01011954  2210\n\n\nNow we can load the boundary data and join to our population estimates.\n\nlsoa_boundaries &lt;- read_sf(\n  httr::modify_url(\n    \"https://services1.arcgis.com\",\n    path = c(\n      \"ESMARspQHYMw9BZ9\",\n      \"arcgis\",\n      \"rest\",\n      \"services\",\n      \"LSOA_Dec_2011_Boundaries_Generalised_Clipped_BGC_EW_V3\",\n      \"FeatureServer\",\n      \"0\",\n      \"query\"\n    ),\n    query = list(\n      outFields=\"*\",\n      where=\"1=1\",\n      f=\"geojson\"\n    )\n  )\n) %&gt;%\n  # there are other fields in the lsoa data, but we only need the LSOA11CD field\n  select(LSOA11CD) %&gt;%\n  inner_join(lsoa_pop_estimates, by = \"LSOA11CD\") %&gt;%\n  # this helps when combining different sf objects and gets rid of some messages\n  st_set_agr(c(LSOA11CD = \"identity\", pop = \"aggregate\"))\n\nWe can now use ggplot to create a simple plot showing the populations of the LSOAs and add points to show the MTCs.\n\nggplot() +\n  # first plot the lsoa boundaries and colour by the population\n  geom_sf(data = lsoa_boundaries, aes(fill = pop), colour = NA) +\n  geom_sf(data = major_trauma_centres) +\n  scale_fill_distiller(type = \"div\", palette = \"Spectral\",\n                       # tidy up labels in the legend, display as thousands\n                       labels = number_format(accuracy = 1e-1, scale = 1e-3, suffix = \"k\")) +\n  theme_void() +\n  theme(legend.position = c(0, 0.98),\n        legend.justification = c(0, 1)) +\n  labs(fill = \"Population (thousands)\")\n\n\n\n\n\n\n\nWe can now try to calculate the population estimates for each of the MTCs. We can take advantage of {sf} to perform a spatial join between our two datasets. A spatial join is similar to a join in a regular SQL database, except instead of joining on columns we join on geospatial features.\nIn this case we want to take each LSOA and find which MTC it is closest to, so we can use the st_nearest_feature predicate function with st_join. This will give us a dataset containing all of the rows from lsoa_boundaries, but augmented with the columns from major_trauma_centres.\nWe can then use standard dplyr functions to group by the MTC name and org_id fields, and calculate the sum of the pop column.\nsf is clever - it knows that when we are summarising a geospatial table we need to combine the geometries somehow. What it will do is call st_union and return us a single geometry per MTC.\nOne thing we need to do before joining our data though is to transform our data temporarily to a different CRS. Our data currently is using latitude and longitude in a spherical projection, but st_nearest_points assumes that the points are planar. That is, it assumes that the world is flat. This can lead to incorrect results.\nBut, we can transform the data to use the British National Grid. This instead specifies points as how far east and north from an origin. This has a CRS number of 27700. Once we are done summarising our data we can again project back to the previous CRS (4326). This article explains these numbers in a bit more detail.\n\nmtcv_pop &lt;- lsoa_boundaries %&gt;%\n  st_transform(crs = 27700) %&gt;%\n  # assign each LSOA to a single, closest, MTC\n  st_join(\n    st_transform(major_trauma_centres, crs = 27700),\n    join = st_nearest_feature\n  ) %&gt;%\n  # now, summarise our data frame based on MTCs to find the total population\n  group_by(name, org_id) %&gt;%\n  # note: summarise will automatically call st_union on the geometries\n  # this gives us the lsoas as a combined multipolygon\n  summarise(across(pop, sum), .groups = \"drop\") %&gt;%\n  st_transform(crs = 4326)\n\nWe can now visualise this map. This time we will use the leaflet package to create an interactive html map.\nFirst we need a function for colouring the areas.\n\npal_fun &lt;- colorNumeric(\"Spectral\", NULL, n = 7, reverse = TRUE)\n\nAs this is an interactive map we can add popups to the areas to make it easier to identify which MTC we are looking at and to show us the estimated population.\n\np_popup &lt;- glue_data(mtcv_pop,\n                     \"&lt;strong&gt;{name}&lt;/strong&gt; ({org_id})\",\n                     \"Estimated Population: {comma(pop)}\",\n                     .sep = \"\\n\")\n\nNow we can create our map\n\nmtcv_pop %&gt;%\n  leaflet() %&gt;%\n  # add the areas that have been assigned to each MTC\n  addPolygons(stroke = TRUE,\n              color = \"black\",\n              opacity = 1,\n              weight = 1,\n              fillColor = ~pal_fun(pop),\n              fillOpacity = 1,\n              popup = p_popup) %&gt;%\n  # add markers for the MTCs\n  addCircleMarkers(data = major_trauma_centres,\n                   color = \"black\",\n                   stroke = TRUE,\n                   fillColor = \"white\",\n                   weight = 1,\n                   radius = 3,\n                   opacity = 1,\n                   fillOpacity = 1) %&gt;%\n  setMapWidgetStyle(list(background= \"white\"))\n\n\n\n\n\nThis method certainly is not perfect. For example, we can see that the closest MTC for a large part of Somerset is Cardiff, but in order to reach Cardiff you need to cross the Bristol Channel! Not so easy by a Land Ambulance!\nA better approach would be to use some method to calculate travel time from each LSOA to the MTCs, an example of using the Google Maps Travel Time API is covered here. However, as a quick and dirty approach this method doesn’t produce too bad results.\nComments from WordPress site publishing Mike Dunbar\n7 June 2021\nHi Tom, interestingly something has gone wrong with the isochrone data for Derriford Hospital: on the map, Derriford is within the 30-60 minute zone for itself, as is the whole of Plymouth and towns close to the A38 such as Ivybridge, South Brent and Buckfastleigh. Yet large parts of presumably remote Dartmoor are in the 0-30 minute zone! Tom Jemmett\n9 June 2021\nHi Mike! Interesting… the issue is with the resolution of the isochrones. For whatever reason osrm returns something a bit odd for this hospital! Try the code chunk below, may take a few minutes to run as you tweak the res argument. 50 seems to be a good balance. Leaving res off was what was used in the blog post, though adding in res = 30 explicitly seems to give me different results! suppressPackageStartupMessages({ library(tidyverse) library(sf) library(osrm) library(leaflet) library(leaflet.extras) }) deriford &lt;- c(-4.113684, 50.41672) deriford_iso &lt;- osrmIsochrone(deriford, breaks = seq(0, 150, by = 30), returnclass = “sf”, res = 50) %&gt;% mutate(time = factor(paste0(“[”, min, “,”, max, “)”))) pal &lt;- colorFactor(viridis::viridis(length(levels(deriford_iso\\(time))), levels(deriford_iso\\)time)) leaflet() %&gt;% addTiles() %&gt;% addPolygons(data = deriford_iso, fillColor = ~pal(time), color = “#000000”, weight = .5, fillOpacity = 0.5, popup = ~time) %&gt;% addMarkers(lng = deriford[[1]], lat = deriford[[2]]) Spatially weighted averages in R with sf | WZB Data Science Blog\n1 July 2021\n[…] example as circular regions around the points or as Voronoi regions. Additionally, you may consider nearest-feature-joins or travel time isochrones. Which option is more appropriate depends on your […] Nathan Thomas\n12 October 2021\nFascinating work Tom! How easy is it to apply this approach to services other than MTCs - is all that’s needed just a geojson file containing the service locations, or does the file need to be formatted in a certain way? Tom Jemmett\n19 October 2021\nyou should be able to just replace the mtc sf object with some other set of points and it should just work! You could run into runtime issues… I only tried it with the relative small set of mtc’s, if you were to replace this with GP practices it could explode and run for a very long time. You could always try tweaking the size of the isochrones to limit the results! :-) Nathan Thomas\n2 November 2021\nCool, thanks! Will give it a try when time allows. Deon Ware\n20 December 2022\nVery well presented. Every quote was awesome and thanks for sharing the content. Keep sharing and keep motivating others.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/a-roundup-from-the-2022-nhs-r-conference-part-2.html",
    "href": "blog/a-roundup-from-the-2022-nhs-r-conference-part-2.html",
    "title": "A roundup from the 2022 NHS-R Conference – Part 2",
    "section": "",
    "text": "What are the latest health research applications using R? A roundup from the 2022 NHS-R conference.\nR Markdown\nPerformance reports published on a regular basis are common across the NHS. Rather than copying and pasting outputs from Excel to Powerpoint, Richard Wilson made the case for creating an interactive HTML report, using R Markdown, that is reproducible and quick to deliver. A report example is available on Github using the Summary Hospital-level Mortality Indicator (SHMI) dataset.\nImproving the reporting process for the radiotherapy data has been Louise Reynolds’s (NDRS/NHS Digital) priority. Her department’s reporting process that used to be time-consuming and prone to error is now automated and produces standardised output in a few minutes. Relevant packages used for this project are R Markdown, DBI, tidyverse, data.table, and openxlsx.\nDaniel Weiand (Newcastle upon Tyne Hospitals NHS Foundation Trust) wrote entire academic papers using Quarto and R Markdown. Combined with Zotero, it is possible to embed citations into your document. Quarto also has cross-referencing functionalities where it is possible to integrate ggplot with captions into the text.\nR visualisation\nTwo visualisation sessions were run by Cara Thompson, a freelance data consultant. In the first one, she showed us how to build a custom ggplot theme in less than 10 minutes. She highlights three reasons why ggplot themes are useful “-1. They add text hierarchy, -2. They give space to breathe, -3. They create aesthetic consistency across the project”.\nIn the second one, she shared 5 tips to create bespoke colour schemes to plot. “-1. Use colour purposefully, -2. Let others help you, -3. Apply colour using a named vector, -4. Check  for accessibility, and finally -5. Make use of colour interpolation.”\nR tools\nChris Beeley presented Shiny endomineR, a text mining tool for clinical text developed by the NHS-R Community. The package is not finalised yet so keep an eye out for it.\nAcross databases, patient information can be spelled differently and thus making data integration more difficult and time consuming. The NHSBSA Data Science team has developed the addressMatchR package to identify the best address match based on a similarity algorithm.\nRuchir Shah, a radiology registrar in Oxford, was able to automatically extract paediatric lung function data from pdf format to a usable dataframe using the R Tesseract package.\nUseful data\nSamuel Channon-Wells (Imperial College London) used electronic prescribing records to monitor the level of antibiotic use in children. Interactive dashboards were built for Oxford University Hospitals using R and R Shiny and aimed to help infectious paediatric doctors. Implementation challenges include lack of familiarity with R, assimilating existing infrastructure, and maintenance debugging. After a year of use, the initiative allowed to identify and prevent overuse of antibiotics among children patients.\nCalum Polwart (South Tees Hospital NHS Foundation Trust) developed the SACT Analyser package to clean and process systematic anti-cancer treatment data (SACT). SACT is a database gathering information on anti-cancer therapy activity from all NHS providers. The data is published on a monthly basis and is key to understanding treatment patterns and outcomes at a national level. The package is still in early development.\nStatistical Methods\nNHSRplotthedots is a R package built by the NHS-R community to provide tools for drawing SPC charts. It supports NHSE/I programme ‘Making Data Count’ and allows users to draw XmR charts, use change points, apply rules with summary indicators.\nChris Mainey (NHS North Central London ICB) manages to pull a crash course on linear and non-linear regression modelling in less than 15 minutes. Use the mgcv package in base R to build nonlinear regression models with smoothers.\nLaura Moscoviz is a Health Care and Life Science Consultant at RwHealth in London. As part of her role, she uses data science, technology, and predictive analytics to deliver insight-driven solutions to improve quality of care and operational delivery.\nEmail :  laura.moscoviz@realworld.health Twitter: @lhmosco\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-newscast-20th-july-2023.html",
    "href": "blog/nhs-r-newscast-20th-july-2023.html",
    "title": "NHS-R newscast 20th July 2023",
    "section": "",
    "text": "We have another newscast from the podcast, if you don’t do the whole podcast thing here are the notes and links from what we talked about.\nThe NHS-R 2023 Conference tickets are now available for the in-person and virtual event on 17 and 18 October. We’ll be having a side room unconference and the social event was mentioned – details are all to come!\nTom Smith gave an update on the HACA 2023 conference as, although he hadn’t made it to the event himself due to work commitments, he had seen some of the posters. One poster stood out to him from someone in his own Trust who he hadn’t realised was working on a similar project and it showed the power of connections that conferences like this can have in connecting people and projects.\nZoë mentioned (a few times!) about the pull request she’d had accepted from Hadley Wickham for the inclusion of back ticks in the second edition R for Data Science https://github.com/hadley/r4ds/pull/1522. And on the theme of “small changes” to famous repositories she had also had a pull requested accepted to the {NHSRplotthedots} for text change to remove a reference to here and a link https://github.com/nhs-r-community/NHSRplotthedots/pull/183/files, making the text more readable for screen readers.\nTom updated on a few recent, bigger, changes to {NHSRplotthedots} in preparation for a new release ahead of this year’s NHS-R Community Conference. This has included some {plotly} code from a colleague who could contribute with their knowledge of {plotly} to help make {NHSRplotthedots} more interactive in its charts.\nTom put out a plea to everyone to get in touch via NHS-R Slack or email if you use {NHSRplotthedots} and what you have used it for."
  },
  {
    "objectID": "blog/nhs-r-newscast-20th-july-2023.html#finds-from-the-slack-group",
    "href": "blog/nhs-r-newscast-20th-july-2023.html#finds-from-the-slack-group",
    "title": "NHS-R newscast 20th July 2023",
    "section": "Finds from the Slack group",
    "text": "Finds from the Slack group\nChris shared his life hack of ordering GitHub notifications by repository instead of the default of by date. It’s possible to subscribe and watch any repository on GitHub and notifications can build up rapidly if you are part of teams repositories like NHS-R Community. Having them grouped by repository can help manage those messages.\nTom gave his regular shout out to John Mackintosh’s {patientcounter} package. If you work with patient level data this package is likely to help you at some point with your analysis:\nHow many patients were in the hospital at 10 AM yesterday?\nHow many were in during each 15 minute spell between 2pm and 6pm?\nHow many were in during the last week, by hour?\nThis package aims to make answering these questions easier and quicker.\nNo SQL? No problem!\nIf you have time in, time out, a unique patient identifier, and optionally, a grouping variable to track moves between departments, this package will tell you how many patients were ‘IN’ at any time, at whatever granularity you need.\nZoë started by mentioning the {pivotr} package:\nA shiny implementation of Excel’s PivotTables. Perform your aggregation/pivoting in the GUI, then copy the dplyr/tidyr code into your R script\nbut quickly changed to talking about Matt Dray’s {quartostamp} because she uses it as regularly now as {tidyverse} as she’s creating Quarto slides and books. The package changes the RStudio IDE so that add ins can be selected to put the required text into your code for Quarto so you don’t have to remember the formats.\nChris talked about a function in {purrr} called pluck() which gets and sets elements within nested structures. Allison Horst has also drawn a lovely image to explain pluck() https://allisonhorst.com/r-packages-functions. Chris also mentioned that page breaks in Quarto can be made really simply:"
  },
  {
    "objectID": "blog/format-ons-spreadsheet.html",
    "href": "blog/format-ons-spreadsheet.html",
    "title": "Format ONS spreadsheet",
    "section": "",
    "text": "Background\nA Public Health consultant colleague Ian Bowns created a report to monitor mortality within the Trust and he used the ONS weekly provisional data for the East Midlands to compare the pattern and trends of deaths over time. This involves downloading a file from:\nhttps://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales\nwhich is updated weekly. Once a month I, manually, add numbers from this to another spreadsheet to be imported to R for the overall analysis.\nDownloaded file formats\nYou may be familiar with ONS and other NHS data spreadsheets format and if you are not, here are some of the issues:\n\ndata is presented in wide form, not long form (so columns are rows and vice versa)\nthe sheets are formatted for look rather than function with numerous blank rows and blank columns\nthere are multiple sheets with information about methodology usually coming first. This means a default upload to programmes like R are not possible as they pick up the first sheet/tab\nthe file name changes each week and includes the date which means any code to pick up a file needs to be changed accordingly for each load\nbeing Excel, when this is imported into R, there can be problems with the date formats. These can get lost to the Excel Serial number and\nthey include a lot of information and often only need a fraction of it\n\nGiven these difficulties there is great temptation, as happened with this, to just copy and paste what you need. This isn’t ideal for the reasons:\n\nit increases the likelihood of data entry input error\nit takes time and\nit is just so very very tedious\n\nThe solution is, always, to automate and tools like Power Pivot in Excel or SSIS could work but as the final report is in R it makes sense to tackle this formatting in R and this is the result…\nImport file\nFor this you can either save the file manually or use the following code within R. Save it to the same place where the code is running and you should see the files in the bottom right window under the tab ‘Files’. The best way to do this is using project and opening up the script within that project.\n\ndownload.file(\"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales/2019/publishedweek082019.xls\",\n  destfile = \"DeathsDownload.xls\",\n  method = \"wininet\", # use \"curl\" for OS X / Linux, \"wininet\" for Windows\n  mode = \"wb\"\n) # wb means \"write binary\"\n\nNot that this file’s name and URL changes each week so the code needs changing each time it is run.\nOnce the file is saved use {readxl} to import which means the file doesn’t need its format changing from the original .xls\nWhen I upload this file I get warnings which are related, I think, to the Excel serial numbers appearing where dates are expected.\n\n\nsheet = :refers to the sheet I want. I think this has to be numeric and doesn’t use the tab’s title.\n\nskip = :is the number of top rows to ignore.\n\n\nDeathsImport &lt;- read_excel(\"DeathsDownload.xls \",\n  sheet = 4,\n  skip = 2\n)\n## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =\n## sheet, : Expecting numeric in C5 / R5C3: got a date\n## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =\n## sheet, : Expecting numeric in F5 / R5C6: got a date\n## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =\n## sheet, : Expecting numeric in G5 / R5C7: got a date\n## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =\n## sheet, : Expecting numeric in H5 / R5C8: got a date\n## Warning in read_fun(path = enc2native(normalizePath(path)), sheet_i =\n## sheet, : Expecting numeric in I5 / R5C9: got a date\n## New names:\n## * `` -&gt; `..2`\n\nFormatting the data\nThe next code creates a list that is used in the later code that is similar to the SQL IN but without typing out the list within the code for example:\n\nSQL : WHERE city IN ('Paris','London','Hull')\n\nR : filter(week_number %in% filter)\n\n\n\nLookupList &lt;- c(\n  \"Week ended\",\n  \"Total deaths, all ages\",\n  \"Total deaths: average of corresponding\",\n  \"E12000004\"\n)\n\nThe next bit uses the {dplyr} package, which has loaded as part of tidyverse, as well as the {janitor} package. Not all packages are compatible with {tidyverse} but many do as this is often the go-to data manipulation package.\nAs an aside the %&gt;% is called a pipe and the short cut is Shift + Ctrl + m. Worth learning as you’ll be typing a lot more if you type out those pipes each time.\n{janitor} commands\n\n\nclean_names(): removes spaces in column headers and replaces with _\n\nremove_empty(): gets rid of rows and columns – this dataset has a lot of those!\n\n{dplyr} command\n\n\nfilter(): is looking just for the rows with the words from the list LookupList. These will become the column names later.\n\n\nDeathsImport2 &lt;- DeathsImport %&gt;%\n  clean_names() %&gt;%\n  remove_empty(c(\"rows\", \"cols\")) %&gt;%\n  filter(week_number %in% LookupList)\n\n\n\ngather() and spread() from the {tidyr} package, which is part of {tidyverse} have been superseded by functions pivot_longer() and pivot_wider().\nThere are great commands called gather() and spread() which can be used to move wide form data to long and vice versa but with this I noticed that I just needed to turn it on its side so I used t() which is also useful as it turns the data frame to a matrix. You can see this by looking in the ‘Environment’ window in the top right of R Studio; there is no blue circle with an arrow next to t_DeathsImport.\n\nt_DeathsImport &lt;- t(DeathsImport2)\n\nBeing a matrix is useful as the next line of code makes the first row into column headers and this only works on a matrix.\n\ncolnames(t_DeathsImport) &lt;- t_DeathsImport[1, ]\n\n{dplyr} gives an error on matrices:\nCode:\n\nt_DeathsImport %&gt;% mutate(serialdate = excel_numeric_to_date(as.numeric(as.character(Week_ended)), date_system = \"modern\"))\n\nResult:\nError in UseMethod(“mutate_”) : no applicable method for 'mutate_' applied to an object of class “c('matrix', 'character')”\nAs later code will need {dplyr} turn the matrix into a data frame using some base R code:\n\nt_DeathsImport &lt;- as.data.frame(t_DeathsImport)\n\nPrevious {dplyr} code filtered on an %in% bit of code and it’s natural to want a %not in% but it doesn’t exist! However, cleverer minds have worked out a function:\n\n\nSince this blog more responses on StackOverflow have been submitted using other packages like {purrr}\n\n'%!ni%' &lt;- function(x, y) !('%in%'(x, y))\n\nThe text between the ’’ can be anything but I like %ni% as it’s reminiscent of Monty Python.\nBecause of the moving around of rows to columns the data frame now has a row of column names which is not necessary as well as a row with just ‘East Midlands’ in one of the columns so the following ‘remove’ list is a workaround to get rid of these two lines.\nBecause of the moving around of rows to columns the data frame now has a row of column names which is not necessary as well as a row with just ‘East Midlands’ in one of the columns so the following ‘remove’ list is a workaround to get rid of these two lines.\n\nremove &lt;- c(\"E12000004\", \"East Midlands\")\n\nThe next code uses the above list followed by a mutate which is followed by a {janitor} command excel_numeric_to_date(). This tells it like it is but, as often happens, the data needs to be changed to a character and then to numeric. The date system = \"modern\" isn’t needed for this data but as I took this from the internet and it worked, so I left it.\nAn error will appear about NAs (nulls).\n\nt_DeathsImport &lt;- t_DeathsImport %&gt;%\n  filter(E12000004 %!ni% remove) %&gt;%\n  mutate(serialdate = excel_numeric_to_date(as.numeric(as.character(`Week ended`)), date_system = \"modern\"))\n## Warning in excel_numeric_to_date(as.numeric(as.character(`Week ended`)), :\n## NAs introduced by coercion\n\nNow to deal with this mixing of real dates with Excel serial numbers.\nFirstly, the following code uses base R to confirm real dates are real dates which conveniently wipes the serial numbers and makes them NAs.\n\nt_DeathsImport$`Week ended` &lt;- as.Date(t_DeathsImport$`Week ended`, format = \"%Y-%m-%d\")\n\nThis results in two columns:\n\n\nWeek ended which starts off with NAs then becomes real dates and\n\nserialdate which starts off with real dates and then NAs.\n\nThe human eye and brain can see that these two follow on from each other and just, somehow, need to be squished together and the code to do it is as follows:\n\nt_DeathsImport &lt;- t_DeathsImport %&gt;%\n  mutate(date = if_else(is.na(`Week ended`), serialdate, `Week ended`))\n\nTo translate the mutate, this creates a new column called date which, if the Week ended is null then takes the serial date, otherwise it takes the Week ended.\nInterestingly if ifelse() without the underscore is used it converts the dates to integers and these are not the same as the Excel serial numbers so use if_else()!\nAnd that’s it.\nOr is it?\nYou might want to spit out the data frame back into excel and that’s where a different package called {openxlsx} can help. As with many things with R, “other packages are available”.\n\nwrite.xlsx(DeathsImport, \"ImportProvisionalDeaths.xlsx\")\n\nIf you haven’t used a project (which is really the best way to work) this will probably save in some obscure C: drive that you’ll see in the bottom left ‘Console’ just under the tab names for ‘Console’ and ‘Terminal’. Using projects means you set the pathway and that will mean the file saves in the same place and will also appear in the bottom right panel under ‘Files’.\nFeedback\nI’m pretty early on in my journey in R and many of my colleagues still haven’t started yet so I’m throwing this out there so everyone can see it, newbies and old hands alike. If you spot anything, can explain anything further, need more explanation or can offer any alternatives to what I’ve done please please feel free to comment.\nThis blog has been edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "",
    "text": "Greetings, esteemed members of the NHS-R Community, data enthusiasts, and healthcare aficionados! It is with great excitement that I share my recent journey as a committee member with the NHS-R Community, a vibrant hub dedicated to championing the use of R and data science tools in the UK health and care system."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#the-community",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#the-community",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "The Community",
    "text": "The Community\nEstablished in 2018, the NHS-R Community has rapidly evolved into a dynamic collective of professionals from diverse backgrounds. Our community encompasses members from public sector organizations, including Local Authorities and Civil Service, academia, and the voluntary sector. We are united by a shared passion for advancing healthcare through the transformative power of data. While our core language is R, we are equally enthusiastic about embracing a broad spectrum of data science tools, recognizing their integral role in shaping the future of healthcare analytics."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#steering-the-course-the-committees-impact",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#steering-the-course-the-committees-impact",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Steering the Course: The Committee’s Impact",
    "text": "Steering the Course: The Committee’s Impact\nAs a committee member, my role involves active participation in shaping the community’s approach, focus, and overall strategy. We meet regularly to deliberate on initiatives, conferences, and the overarching direction for the community. The committee serves as a crucial forum for collective decision-making, ensuring that our initiatives align with the evolving needs of the community and contribute to positive change."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#committee-meetings-a-glimpse-into-our-world",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#committee-meetings-a-glimpse-into-our-world",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Committee Meetings: A Glimpse into Our World",
    "text": "Committee Meetings: A Glimpse into Our World\nOur meetings are a lively exchange of ideas, experiences, and visions for the future. Whether we’re brainstorming innovative projects, refining strategies, or planning impactful conferences, each committee member brings a unique perspective to the table. The collaborative spirit is palpable as we navigate the dynamic landscape of data science in healthcare."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#driving-positive-change",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#driving-positive-change",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Driving Positive Change",
    "text": "Driving Positive Change\nBeing part of the committee means playing a pivotal role in fostering collaboration and steering the community toward meaningful outcomes. We celebrate diversity in thought and expertise, recognizing that it is the key to unlocking the full potential of data science in healthcare."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#contributions-welcome",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#contributions-welcome",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Contributions Welcome",
    "text": "Contributions Welcome\nOne of the hallmarks of our community is the open invitation for contributions. Our GitHub repository: NHS-R Community hosts a wealth of resources, including packages and training materials, and we wholeheartedly welcome contributions from community members. It’s an empowering experience to witness the collective impact of our shared knowledge and skills."
  },
  {
    "objectID": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#join-the-conversation",
    "href": "blog/navigating-the-data-driven-frontier-insights-from-an-nhs-r-committee-member.html#join-the-conversation",
    "title": "Navigating the Data-Driven Frontier: Insights from an NHS-R Committee Member",
    "section": "Join the Conversation",
    "text": "Join the Conversation\nIf you’re passionate about data science, healthcare, and making a positive impact, the NHS-R Community is the place to be. Whether you’re an R enthusiast, a data science wizard, or a healthcare professional with an interest in analytics, there’s a place for you in our diverse and welcoming community.\nAs I continue my journey as a committee member, I’m excited about the endless possibilities that lie ahead. Together, we’re shaping the future of healthcare analytics, one R script at a time.\nStay tuned for more updates from the NHS-R Community, where data meets healthcare, and innovation knows no bounds. See you in the data-driven frontier!"
  },
  {
    "objectID": "blog/apha-blog-may-2023.html",
    "href": "blog/apha-blog-may-2023.html",
    "title": "AphA blog – May 2023",
    "section": "",
    "text": "First published for the AphA May 2023 newsletter:"
  },
  {
    "objectID": "blog/apha-blog-may-2023.html#haca23-11-12-july",
    "href": "blog/apha-blog-may-2023.html#haca23-11-12-july",
    "title": "AphA blog – May 2023",
    "section": "HACA23 11-12 July",
    "text": "HACA23 11-12 July\nThe inaugural hybrid Health and Care Analytics Conference HACA23 is fast approaching and has received over 150 abstracts. Tickets are free and have all gone now for the in-person but you can sign up to the waiting list and there are plenty of virtual tickets. Follow the Twitter account HACA_Conf for more information."
  },
  {
    "objectID": "blog/apha-blog-may-2023.html#nhsr-community-conference-17-18-october",
    "href": "blog/apha-blog-may-2023.html#nhsr-community-conference-17-18-october",
    "title": "AphA blog – May 2023",
    "section": "NHSR Community Conference 17-18 October",
    "text": "NHSR Community Conference 17-18 October\nThe hybrid NHS-R Conference is in its 6th year and the call for abstracts is out now. Talks can be in R and Python, both or with other languages and we are keen to get a range of talks from people from wherever people are in their R and Python journeys."
  },
  {
    "objectID": "blog/apha-blog-may-2023.html#sql-finds",
    "href": "blog/apha-blog-may-2023.html#sql-finds",
    "title": "AphA blog – May 2023",
    "section": "SQL finds",
    "text": "SQL finds\nIn the #finds channel, which was set up by Chris Beeley:\nPartly inspired by a Linux podcast that I listen to, and partly by a message in #help-with-r, I have created a channel for posting cool stuff you find. Links, code snippets, communities, whatever.\nI am going to add a “Finds” section to the newscast episodes of the podcast, with raw material from the work of myself and the co-hosts, as well as (hopefully) stuff from here too\nwe had the link https://mystery.knightlab.com/ shared which uses “SQL queries to solve the murder mystery”. Suitable for beginners or experienced SQL sleuths!\nPlease come and let us know if you have any other training suggestions for SQL!"
  },
  {
    "objectID": "blog/apha-blog-may-2023.html#sql-help",
    "href": "blog/apha-blog-may-2023.html#sql-help",
    "title": "AphA blog – May 2023",
    "section": "SQL help",
    "text": "SQL help\nWe also had a question in the #help-with-r channel from a newbie SQL coder asking:\nHow do I summarise a table by concatenating values (or pivot then concatenate across cols)? I don’t know in advance what the output columns of a pivot will be (or at least I can’t be bothered to list them out).\nExample input:\n\nTable of one user with two issues A and B in two rows\n\nEntity\nIssues\n\n\n\nUser1\nIssueA\n\n\nUser1\nIssueB\n\n\n\nDesired output:\n\nTable of one user with two issues A and B in one row\n\nEntity\nIssues\n\n\nUser1\nIssueA, IssueB\n\n\nIn R I would want to do something like:\n\ntibble::tibble(Entity = \"User1\", \n               Issues = c(\"IssueA\", \n                          \"IssueB\")) |&gt;\n  dplyr::summarise(across(\"Issues\", \n                          \\(x) stringr::str_flatten(x,\n                                                    collapse = \",\")), \n                   .by = Entity)\n\n# A tibble: 1 × 2\n  Entity Issues       \n  &lt;chr&gt;  &lt;chr&gt;        \n1 User1  IssueA,IssueB\n\n\nIn SQL if I try to use CONCAT it says it needs at least 2 arguments. I also can’t get my head round how PIVOT works, yet.\nThis type of question is brilliant because quite a lot of people already have an idea of how to solve the problem in one language and want to know how to do that in another. It can often be a simple translation like R’s {dplyr} function arrange() is ORDER BY in SQL. However, sometimes we need to approach the problem slightly differently because of the way the languages work.\nIn SQL CONCAT needs to have inputs of distinct columns into the function so CONCAT(colm1, colm2) but as this example doesn’t yet have those separate columns, the question is how to pivot that data from the long form it’s currently in, to something wider with each column referring to a single issue.\nIn R’s {tidyr} package, part of the {tidyverse} along with {dplyr}, we’d use the functions pivot_wider() or pivot_longer(). The concept of pivotting exists in SQL but is different in that the code needs to stipulate each and every column and for this analysis those columns are not necessarily known in advance to hard code.\nOne way we thought to get around this, as R users, was to use a package called {dbplyr} which, like {tidyverse}, is maintained by Posit and has functions that convert R {dplyr} commands to SQL, running them directly on SQL databases. That’s a great way of utilising the power of SQL and also helpful in that it can give you the code it’s translated."
  },
  {
    "objectID": "blog/apha-blog-may-2023.html#specific-sql-solutions",
    "href": "blog/apha-blog-may-2023.html#specific-sql-solutions",
    "title": "AphA blog – May 2023",
    "section": "Specific SQL solutions",
    "text": "Specific SQL solutions\nSuggestions for SQL included:\n\nA Stackoverflow page which suggests using XML to solve the question: How to create a SQL Server function to “join” multiple rows from a subquery into a single delimited field?\nuse a recursive CTE (Common Text Expression)\nif you can set an upper limit on the number of items, then PIVOT or MAX(CASE WHEN IssueA THEN Issue END)\nUse LISTAGG (Oracle) or STRING_AGG for example SELECT Entity, STRING_AGG([Issues], ',') FROM table GROUP BY Entity;\n\nThere may be other solutions to this and it would be great to hear from you!"
  },
  {
    "objectID": "blog/apha-blog-may-2023.html#demos-and-how-tos",
    "href": "blog/apha-blog-may-2023.html#demos-and-how-tos",
    "title": "AphA blog – May 2023",
    "section": "Demos and How tos",
    "text": "Demos and How tos\nOne place the NHS-R Community shares snippets of useful code is in the GitHub repository demos-and-how-tos. This is currently a series of folders but could possibly be better in a book format. We’ve got an issue about this along with other suggestions which anyone can contribute to by adding more suggestions or even picking up and issue or two!"
  },
  {
    "objectID": "blog/using-r-to-create-column-charts-featuring-95-confidence-intervals.html",
    "href": "blog/using-r-to-create-column-charts-featuring-95-confidence-intervals.html",
    "title": "Using R to create column charts featuring 95% confidence intervals",
    "section": "",
    "text": "Daniel Weiand – Consultant Medical Microbiologist – Newcastle upon Tyne NHS Foundation Trust\nHello!\nThis is my first blog post for the NHS R Community, which I stumbled across in the course of my work as a consultant medical microbiologist at Newcastle upon Tyne Hospitals NHS Foundation Trust.\nAt work, I’ve been trying to use R to create column charts featuring 95% confidence intervals. I approached the friendly people on the NHS R Community’s Slack channel for further information and guidance.\nI must add here that the Community’s Slack channel has been extremely helpful to me, as a novice R user, in solving some of the issues I’ve experienced, and highlighting R packages of potential interest. This is the first time I’ve tried to create a ReprEx and now I understand why people love (?) the mtcars database as much as they do!\nStep 1: Calculate some summary statistics\nI wanted to calculate some summary statistics, including the mean, and standard error or 95% confidence intervals.\nInitially I came across the summary() function of Base R, which is helpful as it calculates the Min., 1st Qu., Median, Mean, 3rd Qu., and Max.\nHowever, the summary() function of {base} R does not calculate either the standard error or the 95% confidence intervals:\n\nlibrary(tidyverse)\n# calculate summary statistics for all numeric data using summary() and where(is.numeric())\nmtcars %&gt;%\n  select(where(is.numeric)) %&gt;%\n  summary()\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n\n# calculate summary statistics for mpg using summary() and where(is.numeric())\nmtcars %&gt;%\n  select(mpg) %&gt;%\n  summary()\n\n      mpg       \n Min.   :10.40  \n 1st Qu.:15.43  \n Median :19.20  \n Mean   :20.09  \n 3rd Qu.:22.80  \n Max.   :33.90  \n\n\nThen zx8754 very kindly pointed me towards a method for calculating the standard error on StackOverflow: https://stackoverflow.com/q/2676554/680068\n\n# create stderr function\nstderr &lt;- function(x, na.rm = TRUE) {\n if (na.rm) x &lt;- na.omit(x)\n sqrt(var(x)/length(x))\n}\n\nThen I used this function to calculate summary statistics, incl. mean and standard error, using the summarise() and across() functions of {dplyr}:\n\n# calculate summary statistics using summarise() and across() and n/mean/min/median/max/sd/stderr\n# stderr &lt;- function(x, na.rm=TRUE) {\n#  if (na.rm) x &lt;- na.omit(x)\n#  sqrt(var(x)/length(x))\n# }\nmtcars %&gt;% \n group_by(cyl) %&gt;% \n mutate(\n   across(mpg, \n          list(\n            n = ~ n(),\n            mean = ~ mean(.x, na.rm = TRUE),\n            min = ~ min(.x, na.rm = TRUE),\n            median = ~ median(.x, na.rm = TRUE),\n            max = ~ max(.x, na.rm = TRUE),\n            sd = ~ sd(.x, na.rm = TRUE),\n            stderr = ~ stderr(.x)),\n          .names = NULL)) %&gt;%\n select(starts_with(\"mpg\")) %&gt;% \n summarise(mean = mean(mpg_mean),\n           min = mean(mpg_min),\n           median = mean(mpg_median),\n           max = mean(mpg_max),\n           sd = mean(mpg_sd),\n           stderr = mean(mpg_stderr)) %&gt;% \n#create column chart with error bars (using stderr) \n ggplot(aes(cyl, mean))+\n geom_col(na.rm = TRUE)+\n geom_errorbar(aes(ymin = mean-stderr, ymax = mean + stderr), \n               position = \"dodge\", width = 0.25)\n\n\n\n\n\n\n#calculate summary statistics using summarise() and across() and n/mean/min/median/max/sd/stderr\n# stderr &lt;- function(x, na.rm=TRUE) {\n#  if (na.rm) x &lt;- na.omit(x)\n#  sqrt(var(x)/length(x))\n# }\nmtcars %&gt;% \n group_by(cyl) %&gt;% \n mutate(\n   across(mpg, \n          list(\n            n = ~ n(),\n            mean = ~ mean(.x, na.rm = TRUE),\n            min = ~ min(.x, na.rm = TRUE),\n            median = ~ median(.x, na.rm = TRUE),\n            max = ~ max(.x, na.rm = TRUE),\n            sd = ~ sd(.x, na.rm = TRUE),\n            stderr = ~ stderr(.x)),\n          .names = NULL)) %&gt;%\n select(starts_with(\"mpg\")) %&gt;% \n summarise(mean = mean(mpg_mean),\n           min = mean(mpg_min),\n           median = mean(mpg_median),\n           max = mean(mpg_max),\n           sd = mean(mpg_sd),\n           stderr = mean(mpg_stderr)) %&gt;% \n#create column chart with error bars (using stderr) \n ggplot(aes(cyl, mean))+\n geom_col(na.rm = TRUE)+\n geom_errorbar(aes(ymin = mean-stderr, ymax = mean + stderr), \n               position = \"dodge\", width = 0.25)\n\n\n\n\n\n\n\nStep 2: Create column charts with error bars (using 95% confidence intervals)\nThen Seb Fox pointed me towards a method for calculating 95% confidence intervals using the {PHEindicatormethods} package, available on CRAN: https://cran.r-project.org/web/packages/PHEindicatormethods/index.html\n\n# create MEAN column chart with error bars (using 95% confidence intervals) \nrequire(PHEindicatormethods)\n\nmtcars %&gt;% \n filter(!is.na(cyl)) %&gt;% \n group_by(cyl) %&gt;% \n#use phe_mean()\n phe_mean(x = mpg, # field name from data containing the values to calculate the means for\n          type = \"full\", # defines the data and metadata columns to be included in output; can be \"value\", \"lower\", \"upper\", \"standard\" (for all data) or \"full\" (for all data and metadata); quoted string; default = \"full\"\n          confidence = 0.95) %&gt;% #required level of confidence expressed as a number between 0.9 and 1\n  \n# create column chart with error bars (using 95% CI calculated using phe_mean())\n ggplot(aes(cyl, value))+\n geom_col(na.rm = TRUE)+\n geom_errorbar(aes(ymin = lowercl, ymax = uppercl), \n               position = \"dodge\", width = 0.25)\n\n\n\n\n\n\n# create PROPORTION column chart with error bars (using 95% confidence intervals) \nrequire(PHEindicatormethods)\n\nmtcars %&gt;% \n group_by(cyl) %&gt;% \n summarise(n = n(),\n           sum = sum(n)) %&gt;% \n mutate(sum = sum(n)) %&gt;%\n# phe_proportion()\n phe_proportion(x = n, # numerator\n                n = sum, # denominator\n                type = \"full\", # defines the data and metadata columns to be included in output; can be \"value\", \"lower\", \"upper\", \"standard\" (for all data) or \"full\" (for all data and metadata); quoted string; default = \"full\"\n                confidence = 0.95, # required level of confidence expressed as a number between 0.9 and 1 \n                multiplier = 100) %&gt;%   # the multiplier used to express the final values (for example 100 = percentage); numeric; default 1\n# create column chart with error bars (using 95% CI calculated using phe_proportion())\n ggplot(aes(cyl, value))+\n geom_col(na.rm = TRUE)+\n geom_errorbar(aes(ymin = lowercl, ymax = uppercl), \n               position = \"dodge\", width = 0.25)\n\n\n\n\n\n\n\nI hope that the code, above, helps a few colleagues of mine across the NHS, in some small way.\nThank you, again, to all members of the NHS R Community, for all your help. Particular thanks go to everyone who has helped me, to date, on the NHS R Community’s Slack channel.\nComments (3)\n\n\nChuck Burks\n5 January 2022\nI used to use a function like that, then I realized that I could get a function for standard error through the FSA package, FSA::se(). Arguments can be made either way; why reinvent the wheel vs. why load a large package to avoid writing one function.\n\n\nQin Zeng\n5 January 2022\nThis works out about the same: mtcars %&gt;% group_by(cyl) %&gt;% summarise_at(vars(mpg), funs(n(), mean, min, median, max, sd, stderr)) %&gt;% ggplot(aes(cyl, mean))+ geom_col(na.rm = TRUE)+ geom_errorbar(aes(ymin = mean-stderr, ymax = mean+stderr), position = \"dodge\", width = 0.25)\n\n\nStephen\n6 January 2022\nTwo suggestions: mtcars %&gt;% group_by(cyl) %&gt;% summarise(n = n(), sum = sum(n)) %&gt;% mutate(sum = sum(n)) is more easily said as mtcars %&gt;% count(cyl) %&gt;% mutate(sum = sum(n)) Qin, your sequence is better as mtcars %&gt;% group_by(cyl) %&gt;% summarise(n(), across(mpg, c(mean, min, median, max, sd, stdeQi, in modern dplyr (summarise_at is deprecated)\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "",
    "text": "Analysing the pre-conference workshop sentiments\nLoading in the required packages:\ninstall_or_load_pack &lt;- function(pack) {\n  create.pkg &lt;- pack[!(pack %in% installed.packages()[, \"Package\"])]\n  if (length(create.pkg)) {\n    install.packages(create.pkg, dependencies = TRUE)\n  }\n  sapply(pack, require, character.only = TRUE)\n}\npackages &lt;- c(\n  \"ggplot2\", \"tidyverse\", \"data.table\", \"wordcloud\", \"tm\", \"wordcloud2\",\n  \"scales\", \"tidytext\", \"devtools\", \"twitteR\", \"caret\", \"magrittr\", \"RColorBrewer\", \"tidytext\", \"ggdendro\",\n  \"tidyr\", \"topicmodels\", \"SnowballC\", \"gtools\"\n)\ninstall_or_load_pack(packages)\nThis function was previously covered in blog post: https://nhsrcommunity.com/blog/a-simple-function-to-install-and-load-packages-in-r/.\nHere I specify that I want to load the main packages for dealing with sentiment and discourse analysis in R. Libraries such as {tm}, {wordcloud} and {wordcloud2} are loaded for working with this type of data."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#choosing-the-file-to-import",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#choosing-the-file-to-import",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Choosing the file to import",
    "text": "Choosing the file to import\nThe file we have to import is a prepared csv file and instead of hard coding the path to load the file from I simply use:\n\npath &lt;- choose.files()\n\n\n\nScreenshot of Select files window that pops up to allow files to be chosen\n\nThis is a special function which allows you to open a dialog UI from R:\nFrom this dialog I select the csv file I want to be imported. Once I have selected the csv and hit open, the path variable will be filled with the location of the file to work with."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#creating-the-r-data-frame",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#creating-the-r-data-frame",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Creating the R Data Frame",
    "text": "Creating the R Data Frame\nTo create the data frame I can now pass the variable path to the read_csv() command:\n\nworkshop_sentiment &lt;- read_csv(path, col_names = T)\n\nThis will read the textual data from the workshops in to a data frame with 2 columns. The first relates to what the attendees enjoyed about the workshop and the second relates to improvements that can be made:\n\n\nScreenshot of the highlights and improvements text from a pre-conference survey"
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#function-to-create-textual-corpus",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#function-to-create-textual-corpus",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Function to create textual corpus",
    "text": "Function to create textual corpus\nAs I want to replicate this for highlights and improvements – I have created a function that could be replicated with any text analysis to create what is known as a text corpus (see: https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf) this creates a series of documents, in our case sentences.\n\ncorpus_tm &lt;- function(x) {\n  library(tm)\n\n  corpus_tm &lt;- Corpus(VectorSource(x))\n}\n\nThis function allows you to pass any data frame to the function and creates a corpus for each data frame you pass to the function. The data frame would be passed to the x parameter. The VectorSource() function creates an element for each part of the corpus."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-corpus-for-highlights-and-improvements-data-frame",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-corpus-for-highlights-and-improvements-data-frame",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Create Corpus for Highlights and Improvements data frame",
    "text": "Create Corpus for Highlights and Improvements data frame\nNow the function has been created, I can simply pass the two separate data frames I created before to create two corpuses:\n\ncorpus_positive &lt;- corpus_tm(ws_highlights$Highlights)\n\ncorpus_improvements &lt;- corpus_tm(ws_improvements$Improvements)\n\nThe code block above shows that I create a corpus for the positive (highlights) data frame and an improvements corpus. This will display as hereunder in your environment:\n\n\nScreenshot of the corpus output from the Environment in RStudio\n\n##Function to clean data in the corpus\nThe most common cleaning task of working with text data is to remove things like punctuation, common English words, and so on This is something I have to repeat multiple times when dealing with discourse analysis:\n\nclean_corpus &lt;- function(corpus_to_use) {\n  library(magrittr)\n\n  library(tm)\n\n  corpus_to_use %&gt;%\n    tm_map(removePunctuation) %&gt;%\n    tm_map(stripWhitespace) %&gt;%\n    tm_map(content_transformer(function(x) iconv(x, to = \"UTF-8\", sub = \"byte\"))) %&gt;%\n    tm_map(removeNumbers) %&gt;%\n    tm_map(removeWords, stopwords(\"en\")) %&gt;%\n    tm_map(content_transformer(tolower)) %&gt;%\n    tm_map(removeWords, c(\"etc\", \"i.e.\", \"e.g.\", stopwords(\"english\")))\n}\n\nThe parameter here takes the corpus object previously created and uses the corpus passed to:\n\nRemove punctuation\n\nStrip out whitespace between each text item, as the VectorSource has stripped out each word from each sentence in the data frame\n\nChange the underlying formatting of the text to UTF-8\n\nRemove numbers\n\nRemove common English word (stop words)\n\nChange the case to lower case\n\nRemove a custom vector of words to adjust for things like “e.g.”, “i.e.”, “etc”.\n\nTo clean the corpus objects I simply pass the original corpus objects back through this function to perform cleaning:\n\ncorpus_positive &lt;- clean_corpus(corpus_positive)\n\ncorpus_improvements &lt;- clean_corpus(corpus_improvements)\n\nInspection of one of the data frames confirms that this has successfully been cleaned:"
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-termdocumentmatrix-to-attain-frequent-terms",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-termdocumentmatrix-to-attain-frequent-terms",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Create TermDocumentMatrix to attain frequent terms",
    "text": "Create TermDocumentMatrix to attain frequent terms\nThe term document matrix (explained well here: https://www.youtube.com/watch?v=dE10fBCDWQc) can be used with the corpus to identify frequent terms by classification on a matrix. However, more code is needed to do this:\n\n find_freq_terms_fun &lt;- function(corpus_in){\n\n  doc_term_mat &lt;- TermDocumentMatrix(corpus_in)\n\n  freq_terms &lt;- findFreqTerms(doc_term_mat)[1:max(doc_term_mat$nrow)]\n\n  terms_grouped &lt;-\n\n    doc_term_mat[freq_terms,] %&gt;%\n\n    as.matrix() %&gt;%\n\n    rowSums() %&gt;%\n\n    data.frame(Term=freq_terms, Frequency = .) %&gt;%\n\n    arrange(desc(Frequency)) %&gt;%\n\n    mutate(prop_term_to_total_terms=Frequency/nrow(.))\n\n  return(data.frame(terms_grouped))\n\n }\n\nThis function needs explanation. The function uses as a single parameter the corpus that you need to pass in, then a variable is created to create the doc_term_mat which uses the tm TermDocumentMatrix.\nNext, I use the findFreqTerms function to iterate from the first entry to the maximum number of rows in the matrix. These are the powerhouses of the function, as they highlight how many times a word has been used in a sentence across all the rows of text.\nThe terms_grouped variable then slices the term matrix with the frequent terms, this is converted to a matrix, sum of each row are calculated for example the number of times the word appears. Then, a data frame is created of the terms in the function with the headings term and Frequency.\nNext, we use the power of {dplyr} to use arrange by the frequency descending and to add a mutated column to the data frame to calculate the proportion of that specific term over all terms. The return(data.frame(terms_group)) then forces R to return the results of the function.\nI then pass my data frames (highlights and improvements) to the function I have just created to see if this method works:\n\npositive_freq_terms &lt;- data.frame(find_freq_terms_fun(corpus_positive))\n\nimprovement_freq_terms &lt;- data.frame(find_freq_terms_fun(corpus_improvements))\n\nThese will be built as data frames and can be viewed in R Studio’s Data environment window:\n\n\nScreenshot of the RStudio’s data environment window with terms and frequency\n\nThis has worked just as expected. You could now use ggplot2 to produce a bar chart / pareto chart of the terms."
  },
  {
    "objectID": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-a-word-cloud-with-the-wordcloud2-package",
    "href": "blog/text-mining-term-frequency-analysis-and-word-cloud-creation.html#create-a-word-cloud-with-the-wordcloud2-package",
    "title": "Text Mining – Term Frequency analysis and Word Cloud creation in R",
    "section": "Create a Word Cloud with the {wordcloud2} package",
    "text": "Create a Word Cloud with the {wordcloud2} package\nR has a {wordcloud} package that produces relatively nice looking word clouds, but {wordcloud2} surpasses this in terms of visualisation. To use this function is easy now I have the frequent terms data frame – using the highlights data frame this can be implemented by using the below syntax:\n\nwordcloud2(positive_freq_terms[,1:2],\n\n           shape=\"pentagon\",\n\n           color=\"random-dark\")\n\nTo use the function I pass the data frame and use the term and frequency fields only to use the visualisation. There are a number of options and these can be accessed by using the help(\"wordcloud2\") function. Here I use the shape and color parameters to set the display of the word cloud:\n\n\nScreenshot of the word cloud with Code being the largest word, followed by plot, useful, pick and facets\n\nThis can be exported in the viewer window by using the Export function.\nThis word cloud relates to the pre workshop prior to the conference. I personally thought the NHS-R conference was amazing and I was honoured to have a spot to speak amongst so many other brilliant R users.\nR is so versatile – every day is like a school day when you are learning it, but what a journey.\nThis blog has been edited for NHS-R styles and has been formatted to remove Latin Abbreviations"
  },
  {
    "objectID": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development.html",
    "href": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development.html",
    "title": "From script-based development to function-based development and onwards to Package Based development",
    "section": "",
    "text": "At the NHS R Conference, I suggested to people that they should embrace the idea of package-based development rather than script-based work.\nI’m going to talk you through that process, using the simplest of scripts – ‘Hello World’. I’m going to assume that you’re using the freely available RStudio Desktop Edition as the editor for this: other versions of RStudio are likely to be essentially identical. Non R-Studio users may need to revert to more basic principles.\nFirst let’s write ‘Hello World’ – the simplest R script in the world. Open a new R script file and get your script underway:\n\nmessage(\"Hello World from the NHS-R Community!\")\n\nSave it (for posterity).\nIn the conference, we discussed that generally writing functions is more helpful than writing scripts as it gives you greater re-usability. This example is a very trivial one (in fact so trivial as to be nearly redundant).\nSo we consider our script carefully and determine – what does it DO? Clearly it’s a way of greeting a person. What if we wanted to greet the person in a different way? What if we wanted to greet a different person?\nSo we have defined it’s purpose, and the parameters that are likely to be useful to others.\nLet’s re-write our script to be more usable.\nWe define a function using the function function. You can see a much more detailed tutorial of how to do this here: http://stat545.com/block011_write-your-own-function-01.html.\nA function is defined by assigning the result of the function() function to a variable which is the function name. The parameters of function() are our new parameter names in our function.\nIt is really important to name your function clearly so people know what it does. Generally use active verbs to describe intent, and a consistent naming scheme. Also choose appropriate and clear names for the parameters. So let’s call our new function greet_person, and we will call our parameters greeting and recipient.\nOur new code will look like this. Stick this into a new R script for now and run it:\n\ngreet_person &lt;- function(greeting, sender) {\n  message(greeting, \" from \", sender)\n}\n\nOnce you’ve run your script you can now call your function from the console:\ngreet_person(“Hello World”, “the NHS-R Community!”) And of course if you want to use a different greeting we can now change our parameter value:\ngreet_person(“Welcome”, “the NHS-R Community!”) So far so good.\nBut – we’ve had to repeat our sender parameter. What if we know we’re usually going to use that first Hello World greeting; but we just want the option of doing something different if the situation arises?\nWe can get around that by supplying default values. In the function() function we can set a value to both greeting and sender using =. Let’s set default values for greet_person:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  message(greeting, \" from \", sender)\n}\n\nNow if you want our ‘default’ message you can just call:\n\ngreet_person()\n\nBut you can customise either parameter without having to specify anything you don’t want to change:\n\ngreet_person(sender = \"Drew Hill\")\n\nInstead of “Drew Hill” from our previous example, you’ll see the sender is “1”.\nWhat if you accidentally sent a vector of names? R will turn those into a concatenated string of names without spaces:\n\ngreet_person(sender = c(\"Bob\", \"Jim\"))\n\nSome things however certainly could break this process – so it is really important to check that you can handle the inputs you receive within a function before trying to use them.\nThe first thing we need to do is to make sure we are dealing with something that can be turned into a character. We can check that by using the is.character function – which returns TRUE if a given value is TRUE, and FALSE if it is not something that can be turned into a character.\nIf is.character is false, we want to stop with an error:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  if (!is.character(greeting)) {\n    stop(\"greeting must be a string\")\n  }\n\n  if (!is.character(sender)) {\n    stop(\"sender must be a string\")\n  }\n\n\n  message(greeting, \" from \", sender)\n}\n\nWe can test how this works by using NULL as a parameter: in real life this happens quite a lot as you try to pass a variable to your new function but forget to set the variable earlier on!\n\ngreet_person(sender = NULL)\n\nError in greet_person(sender = NULL) : sender must be a string\nWe also know that our function actually isn’t very good at handling vectors of strings (that is where there is more than one name): it will simply shove them all together without spaces. However it works and is perfectly functional. So we have a design decision: do we want to allow that, or not? A third way might be to allow it but to use a warning – perhaps a little over the top in our example, but for complex examples that may make more sense. Whereas stop will halt the code and force you to fix your bugs, the warning() function lets the code continue but tells you to go back and do it better later. Let’s add a warning if there was more than one sender:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  if (!is.character(greeting)) {\n    stop(\"greeting must be a string\")\n  }\n\n  if (!is.character(sender)) {\n    stop(\"sender must be a string\")\n  }\n\n\n  if (length(sender) &gt; 1) {\n    warning(\"greet_person isn't very good at handling more than one sender. It is better to use just one sender at a time.\")\n  }\n\n  message(greeting, \" from \", sender)\n}\n\nIf we now called the function with two senders we’d be able to do so but would get politely told that it’s not a good idea:\n\ngreet_person(sender = c(\"Jim\", \"Bob\"))\n\nSo – hopefully from this you’ve moved from having a script which would only do precisely what you wanted in a single set of circumstances, to now having a natty little function which will say greet whoever you want, with the type of greeting that you want.\nAs an exercise to complete: imagine you work in the NHS-R community welcoming team. You are tasked with sending greetings from the team on a regular basis.\nYou used to use a script to do this and had to remember to get the style right every time – but now you sit at your console , run your script containing your function, and greet_person() on demand.\nYour boss has come to you and urgently wants you to change your way of working. Rather than sending a greeting from the team using just a single team name, he wants you to send the individual names in the greeting from both Jim and Bob.\nHave a think about how you could change the function so that we can cope with multiple senders.\nThe greetings will continue as we then think about scaling up the NHS R Community Greetings division in our next instalment.\nThis blog was written by:\nDr. Andrew Hill\nClinical Lead for Stroke, St Helens and Knowsley Teaching Hospitals\nThis blog has been edited for NHS-R Style and to ensure running of code in Quarto.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development-part-2.html",
    "href": "blog/from-script-based-development-to-function-based-development-and-onwards-to-package-based-development-part-2.html",
    "title": "From script-based development to function-based development and onwards to Package Based development: part 2",
    "section": "",
    "text": "At the NHS R Conference, I suggested to people that they should embrace the idea of package-based development rather than script-based work.\nIn the first part of this tutorial, in the fictional NHS-R Community greeting room, our humble analyst was tasked with greeting people. Rather than writing a script and needing to repeat themselves all the time with different variations of greetings and senders, they wrote a rather nifty little function to do this:\n\ngreet_person &lt;- function(greeting = \"Hello World\", sender = \"the NHS-R Community!\") {\n  if (!is.character(greeting)) {\n    stop(\"greeting must be a string\")\n  }\n\n  if (!is.character(sender)) {\n    stop(\"sender must be a string\")\n  }\n\n\n  if (length(sender) &gt; 1) {\n    warning(\"greet_person isn't very good at handling more than one sender. It is better to use just one sender at a time.\")\n  }\n\n  message(greeting, \" from \", sender)\n}\n\nAs we know, R is awesome and many people took up R on the background of some excellent publicity and training work by the NHS-R community. Our poor greeting team got overwhelmed by work: it is decided that the team of greeters needs to expand. There will now be a team of three greeters. Every other bit of work output from our NHS-R community team will involve greeting someone before we present our other awesome analysis to them.\nThis is going to be a nightmare! How can we scale our work to cope with multiple users, and multiple other pieces of work using our greeting function.\nIf we rely upon the scripts, we have to trust that others will use the scripts appropriately and not edit or alter them (accidentally or on purpose). Furthermore, if someone wants to greet someone at the beginning of their piece of analysis, they’ll either have to copy the code and paste it somewhere, or link to our script containing the function – which in turn means they need to keep a common pathname for everything and hope no-one breaks the function. Nightmare!\nFortunately, someone attended the last NHS-R conference and remembered that package-based development is a really handy way of managing to scale your R code in a sustainable way. So after a team meeting with copious caffeine, it is decided that greet_person needs to go into a new package, cunningly named {NHSRGreetings}. And here’s how we’re going to do it.\nIn R Studio, go to File and then to New Project. Click on New Directory, and then click on R Package. I am using RStudio 1.2 Preview for this tutorial which is downloadable from the R website. I would recommend doing this as some of the package management has been greatly simplified and some frustrating steps removed.\n\n\nScreenshot of RStudio’s package preview wizard\n\nClick on ‘Open in new session’ (so we can see the original code), and set the Package name as {NHSRGreetings}. We could just pull our old source files into the package – but for this tutorial I’m going to do things the longer way so you also know how to create new functions within an existing package.\nSet the project directory to somewhere memorable.\nFor now don’t worry about the git or {packrat} options – those are tutorials within themselves!\nYou are greeted with a package more or less configured up for you. A single source file, hello.R is set up for you within an R directory within the package. It’s not as cool as our function of course, but it’s not bad! It comes with some very helpful commented text:\n# Hello, world!\n#\n# This is an example function named 'hello'\n# which prints 'Hello, world!'.\n#\n# You can learn more about package authoring with RStudio at:\n#\n#   http://r-pkgs.had.co.nz/\n#\n# Some useful keyboard shortcuts for package authoring:\n#\n#   Install Package:           'Cmd + Shift + B'\n#   Check Package:             'Cmd + Shift + E'\n#   Test Package:              'Cmd + Shift + T'\nSo let’s check if the comments are right – hit Cmd + Shift + B on a Mac (on Windows and Linux you should see slightly different shortcuts). You can also access these options from the Build menu in the top right pane.\nYou will see the package build. R will then be restarted, and you’ll see it immediately performs the command library(NHSRGreetings) performed, which loads our newly built package.\nIf you type hello() at the command line, it will do as you may expect it to do!\nSo – time to customise our blank canvas and introduce our much more refined greeter.\nIn the root of our project you will see a file called DESCRIPTION. This contains all the information we need to customise our R project. Let’s customise the Title, Author, Maintainer and Descriptions for the package.\nWe can now create a new R file, and save it in the R subdirectory as greet_person.R. Copy over our greet_person function. We should be able to run install and our new function will be built in as part of the package.\nWe can now get individual team members to open the package, run the build once on their machine, and the package will be installed onto their machine. When they want to use any of the functions, they simply use the command library(NHSRGreetings) and the package will be ready to go with all the functions available to them. When you change the package, the authors will have to rebuild the package just the once to get access to the new features.\nWhen writing packages it is useful to be very wary about namespaces. One of the nice things about R is that there are thousands of packages available. The downside is that it makes it very likely that two individuals can choose the same name for a function. This makes it doubly important to pick appropriate names for things within a package.\nFor example, what if instead of the NHS-R Community package someone wrote a {CheeseLoversRCommunity} package with a similarly names greet_person, but it did something totally different?\nIn a script, you have full control over the order you load your packages, so R will happily let you call functions from packages and trust that you know what order you loaded things in.\nIf you are a package author however, it’s assumed you may be installed on many machines, each with a potentially infinite set of combinations of different packages with names that may clash (or if they don’t already they might do in the future).\nSo within the package, any function which doesn’t come from R itself needs to have clearly defined which package it has come from.\nWithin DESCRIPTION you must define which package you use, and the minimum version. You do this with the Imports keyword. Attached is the Imports section of one of the SSNAP packages:\nImports:\n    methods (&gt;= 3.4.0),\n    lubridate (&gt;= 1.7.4),\n    tibble (&gt;= 1.4.2),\n    dplyr (&gt;= 0.7.5),\n    tibbletime (&gt;= 0.1.1),\n    glue (&gt;= 1.2.0),\n    purrr (&gt;= 0.2.5),\n    rlang (&gt;= 0.2.0),\n    readr (&gt;= 1.1.1),\n    stringr (&gt;= 1.3.1),\n    ssnapinterface (&gt;= 0.0.1)\nNext within your functions, rather than just calling the functions use the package name next to the function. For example instead of calling mutate() from the {dplyr} package, refer to it as dplyr::mutate() which tells R you mean the mutate function from the {dplyr} package rather than potentially any other package. There are ways to declare functions you are using a lot within an individual file – but this method makes things pretty foolproof.\nAnother tip is to avoid the {magrittr} pipe within package functions. Whilst {magrittr} makes analysis scripts nice and clean, firstly you still have the namespace issue to deal with (%&gt;%).\nIs actually a function, just one with a funny name – it is really called magrittr::%&gt;%() !) Secondly the way {magrittr} works can make debugging difficult. You don’t tend to see that from a script. But if you’re writing code in a package, which calls a function in another package, which calls code in another package, which uses {magrittr} – you end up with a really horrid nest of debugging errors: it is better to specify each step with a single variable which is reused.\nWhen you’ve got your code in, the next important thing to do is check your package. Build simply makes sure your code works. Check makes sure that you follow a lot of ‘rules’ of package making – including making sure R can safely and clearly know where every R function has come from. Check also demands that all R functions are documented: something which is outside of the scope of this tutorial and is probably the subject for another blog post – a documented function means if you type ?greet_person that you should be able to see how to use the function appropriately. It can help you create your own website for your package using the pkgdown package.\nIf your package both builds and checks completely and without errors or warnings, you might want to think about allowing the wider public to use your project. To do this, you should consider submitting your project to CRAN. This involves a fairly rigorous checking process but means anyone can download and use your package.\nIf we can get enough people to develop, share their code and upload their packages to CRAN we can work together to improve the use of R across our community.\nFeedback and responses to @drewhill79.\nThis blog was written by Dr. Andrew Hill, Clinical Lead for Stroke at St Helens and Knowsley Teaching Hospitals.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/a-roundup-from-the-2022-nhs-r-conference-part-1.html",
    "href": "blog/a-roundup-from-the-2022-nhs-r-conference-part-1.html",
    "title": "A roundup from the 2022 NHS-R Conference – Part 1",
    "section": "",
    "text": "What are the latest health research applications using R? A roundup from the 2022 NHS-R conference.\nWith 47 projects presented during this year’s NHS-R conference, the event aims to promote the use of R in healthcare. It is also an opportunity to learn about the work done by analysts in the NHS and share good practices across public and private healthcare organisations.\nPresentations were led by a diverse range of people, from the expected analytical teams, to developers, academics, and medical experts such as doctors and pharmacists.\nWe attended this year’s annual conference in Birmingham on behalf of RwHealth, a digital health and life science organisation. In this series of blogs we provide a quick takeaway for 20 presentations. Our micro-summaries are sorted by topic.\nIf you are interested in the topic, we encourage you to watch the full presentation. Day 1 and 2 recordings are publicly available.\nDisclaimer – We have categorised selected papers into main topics, but this is not a complete overview of all presentations. The summaries are brief and do not include all details. To fully understand the content, we recommend watching the recorded videos. Additionally, the highlighted points in each paper may not align with the presenter’s main points.\nR for simulating efficiency models\nFollowing the COVID-19 pandemic, waiting lists have dramatically increased and waiting times often exceeded the targets for acute trusts. To accurately predict behaviour of the waiting list, Christopher Reading-Skilton (Worcestershire Acute Hospitals NHS) used the R Simmer package to model patient pathways. The model, called Pythia, is a stochastic discrete event simulation model that forecasts demand, derives treatment pathways from patient history, across specialty levels. The model is still under development and aims to provide guidance on resource usage and prioritisation.\nLongest to date waits for ambulances have been recorded this winter making Martina Fonseca’s – DART NHSE – presentation timely. The project uses the RSimmer package to develop a model tackling ambulance response time breaches and handover delays. The discrete event simulation (DES) model, still at the prototype stage, could be used for resource redeployment to hospital sites based on thresholds.\nFollowing admissions to acute care, some patients with complex care needs have to be transferred to adult community and social care. Transfer delays can happen because of inefficiencies in the system and lack of capacity. 500’000 bed days are estimated to be lost annually because of delayed transfer. Zehra Onon-Dumlu et al. developed a stochastic simulation tool to report on service usage, acute sector delays, acute delay and social care costs. You can read a related paper by the same authors here.\nWaiting lists\nWaiting lists in England have been increasing in the last few years. Dr. Richard Wood (NHS Bristol, North Somerset and South Gloucestershire CCG) presented his model predicting elective waiting times following COVID-19. With the pandemic disruption, the risk of dropping out of the waiting list before being seen increased dramatically. He proposes a scalable model based on referrals, reneges and treatment that could be applied to all trusts and specialties. The model provides multiple scenarios of future waiting list size based on referrals and capacity parameters.\nThe composition of waiting list attendees was explored in the South West by Simon Wellesley-Miller from NHS England. His project consists of creating an automated and reproducible tool to identify health inequalities. Using the XGBoost package, he was able to identify important patient and environmental characteristics that may lead to an emergency admission.\nR for good coding practices\nJessica Morley (Bennett Institute) guided us through some of the key recommendations from the ‘Goldacre review’ – to efficiently and safely use healthcare data for research. Some of the key points to remember included the importance of building a ‘trusted’ research environment and practices to maintain privacy as well as standardising processes.\nOne of the Goldacre review recommendations is to make new source code open. However, NHS data is not systematically open source. Jonny Pearson (NHSE/DART) talks about sharing in the open. It is great to publish code at the end of a project, it is better practice to take a stepwise approach and share as the project progresses. You can hear more about it in this NHS-R podcast.\nHeather Turner (University of Warwick) shared good coding practices in R to make the work transparent, reproducible and maintainable. Great tips include file organisation for easier navigation, project workflow and package management.\nLaura Moscoviz is a Health Care and Life Science Consultant at RwHealth in London. As part of her role, she uses data science, technology, and predictive analytics to deliver insight-driven solutions to improve quality of care and operational delivery.\nEmail :  laura.moscoviz@realworld.health Twitter: @lhmosco\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/the-joy-of-r.html",
    "href": "blog/the-joy-of-r.html",
    "title": "The joy of R",
    "section": "",
    "text": "Hello. My name is Julian and I am an R addict. I got hooked about 3 years ago when I took on a new role in Public Health England developing a public health data science team. My professional background is as a doctor and Consultant in Public Health and have spent the last 15 years in the health intelligence field so I thought I knew something about data and analysis. I realised I didn’t know anything about data science so I decided to do a course and ended up doing the Coursera data science MOOC from Johns Hopkins because it was health related. For the course, you need to learn R - and so my habit started. (It turned out I knew nothing about data and analysis as well).\nI had done an R course 15 years ago but never used it. Any analysis I did used spreadsheets, SPSS, Mapinfo and host of other tools, and I had never written a single line of code until 3 years ago (apart some very basic SQL). That’s all changed.\nApart from a brief obsession with Tableau a few years ago (which I still love), learning R has for me, been utterly transformational. Now my basic analytical workflow is R + Google (for getting answers when you are stuck) + Git (for sharing and storing code) + Mendeley (reference management software). That’s it.\nI barely open Excel except to look at data structure so I can import data into R; I don’t use GIS; I hardly even open word to write a document - I do that in R (like this blog); and recently the option to output to power point has appeared in R Markdown so I’ve started using that as well.\nOn top of that I have learned a whole heap of analytical and other skills through using R. I feel comfortable getting and analysing data of any size, shape and complexity including text, websites, APIs, very large datasets; and quickly. I can now rapidly produce scientific reports, scrape websites, mine text, automate analysis, build machine learning pipelines, create high quality graphics using the fab ggplot2 and its relatives, have co-authored a package to read Fingertips data (fingertipsR - very proud of this) and am getting my head around regular expressions. I have even managed a couple of Shiny documents. There is nothing I have wanted to do that I can’t do in R; and a huge range of things I didn’t know you could do or had never heard of.\nSo what is it about R that makes it so great? In the last 5 years it has moved from an academic stats package to a professional data science tool. One of the reasons is the development of the tidy data framework [1] and tools to make data wrangling or munging much easier. This is a much overlooked part of the analysts life - all the things you need to do with data before you can analyse it (50 - 70% of the process) has been paid serious attention and made much easier with packages like dplyr and tidyr. And a lot of attention has been made to making coding more logical and syntax more “English”. Another reason is the development of R Studio and R Markdown which give you button press outputs in a range of high quality formats. And there is a focus on reproducibility - the ability for analysis to be repeated exactly which requires combining data, analysis and results in a form others can follow. This is good science and will become much more widespread. You can do this in R and Git.\nMy addiction has infected my team and the analytical community in PHE. We are spreading R rapidly and writing packages to automate routine analysis and reporting. We routinely use Gitlab to share and collaborate on code, and are introducing software development ideas like code review and unit testing. In short we are trying to help analysts (if they want to) become analyst-developers.\nThere are downsides to R of course. There is a (big) learning curve, ICT get twitchy, there is a huge range of packages and any number of ways of doing things, and things often break. But as any addict would say, these are just obstacles to be overcome and there is a lot of support out there.\nR is not the only direction of travel - we do use PowerBI (running R scripts), and we do a bit of development in Python, but one thing is certain - I can’t go back to pre R days.\nSo there’s my confession. I’m a data junkie and an R addict. If you want to see my descent I put stuff on an RPubs page from time to time and I have a Github page. If you want to help me - feel free to get into touch or send me a pull request.\nThanks to Seb Fox at PHE and David Whiting of Medway Council for inspiration and support."
  },
  {
    "objectID": "blog/the-joy-of-r.html#references",
    "href": "blog/the-joy-of-r.html#references",
    "title": "The joy of R",
    "section": "References",
    "text": "References\n1 Wickham H. 2014;59:1–23. doi:10.18637/jss.v059.i10"
  },
  {
    "objectID": "blog/posit-nhs-r-a-perfect-partnership.html",
    "href": "blog/posit-nhs-r-a-perfect-partnership.html",
    "title": "Posit + NHS-R: a Perfect Partnership!",
    "section": "",
    "text": "In mid-November of 2022, Posit had the great opportunity to attend the NHS-R Community Conference in Birmingham, UK. While the world is still reeling from the effects of COVID-19, it was comforting to know that this R community and organizers thereof can overcome immense challenges and host a wonderful and safe in-person/virtual hybrid conference.\nPeople may wonder why Posit would send two US-based employees to England to attend NHS-R? We will certainly touch upon that, but first, we’d like to highlight what stood out to us during our time at this event in the beautiful Edgbaston Stadium.\n\nRange in topics – The variety of topics presented at NHS-R was astonishing and a great reminder of how limitless the R language truly is! In listening to the variety of speakers during the two-day event, we witnessed everything from how to create impactful and intuitive visualizations to hospital capacity modeling to cancer therapy treatment customization. The number of thoughtful approaches to specific, common issues was impressive, but what really stood out to us was the powerful, yet unspoken central theme that persisted across all talks: how R can positively impact human health. It is amazing to consider that a specific combination of mouse clicks and keyboard strokes is quite literally saving lives every day.\nA unique and well-organized event – NHS-R Conference is a relatively young event established in 2018, and only the 3rd time the conference has been held in person. That said, we were immensely impressed with how well-organized the conference was and how NHS-R has evolved to accommodate both in-person and virtual attendees – setting an excellent example for future NHS-R (and any similar organizational & community) conferences. We also applaud the uniqueness of NHS-R by hosting a 2-day event sandwiched between numerous virtual workshops for beginner and advanced R users, which spanned the entire month of November. Regardless of where you are in your data science journey, NHS-R had something catered to you. While COVID-19 has certainly imposed limitations on future in-person events, the conference organizers have fully capitalized on the benefits of virtual learning!\nExpanding beyond R – for the first time, NHS-R added a Python track to their conference, which was in the same conference space but dedicated to a unique set of topics and speakers. This enabled attendees to pick and choose which R and Python-based talks were most exciting & valuable for them and to attend sessions accordingly. This was especially thrilling for us to witness as the inclusion of Python in this historically-R-based conference is well-aligned with Posit’s continued mission to support open-source, language-agnostic data science. For NHS-R, this opens many doors for improving England’s health system, and we are excited to see the positive future impacts of enabling more individuals across the organization!\n\nThe points above should be enough rationale to convince anyone to attend the NHS-R Conference in 2023, but we’d like to explain why Posit is so invested in NHS-R. Posit’s mission is to create open-source software for data science, scientific research, and technical communication. We are a Public Benefit Corporation, meaning we have a fiduciary responsibility to address social, economic, and environmental needs to benefit our customers, employees, and the community. Per the NHS Constitution for England, the NHS “belongs to the people,” and “is founded on a common set of principles and values that bind together the communities and people it serves – patients and public – and the staff who work for it.” As you can see, the mission of the NHS is aligned with what Posit hopes to support.\nWe are excited to continue this perfect partnership long into the future!\nAuthors:\nRyan Johnson and Lauren Chadwick, Posit\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/but-this-worked-the-last-time-i-ran-it.html",
    "href": "blog/but-this-worked-the-last-time-i-ran-it.html",
    "title": "But this worked the last time I ran it!",
    "section": "",
    "text": "A common scenario: you are new to a programming language, chuffed that you’ve done a few bits and progressing well. A colleague hands you their code to run a regular report. That’s fine, you’ve got a smattering of understanding, and this is existing code, what could go wrong? But the code doesn’t run. You check through it but nothing looks obviously out of place and then your colleague adds to your utter confusion by saying it works on their machine.\nAnother scenario: you are new to a programming language and have written some code. It’s been working surprisingly well and you feel very pleased with your progress but then, suddenly, it fails. You look on at the computer, stunned. It worked the last time you ran it! You’ve made no changes and yet lots of red errors now appear and these errors are, quite frankly, utterly baffling and even googling them turns up all manner of strange discussions which might as well be in Old English (sort of familiar but you have no idea what it’s saying).\nThe solution: well there won’t necessarily be just one solution but here are a few things I’ve picked up in my early days using RStudio. I’m still in those early days and I probably haven’t encountered all that could possibly go wrong so please add to these and definitely comment if I’ve made any glaring errors!\n\nMy colleague can run this script on this very same computer but I can’t! What’s that all about?\n\nRStudio allows you to install and run your own packages (if your network allows) and that’s really useful when you just want to try something out, follow up on a recommendation or install something a training course has required. Given our strict network and IT installations this is quite a liberating experience!\nBut what isn’t apparent when you are merrily installing packages is that these are installed to your own folder so on a shared network this may not be accessible by a colleague. Step one in the solving the problem is to check the package is installed on your machine and your profile.\nYou may now be familiar with:\ninstall.packages(\"dplyr\") to install and then library(dplyr)\nbut consider using\nif (!require(dplyr)) install.packages(\"dplyr\")\nso RStudio will install this package if it is missing. Very useful when sharing R scripts as they can just be run with no typing by the recipient.\n\nI ran this code the other day and it was fine and now I get errors – but I haven’t done anything!\n\nThis happened recently to a colleague and prompted me to write this blog because I thought, this is probably the kind of thing that happens all of the time and if no one tells you this how could you know? Well there is Google but it’s too much to type I ran this code the other day and it now gives me an error….\nMy colleague’s code had been working, she hadn’t made any changes but one day it just failed. It wasn’t as if she ran it, ran it again seconds later and it failed, this was a run-it-one-day, it works and run-it-the-next-day, it fails. She asked if I could help and all I could think of was to run it myself. Not exactly expert support I thought. A bit like calling IT with a computer problem and being asked if you’ve rebooted your machine but a bit more advanced than “have you switched it on?”. Something strange happened when I ran it; it worked.\nJust as a plug for another blogger she was recreating the plot from this blog:\nhttps://www.johnmackintosh.com/blog/2017-12-21-flow/\nhttps://github.com/johnmackintosh/RowOfDots\nThis was puzzling but I had a faint recollection from other R people’s stories that you should keep your packages are up to date. One course had even said about updating these regularly and had, thankfully, shown us what to do.\nPackages are regularly being updated, a few tweaks here and there I guess. Plus many are built on other packages (like {dplyr} and {ggplot2}) so if they are updated then it’s like a domino effect. RStudio is nicely set up so you don’t have to go to the internet to find out individually what you need to update, you just need to go to ‘Packages’ in the bottom right hand panel of RStudio, select ‘Update’ which has a green circle with an arrow and it brings up a list of what needs updating.\n\n\n\nScreenshot of the packages tab from RStudio along with Files, Plots, Help and Viewer\n\n\nIf you’ve not done this for a while you may have quite a few updates!\n\n\n\nScreenshot of the packages needed to be updated from selecting the RStudio update\n\n\nEagle eyed readers may recognise this Public Health package {fingertipscharts}. If not, check it out!\n\n\n{fingertipscharts} is not on CRAN and can be found at the GitHub repository https://github.com/ukhsa-collaboration/fingertipscharts\nIf you are like me you may have installed some packages that you now rarely use and have no idea what they are. They may ask the following in the console (bottom left of the screen):\nDo you want to install from sources the packages which need compilation?\n\n\n\nScreenshot of the RStudio Console message when packages require compilation\n\n\nThis prompt is so that the package can be updated by building it on your computer. I’ve got a couple of packages that I have tried to do this but each time I go to check for updates they are still requesting an update so I just say no so I can fly through the other updates.\nFinally, a bit of a vague warning as I don’t understand this part but I once updated packages after I’d run a couple of scripts. This meant that a couple of the packages that needed updating were already loaded and so things got a bit muddled. I’m not entirely sure if this is a problem but I now shut all projects and code and run a new R Studio screen to do updates.\nThis blog was written by Zoe Turner, Senior Information Analyst at Nottinghamshire Healthcare NHS Foundation Trust.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/how-nhs-r-community-do-the-apprentice.html",
    "href": "blog/how-nhs-r-community-do-the-apprentice.html",
    "title": "How NHS-R Community do The Apprentice…",
    "section": "",
    "text": "One of the tasks on the Apprentice a number of years ago was for the contestants to put on a corporate event, at no small cost to the people attending I might add. It’s a tale often told because one of the contestants was gluten free and no one had accounted for dietary needs amongst the contestants so the poor ‘gluten free lady’, as she was known, was served a fruit salad.\nThe reason I’m apparently going off tangent so early in a blog, is that it struck me that the Apprentice is all about throwing people in at the deep end and seeing how they cope. It’s entertainment but clashes with the premise that these are potential apprentices to a ‘British business magnate’ (as Wikipedia calls him). Contrast this with NHS-R and how I came to be attending the Train the Trainer event at the end of 2019 and then helped to run the first of 3 local courses this January, having only just started learning R around 2 years ago.\nAnalysts have many expectations made of them. They have to be technical, able to interpret data and communicate themselves clearly to non-analysts. Very rarely though will an analyst be expected to train others. Some may produce or present some training to support or mentor fellow analysts, and even then my experience has always been on the receiving end. Coupled with the fact that I’ve never really had a burning desire to teach, it was a surprise to find myself on a course on how to deliver the NHS-R ‘Introduction to R’ workshop.\nThe reason I did it is that my involvement with NHS-R has led to this natural consequence of training others. I started with attending the course myself, then presented at the conference and facilitated an Introduction Course run by NHS-R but hosted by my Trust. I then didn’t hesitate in agreeing to taking on the training.\nNHS-R Community held their first two-day Train the Trainer event in Birmingham organised through AphA (Association of Professional Healthcare Analysts). I was supported to go on this by my manager, Chris Beeley, who is a huge advocate of R and Shiny. Whilst he himself has run several workshops over the years I, notably, have run zero!\nAt the TtT (got to get an acronym in there) I had the opportunity to meet lots of motivated people from around the British Isles who were as keen as I was, not only to learn how to teach R but also to talk about data – that happened quite a bit in the breaks. We had an opportunity to present to each other, and that was useful as I learn especially from watching others. Everyone has their own style and it gets perfected over time but I was hugely impressed by how engaging people were and how quickly they could read about a subject that was new to them (we looked at the RStudio presentation slides https://education.rstudio.com/teach/materials/ and then go on to express clearly what they’d just learned.\nI could go on about what I learned at the course, but the proof of its value is in what I did with it. And so on 17th January, Chris and I held a workshop for 15 people from various organisations; some had travelled as far as London and Portsmouth, such is the commitment to learn R. Chris led the workshop and I did the live coding/support role which took the edge off that ‘first time’ feeling.\nThis idea of having at least two people in a workshop is a good one, even when the trainer is very experienced. Chris, for example, is very used to running workshops alone, but inevitably people get stuck or things don’t work as they should and so I did the helping while Chris moved between training and coding. It felt, to me at least, like the burden was shared. It helped to ensure that no-one in the group was allowed to fall behind so far that they just gave up.\nChris and I had gone through the slides beforehand as he’d not gone on the TtT course, and having not written them himself, I wanted to make sure he’d know what was covered. What reassured me was that, as he presented the course, there wasn’t a part of it that I didn’t understand myself and couldn’t cover if I had to take over at that moment. And so the day wasn’t as nerve-racking as I anticipated, and I had fun – which must have been noticeable to the group, as I had an email commenting on how we are clearly a happy team!\nWhilst I haven’t actually run a workshop, I think the process I’ve gone through to get to this point has certainly built up my confidence to do it. I’ve taken every opportunity NHS-R community has offered, from doing the introduction to presenting at the conference, and so this next thing – to run the training myself – hasn’t been so scary after all. I feel like a happy and well-supported apprentice of NHS-R, and the great thing about NHS-R in this is that everyone can be an apprentice too – you just have to get involved.\nBeing open-source, all the slides for the trainer course and the introduction course are available on GitHub:\nTrain the Trainer course materials can be found at:\nhttps://github.com/nhs-r-community/train_the_trainer\nCourse materials for the underlying Intro to R course are found at:\nhttps://github.com/nhs-r-community/intro_r\nZoë Turner, Senior Information Analyst, Nottinghamshire Healthcare NHS Foundation Trust.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/coffee-and-code.html",
    "href": "blog/coffee-and-code.html",
    "title": "Coffee and Code",
    "section": "",
    "text": "Coffee and Code is an informal community gathering where people can come together to learn, share, and collaborate on all things R. People can give presentations on their work, ask questions, and share resources. The goal of Coffee and Code is to create a supportive and inclusive environment where everyone can feel comfortable asking for help and sharing their knowledge.\nCoffee and Code is open to people of all skill levels, from beginners to experienced R users. The sessions are a great way to learn new things, meet other people who are interested in R, and get help with your own projects.\nWhen is coffee and code?\nIt is an hour every other Friday at 11:00 – 12:00.\nWho is it aimed at?\nEveryone of course. We did a rough and self-determined straw poll. About 40% said they were beginners, 45% intermediate and 15% advanced. So please don’t feel that it is any way elitist.\nWhat do the sessions look like?\nThey are all different, the whole thing is organic, and we don’t have a strict agenda.\nA rough agenda\n\n5 mins at the start to say hello and provide any updates.\n35 – 45 mins as show and tells, this may be one or several demos of things that people have been doing.\n5 mins for quick tip or library of the week.\nWhatever time is left is then open to general discussion and going over the chat questions. This is a great place for people to post queries or ask for assistance. Hopefully this discussion will inform what will happen at the next session and get some volunteers to come forward.\n\nWe have a set of rules\n\nFirst rule of C&C club – Tell everyone about Coffee and Code.\nTraining sessions will be recorded, but general discussion will not.\nVegas Rules – what happens and C&C stays in C&C.\nWe share code and ideas, never patient data.\nBe open and honest with a willingness to participate.\n\nNo subject is too simple, and we all know there are many ways to program and quite often someone will post a link to a shortcut or a library to make life easier.\nIt is a very friendly atmosphere, and we try to be as inclusive as possible to whatever level you are at. This is all about sharing those little hints, tips and discoveries to make life easier.\nWhen we do run tutorials we publish our code, with reproducible examples if possible.\nWhat are our goals?\nWe want to have set space where we can get together as a community. This also means we link with other communities such as Analyst X, RAP and NHS-R. This helps to maintain the strong networking and positive momentum that are hallmarks of the NHS R community.\nA strong community also gives us a strong voice. This is important to support those struggling with installation issues or getting the right training or all the weird and wonderful things that challenge us. It means we can identify training needs and help commission new courses.\nGoing to say it again three times because it is super important – community, community and community!\nWhat have you missed?\nWe have run sessions on how to write and use loops, how to make choropleth (heat) maps, how to make pretty tables, how to create CSS templates for markdown and quarto, how to create dynamic commentary for reports, how to send and read emails and many other things.\nJoin us!\nThat’s my final plea, come and join us and keep this wonderful community alive and be proud to be part of it.\nIf you want an invite, ping me an email at simon.wellesley-miller@nhs.net\nSimon Wellesley-Miller, Senior Analytical Manager\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/what-was-the-unconference.html",
    "href": "blog/what-was-the-unconference.html",
    "title": "What was the “Unconference”?",
    "section": "",
    "text": "I’m going to invite controversy and say it was the best part of this year’s NHS R Community annual conference.\nNot that the more traditional presentations in the main hall were not informative and interesting as ever.\nNot that it wasn’t great to hear an array of brilliant and enthusiastic speakers – analysts, managers, academics – show how they are using R (and Python!) to solve real world problems with statistics, data science, evidence and coding.\nNot that the new approach of inviting questions through Slack channels devoted to each of the talks didn’t lead to a lively interaction between audience and speakers.\nBut the Unconference embodied something else. Something that has been essential to the philosophy of the NHS R Community since its inception: the fact that NHS R is a community. Not a professional organisation. Not a cabal of experts who decant their wisdom to a group of acolytes. But a community built on participation – and on breaking down organisational boundaries and hierarchies.\nSo, what was it?\nThe brainchild of the hugely enthusiastic Dr Pawel Orzechowski from the University of Edinburgh, it was a simple idea to empower conference attendees.\nIn the morning of the first day of the conference, an ad hoc collage of snipped up conference timetables, oversized index cards, and sticky tape appeared on the window of the refreshments lounge.\nDuring the course of the day people (most of them who had come along as ordinary conference attendees, who had arrived without pre-prepared PowerPoint presentations, who had not expected to be giving talks) quietly came forward* to post their own ideas for rival conference sessions against the more formal timetable for the main session (*some of them may have also received a bit of enthusiastic cajoling).\nScrawled in felt tip. Stuck to the wall with sticky tape. In some cases offering to lead sessions, in others making pleas for “someone who knows what they’re talking about!” to step up.\nAn alternative, crowd-sourced alternative to the main conference – the Unconference – emerged.\nBy the end of the first day the scrappy timetable glued to the pristine window of Edgbaston Cricket Ground conference lounge had been populated with a full day’s session. Pawel keenly handed anyone going into the room with green and gold stickers, encouraging them to stick and vote.\nAround came day 2, and a new challenge.\nInstead of walking into a room next to the main conference session as planned, it emerged Unconference attendees would have to engage in an epic (and confusing) odyssey down the back stairs, through the bowels of the stadium, and up into a different part of the building.\nIt was a bit confusing. But fired with enthusiasm to nerd out discussing statistics and modelling, debate the practicalities of professional registration, put heads together to understand how to better use Quarto to automate reporting, and a whole range of other topics, lively groups and thoughtful individuals made the trek back and forth throughout the day.\nUnlike the main conference sessions these were active discussions. Crowded around the big round tables – at times pushing them together – participants leaned in to discuss, debate, disagree, challenge, and help each other grapple with things that mattered to them in their jobs, their careers, their professional community.\nOne high point for me was a nerd-out discussion on prediction intervals, bravely proposed and hosted by Simon Newey which involved lively discussions about the best error distributions to use when modelling hospital activity, the relative merits of classical vs Bayesian/simulation-based regression models, and the near-universal experience of stopping halfway through trying to build such a model to scratch your head and say “what does it all mean?”\nAnother was a discussion with NHS R Community founder and stalwart Prof Mohammed A Mohammed, about how we can move from lots of people independently working on the same problems – such as projecting/forecasting A&E attendance or hospital occupancy – again and again, and instead taking the best from all of those previous pieces of work and scaling them up collaboratively into sustainable approaches that the whole health and care system can use.\nThese were lively, disruptive (to use Prof Mohammed’s preferred phrase, in the tech innovation sense) sessions which brought together people from across the health and social care sector – hospital modellers, NHS England data scientists, civil servants, academics – and from across the career spectrum, from junior analysts through to senior (technical) managers and specialists, where their job roles, organisations and seniority were not mentioned. Only their ideas and their enthusiasm to use data and analytics to make the NHS – and the wider system – better for everyone.\nThis is the very spirit of the NHS R Community.\nBut it doesn’t end there!\nIf you’ve read this far, then we’ve got you. You have been co-opted.\nProf Mohammed wants you to tell him your ideas for how to solve this tricky problem of scaling our analytics into whole system solutions. Find out more in NHS-R Slack where you can also find Prof Mohammed.\nZoë Turner (Senior Data Scientist at The Strategy Unit and now coordinator for the NHS R Community) wants not just your ideas, but maybe even your membership of the NHS R Community Committee to shape its future. You can join those discussions at the NHS-R Slack, or get in touch with her directly at zoe.turner3@nhs.net.\nIn fact, the discussion did not stop at Edgbaston for any of the Unconference topics – they are all still live on the NHS-R Community #unconferencing-session-2023 Slack channel here, waiting for you to add your voice.\nBen Murch, Senior Analytics Manager NECS Consultancy and NHS R Community Senior Fellow\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/count-of-working-days-function.html",
    "href": "blog/count-of-working-days-function.html",
    "title": "Count of working days function",
    "section": "",
    "text": "It’s at this time of year I need to renew my season ticket and I usually get one for the year. Out of interest, I wanted to find out how much the ticket cost per day, taking into account I don’t use it on weekends or my paid holidays. I started my workings out initially in Excel but got as far as typing the formula =WORKDAYS() before I realised it was going to take some working out and perhaps I should give it a go in R as a function…\nChris Beeley had recently shown me functions in R and I was surprised how familiar they were as I’ve seen them on Stack Overflow (usually skimmed over those) and they are similar to functions in SQL which I’ve used (not written) where you feed in parameters.\nWhen I write code I try to work out how each part works and build it up but writing a function requires running the whole thing and then checking the result, the objects that are created in the function do not materialise so are never available to check. Not having objects building up in the environment console is one of the benefits of using a function, that and not repeating scripts which then ALL need updating if something changes."
  },
  {
    "objectID": "blog/count-of-working-days-function.html#bus-ticket-function",
    "href": "blog/count-of-working-days-function.html#bus-ticket-function",
    "title": "Count of working days function",
    "section": "Bus ticket function",
    "text": "Bus ticket function\nThis is the final function which if you run you’ll see just creates a function.\n\n# Week starts on Sunday (1)\nDailyBusFare_function &lt;- function(StartDate, EmployHoliday, Cost, wfh){\n\n  startDate &lt;- dmy(StartDate)\n  endDate &lt;- as.Date(startDate) %m+% months(12)\n\n# Now build a sequence between the dates:\n  myDates &lt;-seq(from = startDate, to = endDate, by = \"days\")\n\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7)-length(holidayLONDON(year = lubridate::year(startDate))) - EmployHoliday - wfh\n\nper_day &lt;- Cost/working_days\n\nprint(per_day)\n}\n\nRunning the function you feed in parameters which don’t create their own objects:\nDailyBusFare_function(\"11/07/2019\",27,612,1)\n[1] 2.707965"
  },
  {
    "objectID": "blog/count-of-working-days-function.html#going-through-each-line",
    "href": "blog/count-of-working-days-function.html#going-through-each-line",
    "title": "Count of working days function",
    "section": "Going through each line:",
    "text": "Going through each line:\nTo make sure each part within the function works it’s best to write it in another script or move the bit between the curly brackets {}.\nFirstly, the startDate is self explanatory but within the function I’ve set the endDate to be dependent upon the startDate and be automatically 1 year later.\nOriginally when I was trying to find the “year after” a date I found some documentation about {lubridate}’s function dyear():\n\n# Next couple of lines needed to run the endDate line!\nlibrary(lubridate)\nstartDate &lt;- dmy(\"11/07/2019\")\n\nendDate &lt;- startDate + dyears(1)\n\nbut this gives an exact year after a given date and doesn’t take into account leap years. I only remember this because 2020 will be a leap year so the end date I got was a day out!\nInstead, Chris Beeley suggested the following:\n\nendDate &lt;- as.Date(startDate) %m+% months(12)\n\nNext, the code builds a sequence of days. I got this idea of building up the days from the blogs on getting days between two dates but it has also come in use when plotting over time in things like SPCs when some of the time periods are not in the dataset but would make sense appearing as 0 count.\n\nlibrary(lubridate)\n\n# To run so that the sequencing works\n# using as.Date() returns incorrect date formats 0011-07-20 so use dmy from\n# lubridate to transform the date\n\n  startDate &lt;- dmy(\"11/07/2019\")\n  endDate &lt;- as.Date(startDate) %m+% months(12)\n\n# Now build a sequence between the dates:\n  myDates &lt;- seq(from = startDate, to = endDate, by = \"days\")\n\nAll of these return values so trying to open them from the Global Environment won’t do anything. It is possible view the first parts of the values but also typing:\n\n# compactly displays the structure of object, including the format (date in this case)\nstr(myDates)\n\n Date[1:367], format: \"2019-07-11\" \"2019-07-12\" \"2019-07-13\" \"2019-07-14\" \"2019-07-15\" ...\n\n# gives a summary of the structure\nsummary(myDates)\n\n        Min.      1st Qu.       Median         Mean      3rd Qu.         Max. \n\"2019-07-11\" \"2019-10-10\" \"2020-01-10\" \"2020-01-10\" \"2020-04-10\" \"2020-07-11\" \n\n\nTo find out what a function does type ?str or?summary in the console. The help file will then appear in the bottom right Help screen.\nNext I worked out working_days. I got the idea from a blog which said to use length and which:\n\n  working_days &lt;- length(which((wday(myDates)&gt;1&wday(myDates)&lt;7)))\n\nNote that the value appears as 262L which is a count of a logical vector. Typing ?logical into the Console gives this in the Help:\nLogical vectors are coerced to integer vectors in contexts where a numerical value is required, with TRUE being mapped to 1L, FALSE to 0L and NA to NA_integer._\nI was familiar with length(), it does a count essentially of factors or vectors (type ?length into the Console for information) but which() wasn’t something I knew about. Chris Beeley explained what which does with the following example:\n\n# Generate a list of random logical values\na &lt;- sample(c(TRUE, FALSE), 10, replace = TRUE)\n\n# Look at list\na\n\n [1]  TRUE FALSE FALSE  TRUE  TRUE  TRUE FALSE FALSE FALSE FALSE\n\n# using which against the list gives the number in the list where the logic = TRUE\nwhich(a)\n\n[1] 1 4 5 6\n\n# counts how many logical arguments in the list (should be 10)\nlength(a)\n\n[1] 10\n\n# counts the number of TRUE logical arguments\nlength(which(a))\n\n[1] 4\n\n\nThen Chris Beeley suggested just using sum instead of length(which()) which counts a logical vector:\n\nsum(a)\n\n[1] 4\n\n\nIt seems this has been discussed on Stack Overflow before: https://stackoverflow.com/questions/2190756/how-to-count-true-values-in-a-logical-vector\nIt’s worthy of note that using sum will also count NAs so the example on Stack overflow suggest adding:\n\nsum(a, na.rm = TRUE)\n\n[1] 4\n\n\nThis won’t affect the objects created in this blog as there were no NAs in them but it’s just something that could cause a problem if used in a different context.\n\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7)\n\n# Just to check adding na.rm = TRUE gives the same result\n  working_days &lt;- sum(wday(myDates)&gt;1&wday(myDates)&lt;7, na.rm = TRUE)\n\nI then wanted to take into account bank/public holidays as I wouldn’t use the ticket on those days so I used the function holidayLONDON( from the package {timeDate}:\n\nlength(holidayLONDON(year = lubridate::year(startDate)))\n\n[1] 8\n\n\nI used lubridate::year because the package {timeDate} uses a parameter called year so the code would read year = year(startDate) which is confusing to me let alone the function!\nAgain, I counted the vectors using length(). This is a crude way of getting bank/public holidays as it is looking at a calendar year and not a period (July to July in this case).\nI did look at a package called {bizdays} but whilst that seemed to be good for building a calendar I couldn’t work out how to make it work so I just stuck with the {timeDate} package. I think as I get more confident in R it might be something I could look into the actual code for because all packages are open source and available to view through CRAN or GitHub.\nBack to the function…\nI then added - EmployHoliday so I could reduce the days by my paid holidays and - wfh to take into account days I’ve worked from home and therefore not travelled into work.\nThe next part of the code takes the entered Cost and divides by the Working_days, printing the output to the screen:\nper_day &lt;- Cost/working_days\nprint(per_day)\nAnd so the answer to the cost per day is printed in the Console:\n\nDailyBusFare_function(\"11/07/2019\",27,612,1)\n\n[1] 2.707965"
  },
  {
    "objectID": "blog/count-of-working-days-function.html#a-conclusion-of-sorts",
    "href": "blog/count-of-working-days-function.html#a-conclusion-of-sorts",
    "title": "Count of working days function",
    "section": "A conclusion… of sorts",
    "text": "A conclusion… of sorts\nWhilst this isn’t really related to the NHS it’s been useful to go through the process of producing a function to solve a problem and then to explain it, line by line, for the benefit of others.\nI’d recommend doing this to further your knowledge of R at whatever level you are and particularly if you are just learning or consider yourself a novice as sometimes blogs don’t always detail the reasons why things were done (or why they were not done because it all went wrong!)."
  },
  {
    "objectID": "blog/create-synthetic-data.html",
    "href": "blog/create-synthetic-data.html",
    "title": "Creating synthetic data using the synthpop package",
    "section": "",
    "text": "This blog was originally a vignette in the NHSRdatasets package and refers to the synthetic data in that package for NEWS (National Early Warning Score)."
  },
  {
    "objectID": "blog/create-synthetic-data.html#what-is-synthetic-data",
    "href": "blog/create-synthetic-data.html#what-is-synthetic-data",
    "title": "Creating synthetic data using the synthpop package",
    "section": "What is Synthetic data?",
    "text": "What is Synthetic data?\nThe goal is to generate a data set which contains no real units, therefore safe for public release and retains the structure of the data.\nIn other words, one can say that synthetic data contains all the characteristics of original data minus the sensitive content.\nSynthetic data is generally made to validate mathematical models. This data is used to compare the behaviour of the real data against the one generated by the model."
  },
  {
    "objectID": "blog/create-synthetic-data.html#how-we-generate-synthetic-data",
    "href": "blog/create-synthetic-data.html#how-we-generate-synthetic-data",
    "title": "Creating synthetic data using the synthpop package",
    "section": "How we generate synthetic data?",
    "text": "How we generate synthetic data?\nThe principle is to observe real-world statistic distributions from the original data and reproduce fake data by drawing simple numbers.\nConsider a data set with \\(p\\) variables. In a nutshell, synthesis follows these steps:\n\nTake a simple random sample of \\(x_{1,obs}\\) and set as \\(x_{1,syn}\\)\n\nFit model \\(f(x_{2,obs}|x_{1,obs})\\) and draw \\(x_{2,syn}\\) from \\(f(x_{2,syn}|x_{1,syn})\\)\n\nFit model \\(f(x_{3,obs}|x_{1,obs},x_{2,obs})\\) and draw \\(x_{3,syn}\\) from \\(f(x_{3,syn}|x_{1,syn},x_{2,syn})\\)\n\nAnd so on, until \\(f(x_{p,syn}|x_{1,syn},x_{2,syn},...,x_{p-1,syn})\\)\n\n\nFitting statistical models to the original data and generating completely new records for public release. Joint distribution \\(f(x_1,x_2,x_3,…,x_p)\\) is approximated by a set of conditional distributions \\(f(x_2|x_1)\\)."
  },
  {
    "objectID": "blog/create-synthetic-data.html#synthetic-data-generation---national-early-warning-score-news-utilising-real-data",
    "href": "blog/create-synthetic-data.html#synthetic-data-generation---national-early-warning-score-news-utilising-real-data",
    "title": "Creating synthetic data using the synthpop package",
    "section": "Synthetic data generation - National early warning score (NEWS) utilising real data",
    "text": "Synthetic data generation - National early warning score (NEWS) utilising real data\nThe data this is based on is the NEWS Score devised by the Royal College of Physicians.\nSynthetic data can be generated from new data, utilising the above methodology, on the real observed data:\n\nlibrary(readr)\nlibrary(dplyr)\ndf &lt;- suppressWarnings(read_csv(\"https://raw.githubusercontent.com/StatsGary/SyntheticNEWSData/main/observed_news_data.csv\") %&gt;%\n  dplyr::select(everything(), -X1))\n\nglimpse(df)\n\nRows: 1,000\nColumns: 12\n$ male  &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1…\n$ age   &lt;dbl&gt; 68, 94, 85, 44, 77, 58, 25, 69, 91, 70, 87, 93, 61, 75, 97, 80, …\n$ NEWS  &lt;dbl&gt; 3, 1, 0, 0, 1, 1, 4, 0, 1, 1, 7, 2, 5, 1, 1, 3, 1, 5, 0, 2, 1, 2…\n$ syst  &lt;dbl&gt; 150, 145, 169, 154, 122, 146, 65, 116, 162, 132, 110, 166, 123, …\n$ dias  &lt;dbl&gt; 98, 67, 69, 106, 67, 106, 42, 56, 72, 96, 85, 90, 78, 80, 72, 81…\n$ temp  &lt;dbl&gt; 36.8, 35.0, 36.2, 36.9, 36.4, 35.3, 35.6, 37.2, 35.5, 35.3, 37.0…\n$ pulse &lt;dbl&gt; 78, 62, 54, 80, 62, 73, 72, 90, 60, 67, 95, 87, 93, 65, 89, 145,…\n$ resp  &lt;dbl&gt; 26, 18, 18, 17, 20, 20, 12, 16, 16, 16, 24, 16, 26, 12, 16, 16, …\n$ sat   &lt;dbl&gt; 96, 96, 96, 96, 95, 98, 99, 97, 99, 97, 87, 95, 96, 96, 98, 99, …\n$ sup   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0…\n$ alert &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ died  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n\n\nThis reads in the observed NEWS data from the GitHub repository. Now, we will utilise the synthpop package to create a synthetically generated dataset."
  },
  {
    "objectID": "blog/create-synthetic-data.html#generating-synthetic-news-dataset-using-synthpop-package",
    "href": "blog/create-synthetic-data.html#generating-synthetic-news-dataset-using-synthpop-package",
    "title": "Creating synthetic data using the synthpop package",
    "section": "Generating synthetic NEWS dataset using synthpop package",
    "text": "Generating synthetic NEWS dataset using synthpop package\nAs stated, now we will use the real observed data and generate a synthetic set, utilising the equations and process mapped out in the preceding sections:\n\nlibrary(synthpop)\nsyn_df &lt;- syn(df, seed = 4321)\n\nWarning: In your synthesis there are numeric variables with 5 or fewer levels: male, sup, alert, died.\nConsider changing them to factors. You can do it using parameter 'minnumlevels'.\n\nSynthesis\n-----------\n male age NEWS syst dias temp pulse resp sat sup\n alert died\n\n#### synthetic data\nsynthetic_news_data &lt;- syn_df$syn\nglimpse(synthetic_news_data)\n\nRows: 1,000\nColumns: 12\n$ male  &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1…\n$ age   &lt;dbl&gt; 56, 50, 74, 56, 52, 21, 37, 81, 67, 67, 56, 48, 76, 57, 43, 58, …\n$ NEWS  &lt;dbl&gt; 1, 2, 6, 1, 0, 2, 1, 2, 5, 0, 1, 1, 0, 1, 1, 1, 1, 1, 3, 0, 1, 6…\n$ syst  &lt;dbl&gt; 126, 115, 143, 122, 153, 164, 101, 125, 182, 160, 142, 122, 132,…\n$ dias  &lt;dbl&gt; 84, 84, 86, 60, 89, 92, 57, 74, 103, 80, 113, 71, 59, 71, 89, 11…\n$ temp  &lt;dbl&gt; 35.7, 36.8, 36.5, 36.3, 36.2, 35.5, 35.6, 36.6, 37.1, 36.2, 35.3…\n$ pulse &lt;dbl&gt; 72, 94, 82, 94, 78, 97, 76, 71, 95, 86, 73, 62, 88, 70, 63, 100,…\n$ resp  &lt;dbl&gt; 17, 14, 21, 12, 12, 20, 15, 17, 18, 18, 18, 16, 19, 16, 18, 18, …\n$ sat   &lt;dbl&gt; 98, 97, 93, 98, 96, 99, 98, 97, 94, 98, 98, 100, 99, 97, 97, 96,…\n$ sup   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1…\n$ alert &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ died  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n\n\n\nlibrary(ggplot2)\n# Create temperature tibbles to compare observed vs synthetically generated labels\nobs &lt;- tibble(label = \"observed_data\", value = df$temp)\nsynth &lt;- tibble(label = \"synthetic_data\", value = synthetic_news_data$temp)\n\n# Merge the frames together to get a comparison\nmerged &lt;- obs %&gt;%\n  bind_rows(synth)\n\n# Create the plot\nplot &lt;- merged %&gt;%\n  ggplot(aes(value, fill = label)) +\n  geom_histogram(alpha = 0.9, position = \"identity\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"#BCBDC1\", \"#2061AC\")) +\n  labs(\n    title = \"Observed vs Synthetically NEWS values\",\n    subtitle = \"Based on NEWS Temperature score\",\n    x = \"NEWS Temperature Score\", y = \"Score frequency\"\n  ) +\n  theme(legend.position = \"none\")\n\nprint(plot)"
  },
  {
    "objectID": "blog/diverging-bar-charts-plotting-variance-with-ggplot2.html",
    "href": "blog/diverging-bar-charts-plotting-variance-with-ggplot2.html",
    "title": "Diverging Bar Charts – Plotting Variance with ggplot2",
    "section": "",
    "text": "Diverging Bar Charts\nThe aim here is to create a diverging bar chart that shows variance above and below an average line. In this example I will use Z Scores to calculate the variance, in terms of standard deviations, as a diverging bar. This example will use the mtcars stock dataset, as most of the data I deal with day-to-day is patient sensitive.\nData preparation\nThe code below sets up the plotting libraries, attaches the data and sets a theme:\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nNext, we will change some of the columns in the data frame and perform some calculations on the data frame:\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nAs commented, this line uses the existing mtcars data frame and uses the dollar sign notation that is add a new column, or refer to a column, to create a column name called CarBrand. Then we assign the car brand (&lt;-) with the rownames from the data frame. This is obviously predicated on there being some row names in the data frame, otherwise you would have to name the rows using rownames().\nAdding a Z Score calculation\nA Z score is a calculation which uses the x observation subtracts said observation from the mean and divides by the standard deviation. The link shows the mathematics behind this, for anyone who is interested.\nThe following code shows how we would implement this score:\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg)) / sd(mtcars$mpg), digits = 2)\n\nThe statistics behind the calculation have already been explained, but I have also used the round() function to round the results down to 2 digits.\nCreating a cut off (above/below mean)\nThe next step is to use conditional algebra (first advocated by one of my heroes George Boole) to check whether the Z score I have just created is greater or less than 0:\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nThe ifelse() block looks at whether the Z Score is below 0, if so tag as below average, otherwise show this as above.\nThe next two steps are to convert the Car Brand into a unique factor and to sort by the Z Score calculations:\nNow, I have everything I need to start to compute the plot. Great stuff, so let’s get plotting.\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] # Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nCreating the plot\nFirst, I will start with creating the base plot:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score))\n\nHere, I pass in the mtcars data frame and set the aesthetics layer (aes) of the x axis to the brand of car (CarBrand). The y axis is the Z score I created for miles per gallon (mpg) and the label is also set to the z score.\nNext, I will add on the geom_bar geometry:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score)) +\n  geom_bar(stat = \"identity\", aes(fill = mpg_type), width = .5)\n\nThis indicates that I need to use the mpg_z_score field by forcing the stat=\"identity\" option. If this was not added, then it would simply count the number of times the Car Brand appears as a frequency count (not what I want!). Then, I stipulate the fill type of the bar to be equal to whether the value deviates above and below 0 – remember we created a field in the data preparation stage to store whether this deviates below and above 0 and called it mpg_type. The last parameter is the width parameter to indicate the width of the bars.\nNext:\n\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score)) +\n  geom_bar(stat = \"identity\", aes(fill = mpg_type), width = .5) +\n  scale_fill_manual(\n    name = \"Mileage (deviation)\",\n    labels = c(\"Above Average\", \"Below Average\"),\n    values = c(\"above\" = \"#00ba38\", \"below\" = \"#0b8fd3\")\n  )\n\nI use the scale_fill_manual() ggplot option to add the name to the legend, specify the label names using the combine function and stipulate that the values that are above average need to be hex coded by the value and the below values to a different code. I have weirdly chosen blue and green as an alternative to red, as I know we have accessibility there. We are nearly there, the final step is:\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x = CarBrand, y = mpg_z_score, label = mpg_z_score)) +\n  geom_bar(stat = \"identity\", aes(fill = mpg_type), width = .5) +\n  scale_fill_manual(\n    name = \"Mileage (deviation)\",\n    labels = c(\"Above Average\", \"Below Average\"),\n    values = c(\"above\" = \"#00ba38\", \"below\" = \"#0b8fd3\")\n  ) +\n  labs(\n    subtitle = \"Z score (normalised) mileage for mtcars'\",\n    title = \"Diverging Bar Plot (ggplot2)\", caption = \"Produced by Gary Hutson\"\n  ) +\n  coord_flip()\n\nHere, I have added the labs layer on to the plot. This is a way to label your plots to show more meaningful values than would be included by default. So, within labs I use subtitle, title and caption to add labels to the chart. Finally, the important command is to add the coord_flip() command to the chart – without this you would have vertical bars instead of horizontal. I think this type of chart looks better horizontal, thus the reason for the inclusion of the command.\nThe final chart, looks as illustrated hereunder:\n\n\n\n\n\n\n\n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutson-Hacks.\nThis blog has been formatted to remove Latin Abbreviations and edited for NHS-R Style and to ensure running of code in Quarto.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/dygraphs.html",
    "href": "blog/dygraphs.html",
    "title": "Dygraphs",
    "section": "",
    "text": "I recently presented some of the mortality surveillance charts we use to @RLadiesLondon (a very welcoming group!) and one that got some interest was a chart of Nottinghamshire Healthcare NHS Foundation Trust deaths compared to ONS Provisionally Registered deaths. The chart looks good because it’s interactive but this type of chart can be confusing because of the 2 y axes.\nWhen I show this report I make it clear that the two axes units are very different and that its purpose is to show that the pattern of the deaths in the wider population matches that of the deaths recorded by the Trust. It’s well known within Public Health that the pattern of deaths is seasonal, with a peak around January in the UK. However, this Public Health knowledge is not necessarily common knowledge in Secondary Care Trusts and it was one of the great benefits of having @IantheBee both create and present this report.\nGetting ONS Provisional Data\nI wrote about getting and formatting the spreadsheets from ONS for the East Midlands Provisionally Registered deaths:\nhttps://nhsrcommunity.com/blog/format-ons-spreadsheet/\nbut for the purposes of the mortality surveillance report I’ve used several years data and I normally keep the spreadsheets, save the merged data and then load that each time I need to run the R markdown report.\nFor the purposes of this blog I’ve amended the formatting code by adding functions so each year can be transformed and is available to plot:\n\nlibrary(janitor)\nlibrary(readxl)\nlibrary(tidyverse)\n\n# Download ONS spreadsheets Function-----------------------------------------------\n\nGetData_function &lt;- function(www, file) {\n  download.file(www,\n    destfile = file,\n    method = \"wininet\", # use \"curl\" for OS X / Linux, \"wininet\" for Windows\n    mode = \"wb\"\n  ) # wb means \"write binary\" }\n}\n\n# 2019\nGetData_function(\n  \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales/2019/publishedweek282019.xls\",\n  \"DeathsDownload2019.xls\"\n)\n\n# 2018\nGetData_function(\n  \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales/2018/publishedweek522018withupdatedrespiratoryrow.xls\",\n  \"DeathsDownload2018.xls\"\n)\n\n# 2017\nGetData_function(\n  \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales/2017/publishedweek522017.xls\",\n  \"DeathsDownload2017.xls\"\n)\n\n# 2016\nGetData_function(\n  \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/weeklyprovisionalfiguresondeathsregisteredinenglandandwales/2016/publishedweek522016.xls\",\n  \"DeathsDownload2016.xls\"\n)\n\n# Import correct sheets ---------------------------------------------------\nDeaths_Now &lt;- read_excel(\"DeathsDownload2019.xls \",\n  sheet = 4,\n  skip = 2\n)\nDeaths_2018 &lt;- read_excel(\"DeathsDownload2018.xls \",\n  sheet = 4,\n  skip = 2\n)\nDeaths_2017 &lt;- read_excel(\"DeathsDownload2017.xls \",\n  sheet = 4,\n  skip = 2\n)\nDeaths_2016 &lt;- read_excel(\"DeathsDownload2016.xls \",\n  sheet = 4,\n  skip = 2\n)\n\n# Look up code to remove excess rows --------------------------------------\nLookupList &lt;- c(\n  \"Week ended\",\n  \"Total deaths, all ages\",\n  \"Total deaths: average of corresponding\",\n  \"E12000004\"\n)\n\n# Function for formatting data --------------------------------------------\nTransform_function &lt;- function(dataframe) {\n  # Format data frames\n  df &lt;- dataframe %&gt;%\n    clean_names() %&gt;%\n    remove_empty(c(\"rows\", \"cols\")) %&gt;%\n    filter(week_number %in% LookupList)\n\n  # Transpose table\n  df &lt;- t(df)\n\n  # Whilst this is a matrix make the top row the header\n  colnames(df) &lt;- df[1, ]\n\n  # Make this object a data.frame\n  df &lt;- as.data.frame(df)\n\n  # Function to find 'not in'\n  \"%!ni%\" &lt;- function(x, y) !(\"%in%\"(x, y))\n\n  # List of things to remove to tidy the data.frame\n  remove &lt;- c(\"E12000004\", \"East Midlands\")\n\n  # remove the rows and ensure dates are in the correct format\n  df &lt;- df %&gt;%\n    filter(E12000004 %!ni% remove) %&gt;%\n    mutate(serialdate = excel_numeric_to_date(as.numeric(as.character(`Week ended`)),\n      date_system = \"modern\"\n    ))\n\n  df$`Week ended` &lt;- as.Date(df$`Week ended`, format = \"%Y-%m-%d\")\n\n  df &lt;- df %&gt;%\n    mutate(date = if_else(is.na(`Week ended`), serialdate, `Week ended`))\n\n  # Final transformation of data\n  df %&gt;%\n    select(`Total deaths, all ages`, date) %&gt;%\n    filter(!is.na(`Total deaths, all ages`)) %&gt;%\n    mutate(`Total deaths, all ages` = as.numeric(as.character(`Total deaths, all ages`))) # To match other data.frames\n}\n\n# Run functions -----------------------------------------------------------\nDeathsNow &lt;- Transform_function(Deaths_Now)\nDeaths2018 &lt;- Transform_function(Deaths_2018)\nDeaths2017 &lt;- Transform_function(Deaths_2017)\nDeaths2016 &lt;- Transform_function(Deaths_2016)\n\n# Merge data -----------------------------------------------------\nDeaths &lt;- bind_rows(\n  DeathsNow,\n  Deaths2018,\n  Deaths2017,\n  Deaths2016\n) %&gt;%\n  mutate(\n    date = as.Date(date),\n    `Total deaths, all ages` = as.numeric(`Total deaths, all ages`)\n  )\n\nThis code may give a few warnings saying that NAs have been introduced by coercion which is because there are many cells in the spreadsheets that have no data in them at all. It’s a good thing they have nothing (and effectively NAs) as having 0s could confuse analysis as it isn’t clear if the 0 is a real 0 or missing data.\nTo suppress warnings in R Markdown add warning = FALSE to the header, however, I like to keep the warnings just in case.\nIf you want to keep all the data after merging it together use:\n\nlibrary(openxlsx) # To write to xls if required.\n\n# Export complete list to excel for future\nwrite.xlsx(Deaths, \"ImportProvisionalDeaths.xlsx\")\n\nIf you’ve saved the combined file, to call it again in a script use the following code:\n\nlibrary(readxl)\n\nDeaths &lt;- read_excel(\"ImportProvisionalDeaths.xlsx\")\n\nDygraph chart\nThe following data is randomly generated as an example:\n\n\nUsing set.seed() changes the computer’s random number and this might have an affect in other programs or systems. Functions like withr::with_preserve_seed() can help with setting it for the code and resetting the seed when finished.\n\nlibrary(tidyverse)\nlibrary(dygraphs)\nlibrary(xts)\n\n# Fix the randomly generated numbers set.seed(178)\n\nAlldeaths &lt;- Deaths %&gt;%\n  select(date) %&gt;%\n  mutate(n = rnorm(n(), mean = 150))\n\n# Merge the two data frames together:\n\nONSAlldeaths &lt;- Deaths %&gt;%\n  left_join(Alldeaths, by = \"date\") %&gt;%\n  mutate(ds = as.POSIXct(date)) %&gt;%\n  select(ds, y2 = n, y = `Total deaths, all ages`) %&gt;%\n  arrange(ds)\n\n{dygraphs} require dates to be in a time series format and the package {xts} can convert it:\n\nONSAlldeaths_series &lt;- xts(ONSAlldeaths, order.by = ONSAlldeaths$ds)\n\nThe date column is no longer needed so can be removed but this needs to be done using base R and not {dplyr}:\n\n# Remove duplicate date column ONSAlldeaths_series &lt;-\nONSAlldeaths_series[, -1]\n\n                 y2     y\n2016-01-08 149.8720 13045\n2016-01-15 149.8883 11501\n2016-01-22 149.8568 11473\n2016-01-29 151.4292 11317\n2016-02-05 150.0193 11052\n2016-02-12 149.6231 11170\n2016-02-19 150.7557 10590\n2016-02-26 149.6230 11056\n2016-03-04 150.9004 11285\n2016-03-11 150.9368 11010\n       ...               \n2019-05-10 150.3396  9055\n2019-05-17 148.2407 10272\n2019-05-24 149.2737 10284\n2019-05-31 149.0598  8260\n2019-06-07 149.9367 10140\n2019-06-14 151.5180  9445\n2019-06-21 149.7602  9458\n2019-06-28 148.9243  9511\n2019-07-05 152.8544  9062\n2019-07-12 149.5628  9179\n\n\nFinally, the {xts} can be plotted:\n\ndygraph(ONSAlldeaths_series, main = \"East Midlands Weekly Deaths/Randomly generated numbers\") %&gt;%\n  dySeries(\"y\", axis = \"y\", label = \"East Midlands\") %&gt;%\n  dySeries(\"y2\", axis = \"y2\", label = \"Random numbers\") %&gt;%\n  dyAxis(\"x\", drawGrid = FALSE) %&gt;%\n  dyAxis(\"y\", drawGrid = FALSE, label = \"East Midlands\") %&gt;%\n  dyAxis(\"y2\", independentTicks = TRUE, drawGrid = FALSE, label = \"Random numbers\") %&gt;%\n  dyOptions(stepPlot = TRUE) %&gt;%\n  dyRangeSelector()\n\n\n\n\n\nWhen you’ve plotted the chart if you wave the cursor over the points you will see information about those points, you are also able to zoom in by using the scrolling bar at the bottom of the chart (this was from the dyRangeSelector() code.\nOther options are detailed here: https://rstudio.github.io/dygraphs/index.html\nONS Provisional data\nOne of the things that may stand out in the chart for the are the big dips around 29-31 December time each year and we presume that these relate to the week where Registrations may be delayed from GP practices to ONS because of the public holidays around Christmas.\nUnfortunately, only provisionally recorded deaths are available by week as confirmed are monthly: https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/deaths/datasets/monthlyfiguresondeathsregisteredbyareaofusualresidence\n\ndygraph(ONSAlldeaths_series, main = \"East Midlands Weekly Deaths\") %&gt;%\n  dyAxis(\"y\", independentTicks = TRUE, drawGrid = FALSE, label = \"East Midlands\") %&gt;%\n  dyAxis(\"x\", drawGrid = FALSE) %&gt;%\n  dyOptions(stepPlot = TRUE) %&gt;%\n  dyRangeSelector()\n\n\n\n\n\nThe case of the mysteriously disappearing interactive graph in R Markdown html output\nI’d rendered (knit or run) the html reports with the interactive graphs and it had all worked so I emailed the report to people as promised and then the emails came back: “Some of the graphs are missing, can you resend?”. Perplexed, I opened the saved file from the server and, yes, indeed some of the charts had disappeared! Where there should be lovely interactive charts were vast swathes of blank screen. What had happened? The code ran fine, looked fine and how do you even Google this mystery?\nTurns out my default browser, and I suspect it is throughout most of the NHS because lots of NHS systems depend on it, is Microsoft Explorer. Whilst I have the latest version these reports have never opened properly in Explorer.\nThe solution: Chrome (or some other browser). I ask people to copy the link from the Explorer web address bar after opening it from the email and simply paste it to Chrome.\nThis blog was written by Zoë Turner, Senior Information Analyst at Nottinghamshire Healthcare NHS Trust.\nThis blog has been edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html",
    "title": "Using R to track NHS winter pressures",
    "section": "",
    "text": "Every Thursday during winter, roughly from December to March, NHS Digital releases a week’s worth of hospital performance data, known as the Daily Situation Reports. This data often receives media attention because cold weather and seasonal illnesses can lead to higher demand for hospital care, meaning that services might be under increased pressure. At the Health Foundation, one of our aims is to provide new insight into the quality of health care through in-depth analysis of policy and data. So, to understand more about the current demand for hospital care and the challenges the NHS is facing, we keep a close eye on the latest seasonal trends.\nKeeping on top of NHS winter indicators has the potential to keep us analysts busy. The raw data is published in a fairly complex spreadsheet, requires a decent amount of cleaning and needs to be reanalysed after every release. In addition to monitoring national figures, this winter our team at the Health Foundation also wanted to see if there might be any variation between different areas of England. Sustainability and transformation partnerships (STPs) are areas where health and care leaders develop shared proposals for local services. Therefore, we enriched the raw data with information about where hospitals are located, and which STP they belong to. But with a similar analytical approach, more fine-grained local structures (such as Clinical Commissioning Groups) could be used.\nFor a more efficient and reproducible way of tracking NHS winter indicators this year, we moved to our whole analysis pipeline to R. We then used the clean data for visualisations in R and other applications, like Tableau. This blog takes you through our analysis workflow and describes how we got through some tricky steps. The complete R script is also available on GitHub, if you want to give it a go yourself. You can also read a blog on the Health Foundation’s website to find out why we looked at local areas this year and what we found."
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html#trust-exclusions",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html#trust-exclusions",
    "title": "Using R to track NHS winter pressures",
    "section": "Trust exclusions",
    "text": "Trust exclusions\nWe excluded the three children’s hospitals when calculating aggregate measures, such as the average bed occupancy within STPs. Our reasoning was that their patient profiles would be different from other acute trusts and this might skew the averages. Nevertheless, we kept track of them at a trust level.\nThis applies to Birmingham Women’s and Children’s NHS Foundation Trust (code RQ3), Alder Hey Children’s NHS Foundation Trust (RBS) and Sheffield Children’s NHS Foundation Trust (RCU).\n\ntrusts_to_exclude_for_aggregation &lt;- c(\"RQ3\", \"RBS\", \"RCU\")"
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html#how-we-approached-defining-missingness-when-is-a-zero-not-a-zero",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html#how-we-approached-defining-missingness-when-is-a-zero-not-a-zero",
    "title": "Using R to track NHS winter pressures",
    "section": "How we approached defining ‘missingness’: when is a zero not a zero?",
    "text": "How we approached defining ‘missingness’: when is a zero not a zero?\nData collection and validation errors do happen, so finding suspicious data points is an important step during data cleaning.\nWhile this is easy if a value is missing (or NA), it’s much harder to decide whether a zero truly represents ‘zero events’ or a missing value (in fact, it could even be both within the same data set). To distinguish between the two, at the Health Foundation we came up with the following criteria:\n\nHow likely is a ‘zero event’ for an indicator? For example, when counting beds in a large hospital the likelihood of having zero open seems small, but when counting long-stay patients having none seems possible.\nHow consistent is the zero value, in that trust, over time? Or in plain English: does the value jump from a higher number to zero (and back) or is it hovering somewhere close to zero.\n\nThe next two sections describe how we found and dealt with these missing values."
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html#finding-longer-periods-of-missing-data",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html#finding-longer-periods-of-missing-data",
    "title": "Using R to track NHS winter pressures",
    "section": "Finding longer periods of missing data",
    "text": "Finding longer periods of missing data\nIf any hospital trust had missing values, in any indicator, on 4 or more consecutive days during the reporting period, it was excluded from the analysis. We were only looking for these periods in variables where we would not expect any zeros (the list is shown as cols_to_check).\nWhy this particular cut-off? We wanted to aggregate the data and calculating weekly averages did not seem justified if we were missing more than half of a week for any trust.\nHere’s how we summed up how many consecutive days were zero or NA within each trust/variable combination:\n\n# Only check variables that are not derived from other variables\ncols_to_check &lt;- c(\n  \"core.beds.open\", \"total.beds.occd\",\n  \"more.than.7.days\", \"more.than.14.days\", \"more.than.21.days\"\n)\n\n# Find values that are 0 or NA\n# within any trust/variable combination\nSitrep_missing_or_zero &lt;- Sitrep_daily %&gt;%\n  filter(name != \"ENGLAND\") %&gt;%\n  # Updated code from `gather(cols_to_check, key = \"variable\", value = \"value\") %&gt;%`\n  pivot_longer(\n    cols = cols_to_check,\n    names_to = \"variable\",\n    values_to = \"value\"\n  ) %&gt;%\n  filter(value == 0 | is.na(value)) %&gt;%\n  # Sort and assign a period ID to consecutive days\n  arrange(code, variable, date) %&gt;%\n  group_by(code, variable) %&gt;%\n  mutate(\n    diff = c(0, diff(date)),\n    periodID = 1 + cumsum(diff &gt; 1)\n  )\n\n# Summarise consecutive days that variables are missing\nDays_missing &lt;- Sitrep_missing_or_zero %&gt;%\n  # remove trusts we already decided to exclude\n  filter(!is.element(code, trusts_to_exclude_for_aggregation)) %&gt;%\n  group_by(code, variable, periodID) %&gt;%\n  summarise(days = as.numeric((last(date) - first(date) + 1))) %&gt;%\n  arrange(desc(days))\n\n\nprint(Days_missing[Days_missing$days &gt;= 4, ])\n\n# # A tibble: 11 x 4\n# # Groups:   code, variable [11]\n#    code  variable          periodID  days\n#    &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt; &lt;dbl&gt;\n#  1 RTD   more.than.14.days        1    85\n#  2 RTD   more.than.21.days        1    85\n#  3 RTD   more.than.7.days         1    85\n#  4 RQW   more.than.14.days        1     8\n#  5 RQW   more.than.21.days        1     8\n#  6 RQW   more.than.7.days         1     8\n#  7 RQW   total.beds.occd          1     8\n#  8 RQW   core.beds.open           1     7\n#  9 RAX   more.than.14.days        1     6\n# 10 RAX   more.than.21.days        1     6\n# 11 RAX   more.than.7.days         1     6\n\nWhen we filtered for 4 or more consecutive days, we found that:\n\nThe trust with the code RTD reported zero long-stay patients (of any length of stay) for the whole reporting period to date, which seemed unrealistic for a general and acute hospital.\nTrust RQW had a gap of 7–8 days, that coincided for the indicators shown (we checked this separately in the raw data).\nTrust RAX reported zero long-stay patients (of any length of stay) for 6 days during January, but reported a high number before and after.\n\nBased on this, all variables from the trusts RTD, RQW and RAX were excluded from the analysis of this year’s (2018/19) winter data. This left us with 128 out of 134 trusts.\nIt’s worth noting that with this data-driven approach different trusts might be excluded each year and the number of excluded trusts could change over the winter period as new ‘gaps’ appear. Keep this in mind when making comparisons, both throughout the winter period and between years.\n\ntrusts_to_exclude &lt;- Days_missing %&gt;%\n  filter(days &gt;= 4) %&gt;%\n  ungroup() %&gt;%\n  # Extract column as vector\n  pull(code) %&gt;%\n  unique()\n\nprint(trusts_to_exclude)\n\n[1] \"RTD\" \"RQW\" \"RAX\"\n\n# [1] \"RTD\" \"RQW\" \"RAX\"\n\nSitrep_daily &lt;- Sitrep_daily %&gt;%\n  filter(!is.element(code, trusts_to_exclude))\n\ndim(Sitrep_daily)\n\n[1] 12012    12\n\n# [1] 12012    12"
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html#dealing-with-shorter-data-gaps",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html#dealing-with-shorter-data-gaps",
    "title": "Using R to track NHS winter pressures",
    "section": "Dealing with shorter data gaps",
    "text": "Dealing with shorter data gaps\nNext, we checked how many missing or zero values were left:\n\n# How many 1,2 and 3-day gaps are there?\nDays_missing %&gt;%\n  filter(!is.element(code, trusts_to_exclude)) %&gt;%\n  group_by(days) %&gt;%\n  count()\n\n# # A tibble: 3 x 2\n# # Groups:   days [3]\n#    days     n\n#   &lt;dbl&gt; &lt;int&gt;\n# 1     1    42\n# 2     2     3\n# 3     3     9\n\n# How are they distributed between trusts and variables?\nDays_missing %&gt;%\n  filter(!is.element(code, trusts_to_exclude)) %&gt;%\n  group_by(code, variable) %&gt;%\n  count() %&gt;%\n  spread(key = \"variable\", value = \"n\")\n\n# # A tibble: 11 x 6\n# # Groups:   code [11]\n#    code  core.beds.open more.than.14.days more.than.21.days more.than.7.days total.beds.occd\n#    &lt;chr&gt;          &lt;int&gt;             &lt;int&gt;             &lt;int&gt;            &lt;int&gt;           &lt;int&gt;\n#  1 RA2               NA                 1                NA               NA              NA\n#  2 RA3               NA                 1                 1                1              NA\n#  3 RA4                1                 1                 1                1               1\n#  4 RAL               NA                 1                 1                1              NA\n#  5 RBT               NA                 1                 1                1              NA\n#  6 RFR                3                 3                 3                3               3\n#  7 RHM               NA                 2                 2                2              NA\n#  8 RJ7               NA                 1                 1                1              NA\n#  9 RM3               NA                 1                 1                1              NA\n# 10 RQM               NA                 1                 1                1              NA\n# 11 RQX               NA                 3                 3                3              NA\n\nMost of the remaining gaps (42 out of 54) consisted of only a single day and they were mostly found in variables relating to long-stay patients. To judge whether these looked like real ‘zero events’ or were more likely to be reporting errors, we had a closer look at the data:\n\n# Extract and plot trusts with zeros in their data.\nSitrep_daily_small_gaps &lt;- Sitrep_daily %&gt;%\n  select(code, date, cols_to_check) %&gt;%\n  filter(code %in% Days_missing$code & !is.element(code, trusts_to_exclude)) %&gt;%\n  gather(cols_to_check, key = \"variable\", value = \"value\")\n\nggplot(Sitrep_daily_small_gaps, aes(x = date, y = value, group = code, color = code)) +\n  theme_bw() +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(\"variable\", scales = \"free_y\")\n\n\n\nBefore cleaning: plots showing the subset of trusts reporting ‘0’ in any (non-derived) variable.\n\n\n\nBased on the data before and after the zeroes, these were unlikely to be true values. It would have been possible to impute these gaps in some way, for example by taking the mean of day before and the day after. Instead, we took the approach that required fewest assumptions and we just replaced the gaps with NA:\n\n##### Original code (base R) gives error \"Error in `na_if()`:! Can't convert `y` &lt;double&gt; to match type of `x` &lt;tbl_df&gt;.\"\n\n# Create a 'clean' version where 0s were replaced with NA\n# Sitrep_daily[cols_to_check] &lt;- na_if(Sitrep_daily[cols_to_check], 0)\n\n\n# Coded to na using {tidyverse} functions and then plotted\nSitrep_daily_na &lt;- Sitrep_daily |&gt;\n  mutate(across(where(is.numeric), ~ na_if(., 0)))\n\nSitrep_daily_na &lt;- Sitrep_daily_na %&gt;%\n  select(code, date, cols_to_check) %&gt;%\n  filter(code %in% Days_missing$code & !is.element(code, trusts_to_exclude)) %&gt;%\n  gather(cols_to_check, key = \"variable\", value = \"value\")\n\nggplot(Sitrep_daily_na, aes(x = date, y = value, group = code, color = code)) +\n  theme_bw() +\n  geom_line() +\n  geom_point(size = 1) +\n  facet_wrap(\"variable\", scales = \"free_y\")"
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html#validating-derived-variables",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html#validating-derived-variables",
    "title": "Using R to track NHS winter pressures",
    "section": "Validating derived variables",
    "text": "Validating derived variables\nSome variables present in the data set were derived from others: for example,total.beds.occd should be the sum of core.beds.open and escalation.beds.open.\nWe could have discarded derived variables straight away and then computed them ourselves, in order to be completely sure about how they have been derived and what they mean. Since we were curious about their quality, we first checked if total.beds.open and occupancy.rate had been calculated as expected so we could decide whether or not to replace them (spoiler: yes, we did).\n\nSitrep_daily &lt;- Sitrep_daily %&gt;%\n  mutate(\n    total.beds.open.check = core.beds.open + escalation.beds.open,\n    occupancy.rate.check = total.beds.occd / (core.beds.open + escalation.beds.open)\n  )\n\n\n# Are the newly derives values the same as the existing ones?\nall(round(Sitrep_daily$occupancy.rate, 6) == round(Sitrep_daily$occupancy.rate.check, 6))\n# FALSE\nall(Sitrep_daily$total.beds.open == Sitrep_daily$total.beds.open.check)\n# FALSE\n\n# Where are the mismatches?\nSitrep_daily[\n  Sitrep_daily$total.beds.open != Sitrep_daily$total.beds.open.check,\n  c(\n    \"code\", \"date\", \"core.beds.open\", \"escalation.beds.open\", \"total.beds.open\",\n    \"total.beds.open.check\"\n  )\n]\n\n#  A tibble: 5 x 6\n#   code  date       core.beds.open escalation.beds.open total.beds.open total.beds.open.check\n#   &lt;chr&gt; &lt;date&gt;              &lt;dbl&gt;                &lt;dbl&gt;           &lt;dbl&gt;                 &lt;dbl&gt;\n# 1 -     2018-12-16          93489                 2125           95604                 95614\n# 2 -     2018-12-30          93014                 2475           95104                 95489\n# 3 -     2019-02-08          93931                 3718           97080                 97649\n# 4 R1H   2019-02-08           1270                   80             781                  1350\n# 5 RXK   2018-12-16            587                   22             599                   609\n\n\n# Looks like we will have to re-derive both variables after all\nSitrep_daily &lt;- Sitrep_daily %&gt;%\n  mutate(\n    total.beds.open = core.beds.open + escalation.beds.open,\n    occupancy.rate = total.beds.occd / (core.beds.open + escalation.beds.open)\n  )\n\nSimilarly, if it had been the focus of our analysis, we would also have re-derived national aggregates for England."
  },
  {
    "objectID": "blog/using-r-to-track-nhs-winter-pressures.html#making-read-outs-comparable-between-trusts-from-raw-counts-to-rates",
    "href": "blog/using-r-to-track-nhs-winter-pressures.html#making-read-outs-comparable-between-trusts-from-raw-counts-to-rates",
    "title": "Using R to track NHS winter pressures",
    "section": "Making read-outs comparable between trusts: from raw counts to rates",
    "text": "Making read-outs comparable between trusts: from raw counts to rates\nAs hospital trusts come in different sizes and serve different numbers of patients, raw patient counts are not suitable for comparisons between trusts or regions. Percentages or fractions, such bed occupancy rates, are more comparable.\nTherefore, we derived the fraction of occupied beds, which are occupied by long-stay patients over 7, 14 or 21 days:\n\n# rate of occupied beds that are occupied by long-stay patients\nSitrep_daily &lt;- Sitrep_daily %&gt;%\n  mutate(\n    more.than.7.rate = more.than.7.days / total.beds.occd,\n    more.than.14.rate = more.than.14.days / total.beds.occd,\n    more.than.21.rate = more.than.21.days / total.beds.occd\n  )"
  },
  {
    "objectID": "blog/designing-my-first-shiny-dashboard.html",
    "href": "blog/designing-my-first-shiny-dashboard.html",
    "title": "Designing my first Shiny dashboard",
    "section": "",
    "text": "Archived data\n\n\n\n\n\nThe code in this blog will not work as the files have been renamed for archiving.\nUpdated code can be found at Pablo’s Github Repository\n\n\n\nProject structure\nThe aim of this blog article is to describe the initial experience of creating a Shiny dashboard this process involved a bit of reading on markdown documents and Shiny apps to learn how to code it.\nWhen designing this dashboard, I aimed to cover the following basic steps:\n\nDownload open data from a Github Repository\nCreate several indicators with their population rates by using World Band API\nUse {plotly} library for interactive plots to animate charts and maps in the Shiny app\nBuild a Shiny dashboard containing different visualizations types\n\nBelow there is a detailed description of the steps followed to design the app.\n1. Download COVID19 data\nWhen creating any dashboard, I would like to feed daily data to it and also update it as soon as that data becomes available. Since the start of the pandemic, many resources have become available to analyze and visualize information about cases and deaths on different countries worldwide, so I decided to use JHU Github [https://github.com/CSSEGISandData/COVID-19/archive/master.zip] to download daily data.\nThe script below selects daily confirmed, death and recovered COVID-19 cases, downloads it and compresses them. Finally it extracts the relevant indicators (confirmed, death and recovered cases) as .csv files.\n\nDownloadCOVIDData &lt;- function() {\n  # Create data directory if doesn't exist\n  if (!dir.exists(\"data\")) {\n    dir.create(\"data\")\n  }\n  # Download master.zip file\n  download.file(\n    url = \"https://github.com/CSSEGISandData/COVID-19/archive/master.zip\",\n    destfile = \"data/covid19JH.zip\"\n  )\n  data_path &lt;- \"COVID-19-master/csse_covid_19_data/csse_covid_19_time_series/\"\n\n  # Unzip covid19JH.zip file to extract .csv metric files (confirmed, deaths, recovered)\n  # time_series_covid19_confirmed_global.csv, time_series_covid19_deaths_global.csv,\n  # time_series_covid19_recovered_global.csv\n  unzip(\n    zipfile = \"data/covid19JH.zip\",\n    files = paste0(data_path, c(\n      \"time_series_covid19_confirmed_global.csv\",\n      \"time_series_covid19_deaths_global.csv\",\n      \"time_series_covid19_recovered_global.csv\"\n    )),\n    exdir = \"data\",\n    junkpaths = T\n  )\n}\n\nDownloadCOVIDData()\n\nThen I had to update that initial download every half an hour, in case the file was updated throughout the day. In order to get the most up to date info, I thought of running it I though about this script to be run on a server or VM seven days a week, so it will periodically check to get the most up to date information.\n\nDataupdate &lt;- function() {\n  T_refresh &lt;- 0.5 # hours\n  if (!dir_exists(\"data\")) {\n    dir.create(\"data\")\n    DownloadCOVIDData()\n  } else if ((!file.exists(\"data/covid19JH.zip\")) || as.double(Sys.time() - file_info(\"data/covid19JH.zip\")$change_time, units = \"hours\") &gt; T_refresh) {\n    # If the latest refresh exceeds 30 minutes, then you download it again\n    DownloadCOVIDData()\n  }\n}\n\nDataupdate()\n\nOnce the data was downloaded, I did some cleansing and data transformations from wide to long format, and also included new calculations with popularization data extracted from the World Bank API to create each of the indicators as rates per 10,000 population, using seven days rolling average to obtain an average of those daily indicators.\n2. Create new metrics from raw covid data\nAfter downloading the original data files, I extracted and assigned names to the three metrics I will use in the dashboard (data_confirmed,data_deceased,data_recovered).\n\ninput_covid &lt;- list.files(\"data/\", \".csv\")\n\nNFILES &lt;- length(input_covid)\nfile_Name &lt;- c(\"data_confirmed\", \"data_deceased\", \"data_recovered\", \"WDI_indicators\")\n\nfor (i in 1:NFILES) {\n  assign(\n    paste0(file_Name[i]), # Read and store data frames\n    read_csv(paste0(\n      \"data/\",\n      input_covid[i]\n    ))\n  )\n}\n\n3. Reshape data for plots\nOriginally, the data is created in wide format and I transformed it into long format, including some calculations. I also aggregated it to Country level and applied relevant date format to display time series data and animations using a timeline in maps.\nFor the purpose of this blog post, I will only describe this process for one metric COVID19 Confirmed cases, the code for remaining two metrics (deceased cases, recovered cases) can be found in my Github repo.\n\n# Confirmed cases \nlibrary(tidyr)\n\nnames(data_confirmed)\n# First rename the two first columns using rename() function\nconfirmed_tidy &lt;- data_confirmed %&gt;%\n  rename(\n    Province = \"Province/State\",\n    Country = \"Country/Region\"\n  ) %&gt;%\n  pivot_longer(\n    names_to = \"date\",\n    cols = 5:ncol(data_confirmed)\n  ) %&gt;%\n  group_by(Province, Country, Lat, Long, date) %&gt;%\n  summarise(\"Confirmed\" = sum(value, na.rm = T)) %&gt;%\n  mutate(date = as.Date(date, \"%m/%d/%y\"))\n\n4. Stack all COVID19 and Lat Long metrics into a master file\nThe final metrics set is made of recovered and death COVID19 cases, by country and by date. Countries and dates are displayed in rows and metrics in columns.The original data also includes two columns for latitude and longitude used later to produce a map using Leaflet package.\n\n# Now we merge them together\nMAPDATA &lt;- confirmed_tidy %&gt;%\n  full_join(deceased_tidy)\n\nMAPDATAF &lt;- MAPDATA %&gt;%\n  # Now we merge them together\n  MAPDATA() &lt;- confirmed_tidy %&gt;%\n  full_join(deceased_tidy)\n\nMAPDATAF &lt;- MAPDATA %&gt;%\n  full_join(recovered_tidy) %&gt;%\n  arrange(Province, Country, date) %&gt;%\n  # Recode NA values into 0\n  mutate(\n    Confirmed = ifelse(is.na(Confirmed), 0, Confirmed),\n    Deaths = ifelse(is.na(Deaths), 0, Deaths),\n    Recovered = ifelse(is.na(Recovered), 0, Recovered)\n  )\n\nAlong the process of building the final data set, I will produce several csv files to validate each data step.\n\nfile_pathCHK &lt;-('C://Pablo UK//43 R projects 2021//04 My Shiny app//04 Mycovid19 app//CHECKS/')\nFile_name &lt;-'/MAPDATAF.csv' \nwrite.csv(MAPDATAF,paste0(file_pathCHK,File_name),row.names = T)\n\nIt is important to ensure any missing value is left in the file to ensure Leaflet maps works properly.\n\nMAPDATAG &lt;- MAPDATAF %&gt;% mutate(\n  Confirmed = ifelse(is.na(Confirmed), 0, Confirmed),\n  Deaths = ifelse(is.na(Deaths), 0, Deaths),\n  Recovered = ifelse(is.na(Recovered), 0, Recovered)\n)\n\nMAPDATAH &lt;- MAPDATAG %&gt;%\n  pivot_longer(\n    names_to = \"Metric\",\n    cols = c(\"Confirmed\", \"Deaths\", \"Recovered\")\n  ) %&gt;%\n  ungroup()\n\nPLOT_LEAFLET_MAPS &lt;- MAPDATAH %&gt;%\n  pivot_wider(names_from = Metric, values_from = c(value))\n\n5. Final data wrangling output files\nThe data wrangling step outputs two files required for the Shiny app: the first one contains COVID metrics plus Lat Long variable for Leaflet maps and the second includes COVID metrics to be merged with country population figures.\n5.1 COVID metrics set including Lat Long variables for Leaflet maps\nThe next step is to create a new data set fo be used in the map on the first tab. It will contain metric variables and Latitude and Longitude variables. The map will display using pop-up circles as tool tips the number of cases per country, and it will be animated using a timeline below the map.\n\nMAPDATAH &lt;- MAPDATAG %&gt;%\n  pivot_longer(\n    names_to = \"Metric\",\n    cols = c(\"Confirmed\", \"Deaths\", \"Recovered\")\n  ) %&gt;%\n  ungroup()\n\nDATAMAP &lt;- MAPDATAH\nPLOT_LEAFLET &lt;- DATAMAP %&gt;%\n  pivot_wider(names_from = Metric, values_from = c(value))\n\nPLOT_LEAFLET_MAPS &lt;- PLOT_LEAFLET\n\nsave.image(\"C:/Pablo UK/43 R projects 2021/04 My Shiny app/04 Mycovid19 app/PLOT LEAFLET MAPS.RData\")\n\nThese are the set of metrics created for the map tab:\n\n# This file weill be use to comparae COVID19 rates across different countries\nPLOT_LEAFLET2_conf &lt;- PLOT_LEAFLET_MAPS %&gt;%\n  select(Country, date, Confirmed) %&gt;%\n  group_by(Country, date) %&gt;%\n  summarise(\"Confirmed\" = sum(Confirmed, na.rm = T))\n\nPLOT_LEAFLET2_death &lt;- PLOT_LEAFLET_MAPS %&gt;%\n  select(Country, date, Deaths) %&gt;%\n  group_by(Country, date) %&gt;%\n  summarise(\"Death\" = sum(Deaths, na.rm = T))\n\nPLOT_LEAFLET2_Recov &lt;- PLOT_LEAFLET_MAPS %&gt;%\n  select(Country, date, Recovered) %&gt;%\n  group_by(Country, date) %&gt;%\n  summarise(\"Recovered\" = sum(Recovered, na.rm = T))\n\n# Join together\nPLOT_LEAFLET_RATES &lt;- PLOT_LEAFLET2_conf %&gt;%\n  full_join(PLOT_LEAFLET2_death) %&gt;%\n  arrange(Country, date)\n\nPLOT_LEAFLET_RATES &lt;- PLOT_LEAFLET_RATES %&gt;%\n  full_join(PLOT_LEAFLET2_Recov) %&gt;%\n  arrange(Country, date)\n\nPLOT_LEAFLET_CDR_NUM &lt;- PLOT_LEAFLET_RATES\n\nsave.image(\"C:/Pablo UK/43 R projects 2021/04 My Shiny app/04 Mycovid19 app/PLOT LEAFLET CDR NUM.RData\")\n\nOur final set to compute COVID19 population rates is displayed below:\n5.2 COVID19 population rates\nI want to include population figures to obtain 10,000 population rates for each metric (cases, recovered,deaths covid19 cases) I first created new variables for those rates and then I merged the population figures in using the world Bank API. The aim of this section is to download population figures required to compute the relevant metric rates *10,000 population from World bank Development Indicators API. All the details from this script can be found in the Github repository.\nJust to highlight three main tasks included in this population-rates sub-section:\na). The use of Source() function to bring another script that pulls 2019 countries population data from WDI API The aim of this first section is to download population figures from the set of World Development Indicators provided by the World Bank data API.\n\n# # Include population figures   \nsource(\"UI/ui_get_population_figures.R\", local = TRUE)\n\nAfter downloading the requested data by using the World Bank’s API, the resulting XMS file is formatted in long country-year format.\nb). Cleaning WDI population data to match COVID19 country names list This is performed by the script called “ui_get_population_figures.R” located in the Shiny folder structure within a specific folder for UI scripts Using a which statement, it accounts for country name mismatches between COVID and WDI data sources, so population data matches COVID19 metrics.\n\n# LOAD population figures in the right way\n# Input missing values\nPOP_POPULATED &lt;- POP_DATA_2019\n\nCNpop &lt;- c(\n  \"Bahamas, The\", \"Brunei Darussalam\", \"Congo, Dem. Rep.\", \"Congo, Rep.\", \"Egypt, Arab Rep.\", \"Gambia, The\", \"Iran, Islamic Rep.\", \"Korea, Rep.\",\n  \"Kyrgyz Republic\", \"Micronesia, Fed. Sts.\", \"Russian Federation\", \"St. Kitts and Nevis\", \"St. Lucia\", \"St. Vincent and the Grenadines\",\n  \"Slovak Republic\", \"Syrian Arab Republic\", \"United States\", \"Venezuela, RB\", \"Yemen, Rep.\"\n)\n\nlength(CNpop)\n\nCNindic &lt;- c(\n  \"Bahamas\", \"Brunei\", \"Congo (Brazzaville)\", \"Congo (Kinshasa)\", \"Egypt\", \"Gambia\", \"Iran\", \"Korea, South\",\n  \"Kyrgyzstan\", \"Micronesia\", \"Russia\", \"Saint Kitts and Nevis\", \"Saint Lucia\", \"Saint Vincent and the Grenadines\",\n  \"Slovakia\", \"Syria\", \"US\", \"Venezuela\", \"Yemen\"\n)\nlength(CNindic)\n\n# Then we replace those values\nPOP_POPULATED[which(POP_POPULATED$country %in% CNpop), \"country\"] &lt;- CNindic\n\n6. Compute X10,000 population rates for selected metrics\nTwo preliminary calculations are needed before rates for COVID19 confirmed, recovered and death cases are calculated:\nFirst the JHU cases data is based on cumulative data, so previous day is subtracted to obtain the daily count of events for each metrics\nCompute now rates based on daily figures. Get those rates as 7 previous days rolling average to smooth daily fluctuations. Finally a rounding calculation is done on calculated rates to avoid any decimal places\nThere are a couple of calculations needed before rates for COVID19 confirmed, recovered and death cases are calculated:\nFirst the JHU cases is based on cumulative data, so previous day is subtracted to obtain the daily count of events for each metrics.\n\nPOPG_RATES &lt;- POPG %&gt;%\n  arrange(Country, date) %&gt;%\n  mutate(\n    ConD = Confirmed - lag(Confirmed, n = 1),\n    RecD = Recovered - lag(Recovered, n = 1),\n    DeathD = Death - lag(Death, n = 1)\n  )\ntail(POPG_RATES)\n\nCompute now rates based on daily figures.\n\nPOPG_RATESF &lt;- POPG_RATES %&gt;%\n  select(Country, date, year, population, ConD, RecD, DeathD) %&gt;%\n  mutate(\n    CONFR = ceiling(((ConD / population) * 10000)),\n    RECR = ceiling(((RecD / population) * 10000)),\n    DEATHR = ceiling(((DeathD / population) * 10000))\n  )\n\ntail(POPG_RATESF)\n\nGet those rates as 7 previous days rolling average to smooth daily fluctuations.\n\nlibrary(zoo)\n\nRATES7DGAVG &lt;- POPRATESG %&gt;%\n  group_by(Country) %&gt;%\n  select(date, Country, population, ConD, RecD, DeathD) %&gt;%\n  mutate(\n    CONF_ma07 = rollmean(ConD, k = 7, fill = NA),\n    REC_ma07 = rollmean(RecD, k = 7, fill = NA),\n    DEATH_ma07 = rollmean(DeathD, k = 7, fill = NA)\n  )\n\nFinally there is a round done on calculated taxes to avoid any decimal places.\n\nPOP_POPULATEDT &lt;- POP_POPULATED_RENAME %&gt;%\n  mutate(\n    Confirmed_10000 = round(Confirmed_Rate, digits = 0),\n    Recovered_10000 = round(Recovered_Rate, digits = 0),\n    Deaths_10000 = round(Death_Rate, digits = 0)\n  )\n\nPOP_POPULATED &lt;- POP_POPULATEDT %&gt;%\n  select(\n    date, Country, Population, Confirmed, Recovered, Death,\n    Conf_7D_10000 = Confirmed_10000,\n    Rec_7D_10000 = Recovered_10000,\n    Death_7D_10000 = Deaths_10000\n  )\n\nPOP_POPULATED\n\n6.1 Data set with rates ready for Shiny app\nThis is the final data set that goes into the shiny app\n\nhead(POP_POPULATED)\n\n7. Building the Shiny dashboard\nOnce that all the data is ready to build the Shiny dashboard, there where three main tabs that I wanted to display on the dashboard:\nThe script for the dashboard can be quite long, and it is available in the Github repository at the end of this blog article. As a general design choice, I opted for a standard Sidebar layout, using fluidrow and column functions to arrange the plots layout on each tab.\nThis is an example of the functions used to populate the infoBoxes on top of the dashboard.\nThe reactive components used in the plots and the maps to produce several animations were created using a input\\$Time_Slider function.\nui &lt;- dashboardPage(\n  \n  dashboardHeader(title = \"COVID-19\"),\n  # This Sidebar menu allows us to include new items on the sidebar\n  dashboardSidebar(\n                    sidebarMenu(\n                    # Setting id makes input$tabs give the tabName of currently-selected tab\n                    id = \"tabs\",\n                    menuItem(\"About\", tabName = \"about\", icon = icon(\"desktop\")),\n                    menuItem(\"Map\", tabName = \"map\", icon = icon(\"map\")),\n                    menuItem(\"Plots\", tabName = \"plot\", icon = icon(\"wifi\")),\n                    menuItem(\"Forecast\", tabName = \"forecast\", icon = icon(\"chart-line\")))\n  )\n  ,\n  dashboardBody(  # Infobox: Total figures KPI world\nThis is an example of the functions used to populate the infoBoxes on top of the dashboard.\ndashboardBody(  # Infobox: Total figures KPI world\n    fluidRow(infoBoxOutput(\"Total_cases_WORLD\", width = 3),\n             infoBoxOutput(\"Total_recovered_WORLD\", width = 3),\n             infoBoxOutput(\"Total_deaths_WORLD\", width = 3),\n             infoBoxOutput(\"Date\", width = 3)\n             ),\nThe reactive components used in the plots and the maps to produce several animations were created using a input\\$Time_Slider function.\n7.1. Interactive map with KPI and timeline\n\nPop-up and tool tips display COVID19 Total, recovered and death cases\nCircles radius are proportional to number of cases per country\nDynamic animation: Map changes as data varies in time\n\nMap tab\n7.2. Interactive line charts using {plotly} library\n\nKPI number of cases and day to day percent change\nDrop down menu to filter for specific countries\nLine chart cases by country, selected by drop down menu\nTop 10 country rates *10,000 cases\nInteractive plots to Zoom in and Zoom out using {plotly} library to display Top 10 country rates *10,000 cases\n\n7.3. In development\n\nMy intention is to include a new tab in coming weeks to include a predictive modelling tool using some modelling tool such as tidy models or modeltime, just to test it. In the long run I would like to learn how to implement hierarchical Bayesian modelling into some dashboard.\nAlso I would like to include a specific .CSS file in YAML section of the shiny dashboard to fine tuning the format applied on each tab.\n\nModeltime\n8. Source code available in Github\nAs this blog article was a brief description of the Shiny app I’ve designed, please follow the link below to get the source code from Github: 00 Maps data prep_SHINY_APP.R, 01 Leaf and pop figures_SHINY_APP.R, 02 ui_server_SHINY_APP.R.\nSharing this script and connecting with other NHSR analysts will be good starting point to lean how to code consistently and also to follow a specific NHSR coding style when using R.\nAny comments to this blog article, please feel free to email me at: pablo.leonrodenas@nhs.net\nThis blog has been edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-workshop-development-and-validation-of-clinical-prediction-models-using-r.html",
    "href": "blog/nhs-r-workshop-development-and-validation-of-clinical-prediction-models-using-r.html",
    "title": "NHS-R Workshop: Development and validation of clinical prediction models using R",
    "section": "",
    "text": "Dr Faisal provided a very comprehensive introduction to Clinical Prediction Modelling (CPM for short), focusing on the five stages of developing and validating this type of model in R:\n\nModel development.\nPerformance assessment using discrimination and calibration measures.\nInternal validation using bootstrapping.\nExternal validation.\nSensitivity analysis and decision curve analysis (measuring clinical impact).\n\nOne of the first things we covered is how CPM is different to causal inference modelling: CPM focuses only on making accurate predictions, not understanding the cause behind the effects. This means that one should be careful in choosing the appropriate type of model for the task and that the two types of modelling should not be assessed by the same criteria.\nAn important part of the model development phase is deciding whether a model is even needed at all! According to the research presented, many models are developed, but very few are useful. A systematic review from early 2020 concluded that only 4 out of the 731 models they analysed had a low risk of bias. We were helpfully provided a flow chart to help us decide whether a new model is needed.\n\n\n\n\n\nA lot of emphasis was put on model validation and the various internal and external approaches to validation that one can take.\n\n\n\n\n\nParticipants worked on four tasks of increasing difficulty throughout the workshop, although both the objectives and methods were well explained. By the end of the workshop, we had visualised data, trained and validated a model, and even plotted a variety of performance indicators (including the ROC AUC, calibration, and Decision Curve Analysis).\nThis was a great introduction to Clinical Predictive Modelling in R, and I hope to attend any future workshops provided by NHS-R!\nThanks NHS-R for agreeing to have some of their slides shared here.\nEduard Incze\nNHS Wales Delivery Unit - Advanced Analyst/Modeller\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html",
    "href": "blog/showcasing-functions-separate.html",
    "title": "Showcasing a function - separate()\n",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course.\n\ndata &lt;- tibble::tribble(\n    ~Patient,          ~Codes,\n  \"PatientA\", \"A01, A02, A03\",\n  \"PatientB\", \"B01; B02; B03\",\n  \"PatientC\", \"C01; C03\",\n  \"PatientD\", \"D01. D02. D03\"\n  )"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html#create-data",
    "href": "blog/showcasing-functions-separate.html#create-data",
    "title": "Showcasing a function - separate()\n",
    "section": "",
    "text": "This was written originally in an Excel spreadsheet and used {datapasta} to copy into R as code to build the same data frame. {datapasta} can be access through RStudio as an Addin as well as code. Find out more about {datapasta} from the Introduction to R and R Studio course.\n\ndata &lt;- tibble::tribble(\n    ~Patient,          ~Codes,\n  \"PatientA\", \"A01, A02, A03\",\n  \"PatientB\", \"B01; B02; B03\",\n  \"PatientC\", \"C01; C03\",\n  \"PatientD\", \"D01. D02. D03\"\n  )"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html#separate-codes-by-position",
    "href": "blog/showcasing-functions-separate.html#separate-codes-by-position",
    "title": "Showcasing a function - separate()\n",
    "section": "Separate codes by position",
    "text": "Separate codes by position\nSeparate into columns in the order data appears\n\nlibrary(tidyverse)\n\ndata |&gt; \n  tidyr::separate(Codes, c(\"col1\", \"col2\", \"col3\"))\n\n# A tibble: 4 × 4\n  Patient  col1  col2  col3 \n  &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n1 PatientA A01   A02   A03  \n2 PatientB B01   B02   B03  \n3 PatientC C01   C03   &lt;NA&gt; \n4 PatientD D01   D02   D03  \n\n\nhttps://tidyr.tidyverse.org/reference/separate.html"
  },
  {
    "objectID": "blog/showcasing-functions-separate.html#add-a-pivot",
    "href": "blog/showcasing-functions-separate.html#add-a-pivot",
    "title": "Showcasing a function - separate()\n",
    "section": "Add a pivot",
    "text": "Add a pivot\nTo move wide data to longer:\n\ndata |&gt; \n  tidyr::separate(Codes, c(\"col1\", \"col2\", \"col3\")) |&gt; \n  tidyr::pivot_longer(cols = c(starts_with(\"col\")),\n               names_to = \"type\")\n\n# A tibble: 12 × 3\n   Patient  type  value\n   &lt;chr&gt;    &lt;chr&gt; &lt;chr&gt;\n 1 PatientA col1  A01  \n 2 PatientA col2  A02  \n 3 PatientA col3  A03  \n 4 PatientB col1  B01  \n 5 PatientB col2  B02  \n 6 PatientB col3  B03  \n 7 PatientC col1  C01  \n 8 PatientC col2  C03  \n 9 PatientC col3  &lt;NA&gt; \n10 PatientD col1  D01  \n11 PatientD col2  D02  \n12 PatientD col3  D03"
  },
  {
    "objectID": "blog/fuzzy_joining_tables.html",
    "href": "blog/fuzzy_joining_tables.html",
    "title": "Fuzzy joining tables using string distance methods",
    "section": "",
    "text": "I recently had a problem where I had two datasets containing data which I needed to join together. The two datasets had a nice 1:1 mapping between them, but unfortunately there was not a nice coded identifier to join the two datasets. There was just a name field, and annoyingly there were subtle differences between the two.\nFor demonstration purposes, I’m going to show a similar problem. Imagine that we have one dataset that contains data about ICSs, and another about STPs. (For those not familiar with English NHS geographies, STPs were 42 geographic areas covering England, which in July 2022 became ICSs). This has a 1:1 mapping, but some of the names changed slightly when ICSs came into effect.\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(stringdist)\nlibrary(here)\nlibrary(sf)\nlibrary(janitor)\nlibrary(igraph)\nIf you want to follow along, download the list of STPs and ICBs from the ONS Geoportal site.\nstps &lt;- readxl::read_excel(paste0(here::here(), \"/STP_APR_2021_EN_NC.xlsx\")) |&gt;\n  select(code = STP21CDH, description = STP21NM) |&gt;\n  arrange(code)\n\nstps\n\n# A tibble: 42 × 2\n   code  description                           \n   &lt;chr&gt; &lt;chr&gt;                                 \n 1 QE1   Healthier Lancashire and South Cumbria\n 2 QF7   South Yorkshire and Bassetlaw         \n 3 QGH   Herefordshire and Worcestershire      \n 4 QH8   Mid and South Essex                   \n 5 QHG   Bedfordshire, Luton and Milton Keynes \n 6 QHL   Birmingham and Solihull               \n 7 QHM   Cumbria and North East                \n 8 QJ2   Joined Up Care Derbyshire             \n 9 QJG   Suffolk and North East Essex          \n10 QJK   Devon                                 \n# ℹ 32 more rows\nicbs &lt;- readxl::read_excel(paste0(here::here(), \"/ICB_JUL_2022_EN_NC.xlsx\")) |&gt;\n  select(code = ICB22CDH, description = ICB22NM) |&gt;\n  arrange(code)\n\nicbs\n\n# A tibble: 42 × 2\n   code  description                                                    \n   &lt;chr&gt; &lt;chr&gt;                                                          \n 1 QE1   NHS Lancashire and South Cumbria Integrated Care Board         \n 2 QF7   NHS South Yorkshire Integrated Care Board                      \n 3 QGH   NHS Herefordshire and Worcestershire Integrated Care Board     \n 4 QH8   NHS Mid and South Essex Integrated Care Board                  \n 5 QHG   NHS Bedfordshire, Luton and Milton Keynes Integrated Care Board\n 6 QHL   NHS Birmingham and Solihull Integrated Care Board              \n 7 QHM   NHS North East and North Cumbria Integrated Care Board         \n 8 QJ2   NHS Derby and Derbyshire Integrated Care Board                 \n 9 QJG   NHS Suffolk and North East Essex Integrated Care Board         \n10 QJK   NHS Devon Integrated Care Board                                \n# ℹ 32 more rows\nObviously, here we have the “E54* ONS codes which we could join on, a luxury I did not have. I’ve left these in to test the matching does work later.\nFirst of all, how many rows are we able to match joining on the name?\nicbs |&gt;\n  inner_join(stps, by = \"description\")\n\n# A tibble: 0 × 3\n# ℹ 3 variables: code.x &lt;chr&gt;, description &lt;chr&gt;, code.y &lt;chr&gt;\nNone! Looking at the icbs dataset, we can see rows start with “NHS” and end with “Integrated Care Board”, which doesn’t happen in stps. Perhaps, by just stripping these we get a perfect match?\nicbs |&gt;\n  select(description) |&gt;\n  mutate(across(description, str_remove_all, \"^NHS | Integrated Care Board$\")) |&gt;\n  inner_join(stps |&gt; select(description), by = \"description\")\n\n# A tibble: 20 × 1\n   description                                        \n   &lt;chr&gt;                                              \n 1 Herefordshire and Worcestershire                   \n 2 Mid and South Essex                                \n 3 Bedfordshire, Luton and Milton Keynes              \n 4 Birmingham and Solihull                            \n 5 Suffolk and North East Essex                       \n 6 Devon                                              \n 7 Lincolnshire                                       \n 8 Leicester, Leicestershire and Rutland              \n 9 Kent and Medway                                    \n10 Hertfordshire and West Essex                       \n11 Bath and North East Somerset, Swindon and Wiltshire\n12 Northamptonshire                                   \n13 Gloucestershire                                    \n14 Somerset                                           \n15 Buckinghamshire, Oxfordshire and Berkshire West    \n16 Cambridgeshire and Peterborough                    \n17 Bristol, North Somerset and South Gloucestershire  \n18 Dorset                                             \n19 Coventry and Warwickshire                          \n20 Cheshire and Merseyside\nRoughly half… not good enough!"
  },
  {
    "objectID": "blog/fuzzy_joining_tables.html#string-distance-methods-to-the-rescue",
    "href": "blog/fuzzy_joining_tables.html#string-distance-methods-to-the-rescue",
    "title": "Fuzzy joining tables using string distance methods",
    "section": "String distance methods to the rescue?",
    "text": "String distance methods to the rescue?\nMany of us will have had to compare strings at some point, perhaps using LIKE in Sql, or Regular Expressions (regexs) in R. But there are a class of algorithms that can calculate the “distance” or “similarity” between two strings.\nConsider the two words “grey” and “gray”. How similar are these two words? The Hamming Distance algorithm compares two strings of equal length, and returns a number indicating how many positions are different in the string. So for our two words above, we get a distance of 1.\nA generally more useful method though is the Damerau-Levenshtein distance. This calculates the number of operations to make the first string equal the second string.\nOperations in this context are single-character insertions, deletions or substitutions, or transposition of two adjacent characters.\nAlternatively, we could consider the set of unique words used in two strings. We can count the intersection of words (words common to both strings) and divide by the count of all the words used to give us a value between 0 and 1. A value of 0 would indicate that the two strings are completely different, and a value of 1 would indicate that the two strings are very similar. This method is called the Jaccard Similarity.\nThis is a very useful method for the problem I faced, as I expect the names in both datasets to be free of spelling mistakes."
  },
  {
    "objectID": "blog/fuzzy_joining_tables.html#using-the-jaccard-similarity-method",
    "href": "blog/fuzzy_joining_tables.html#using-the-jaccard-similarity-method",
    "title": "Fuzzy joining tables using string distance methods",
    "section": "Using the Jaccard Similarity method",
    "text": "Using the Jaccard Similarity method\nFirst, we can use the stringsimmatrix() function from the stringdist package to calculate the Jaccard Similarity matrix, comparing the names from the first table to the names from the second table.\n\ndist_matrix &lt;- stringdist::stringsimmatrix(\n  icbs$description,\n  stps$description,\n  \"jaccard\"\n)\n\nHowever, simply calculating the string distance matrix doesn’t give us a solution to the problem. In the table below, you can see that in column y, some rows appear more than once, and eyeballing the match it’s clear it hasn’t found the correct pair.\n\n# we can find the index of the maximum\ntibble(\n  x = icbs$description |&gt; str_remove_all(\"^NHS | Integrated Care Board$\"),\n  y = stps$description[apply(dist_matrix, 1, which.max)]\n) |&gt;\n  group_by(y) |&gt;\n  arrange(y) |&gt;\n  filter(n() &gt; 1)\n\n# A tibble: 20 × 2\n# Groups:   y [5]\n   x                                                 y                          \n   &lt;chr&gt;                                             &lt;chr&gt;                      \n 1 Gloucestershire                                   Bristol, North Somerset an…\n 2 Bristol, North Somerset and South Gloucestershire Bristol, North Somerset an…\n 3 Shropshire, Telford and Wrekin                    Hampshire and the Isle of …\n 4 Hampshire and Isle of Wight                       Hampshire and the Isle of …\n 5 Lancashire and South Cumbria                      Healthier Lancashire and S…\n 6 Leicester, Leicestershire and Rutland             Healthier Lancashire and S…\n 7 Lincolnshire                                      North London Partners in H…\n 8 North East London                                 North London Partners in H…\n 9 North Central London                              North London Partners in H…\n10 Birmingham and Solihull                           Nottingham and Nottinghams…\n11 Derby and Derbyshire                              Nottingham and Nottinghams…\n12 Devon                                             Nottingham and Nottinghams…\n13 Sussex                                            Nottingham and Nottinghams…\n14 Humber and North Yorkshire                        Nottingham and Nottinghams…\n15 Northamptonshire                                  Nottingham and Nottinghams…\n16 Somerset                                          Nottingham and Nottinghams…\n17 Nottingham and Nottinghamshire                    Nottingham and Nottinghams…\n18 Dorset                                            Nottingham and Nottinghams…\n19 Surrey Heartlands                                 Nottingham and Nottinghams…\n20 Cheshire and Merseyside                           Nottingham and Nottinghams…"
  },
  {
    "objectID": "blog/fuzzy_joining_tables.html#graph-theory-saves-the-day",
    "href": "blog/fuzzy_joining_tables.html#graph-theory-saves-the-day",
    "title": "Fuzzy joining tables using string distance methods",
    "section": "Graph theory saves the day",
    "text": "Graph theory saves the day\nThere is a quick solution to this though using a Bipartite Graph. A Birpartite Graph is a type of network where we have vertices of two types, and edges only exist between nodes of the different types.\nWe can use the igraph package to construct and manipulate graphs. First, let’s construct a table where we have names from the first table as nodes of one type, and the names from the second table as nodes of the other type.\n\n# the column `name` is special in a named graph. it will uniquely identify each vertex.\nvertices &lt;- dplyr::bind_rows(\n  .id = \"type\",\n  icbs = icbs |&gt; mutate(name = paste0(\"icb_\", code)),\n  stps = stps |&gt; mutate(name = paste0(\"stp_\", code))\n) |&gt;\n  dplyr::relocate(name, .before = dplyr::everything()) |&gt;\n  # the \"type\" column needs to be a logical vector, so we use TRUE for the first type, and FALSE for the second\n  dplyr::mutate(dplyr::across(\"type\", ~ .x == \"icbs\"))\n\nvertices\n\n# A tibble: 84 × 4\n   name    type  code  description                                              \n   &lt;chr&gt;   &lt;lgl&gt; &lt;chr&gt; &lt;chr&gt;                                                    \n 1 icb_QE1 TRUE  QE1   NHS Lancashire and South Cumbria Integrated Care Board   \n 2 icb_QF7 TRUE  QF7   NHS South Yorkshire Integrated Care Board                \n 3 icb_QGH TRUE  QGH   NHS Herefordshire and Worcestershire Integrated Care Boa…\n 4 icb_QH8 TRUE  QH8   NHS Mid and South Essex Integrated Care Board            \n 5 icb_QHG TRUE  QHG   NHS Bedfordshire, Luton and Milton Keynes Integrated Car…\n 6 icb_QHL TRUE  QHL   NHS Birmingham and Solihull Integrated Care Board        \n 7 icb_QHM TRUE  QHM   NHS North East and North Cumbria Integrated Care Board   \n 8 icb_QJ2 TRUE  QJ2   NHS Derby and Derbyshire Integrated Care Board           \n 9 icb_QJG TRUE  QJG   NHS Suffolk and North East Essex Integrated Care Board   \n10 icb_QJK TRUE  QJK   NHS Devon Integrated Care Board                          \n# ℹ 74 more rows\n\n\nThen create weighted edges between each pair of names, using the distance matrix we calculated above.\n\nedges &lt;- dist_matrix |&gt;\n  # this will convert our matrix into a list of lists\n  purrr::array_branch(1) |&gt;\n  # the lists will be in the same order as our original data\n  # so we can use purrr to change into a dataframe\n  purrr::set_names(icbs$code) |&gt;\n  purrr::map_dfr(\n    .id = \"to\",\n    \\(.x) tibble::tibble(from = icbs$code, weight = .x)\n  ) |&gt;\n  mutate(\n    across(to, ~ paste0(\"icb_\", .x)),\n    across(from, ~ paste0(\"stp_\", .x))\n  )\n\nedges\n\n# A tibble: 1,764 × 3\n   to      from    weight\n   &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n 1 icb_QE1 stp_QE1  0.792\n 2 icb_QE1 stp_QF7  0.519\n 3 icb_QE1 stp_QGH  0.52 \n 4 icb_QE1 stp_QH8  0.462\n 5 icb_QE1 stp_QHG  0.483\n 6 icb_QE1 stp_QHL  0.542\n 7 icb_QE1 stp_QHM  0.625\n 8 icb_QE1 stp_QJ2  0.429\n 9 icb_QE1 stp_QJG  0.464\n10 icb_QE1 stp_QJK  0.12 \n# ℹ 1,754 more rows\n\n\nThis tibble gives us the string similarities between each pair of names from our two tables.\nNow that we have our edges and vertices, we can construct a graph, and find the maximum bipartite matching. This works without much effort as we constructed our vertices with a type logical column, and we constructed our edges with a weight numeric column. igraph handles the rest for us.\n\ng &lt;- igraph::graph_from_data_frame(edges, TRUE, vertices)\n\nm &lt;- max_bipartite_match(g)$matching |&gt;\n  enframe(\"icb_code\", \"stp_code\") |&gt;\n  # the results gives us results from icb22cdh-&gt;icb22cd and vice versa\n  # keep just the icb22cdh-&gt;icb22cd results\n  filter(icb_code |&gt; str_starts(\"icb_\")) |&gt;\n  mutate(across(c(icb_code, stp_code), str_remove_all, \"^.{3}_\"))\n\nm |&gt;\n  filter(icb_code == stp_code) |&gt;\n  rename(code = icb_code) |&gt;\n  select(-stp_code) |&gt;\n  inner_join(\n    icbs |&gt; rename(icb_name = description),\n    by = \"code\"\n  ) |&gt;\n  inner_join(\n    stps |&gt; rename(stp_name = description),\n    by = \"code\"\n  ) |&gt;\n  mutate(across(everything(), str_trunc, 30)) |&gt;\n  print(n = 42)\n\n# A tibble: 42 × 3\n   code  icb_name                       stp_name                      \n   &lt;chr&gt; &lt;chr&gt;                          &lt;chr&gt;                         \n 1 QE1   NHS Lancashire and South Cu... Healthier Lancashire and So...\n 2 QF7   NHS South Yorkshire Integra... South Yorkshire and Bassetlaw \n 3 QGH   NHS Herefordshire and Worce... Herefordshire and Worcester...\n 4 QH8   NHS Mid and South Essex Int... Mid and South Essex           \n 5 QHG   NHS Bedfordshire, Luton and... Bedfordshire, Luton and Mil...\n 6 QHL   NHS Birmingham and Solihull... Birmingham and Solihull       \n 7 QHM   NHS North East and North Cu... Cumbria and North East        \n 8 QJ2   NHS Derby and Derbyshire In... Joined Up Care Derbyshire     \n 9 QJG   NHS Suffolk and North East ... Suffolk and North East Essex  \n10 QJK   NHS Devon Integrated Care B... Devon                         \n11 QJM   NHS Lincolnshire Integrated... Lincolnshire                  \n12 QK1   NHS Leicester, Leicestershi... Leicester, Leicestershire a...\n13 QKK   NHS South East London Integ... Our Healthier South East Lo...\n14 QKS   NHS Kent and Medway Integra... Kent and Medway               \n15 QM7   NHS Hertfordshire and West ... Hertfordshire and West Essex  \n16 QMF   NHS North East London Integ... East London Health and Care...\n17 QMJ   NHS North Central London In... North London Partners in He...\n18 QMM   NHS Norfolk and Waveney Int... Norfolk and Waveney Health ...\n19 QNC   NHS Staffordshire and Stoke... Staffordshire and Stoke on ...\n20 QNQ   NHS Frimley Integrated Care... Frimley Health and Care ICS   \n21 QNX   NHS Sussex Integrated Care ... Sussex Health and Care Part...\n22 QOC   NHS Shropshire, Telford and... Shropshire and Telford and ...\n23 QOP   NHS Greater Manchester Inte... Greater Manchester Health a...\n24 QOQ   NHS Humber and North Yorksh... Humber, Coast and Vale        \n25 QOX   NHS Bath and North East Som... Bath and North East Somerse...\n26 QPM   NHS Northamptonshire Integr... Northamptonshire              \n27 QR1   NHS Gloucestershire Integra... Gloucestershire               \n28 QRL   NHS Hampshire and Isle of W... Hampshire and the Isle of W...\n29 QRV   NHS North West London Integ... North West London Health an...\n30 QSL   NHS Somerset Integrated Car... Somerset                      \n31 QT1   NHS Nottingham and Nottingh... Nottingham and Nottinghamsh...\n32 QT6   NHS Cornwall and the Isles ... Cornwall and the Isles of S...\n33 QU9   NHS Buckinghamshire, Oxford... Buckinghamshire, Oxfordshir...\n34 QUA   NHS Black Country Integrate... The Black Country and West ...\n35 QUE   NHS Cambridgeshire and Pete... Cambridgeshire and Peterbor...\n36 QUY   NHS Bristol, North Somerset... Bristol, North Somerset and...\n37 QVV   NHS Dorset Integrated Care ... Dorset                        \n38 QWE   NHS South West London Integ... South West London Health an...\n39 QWO   NHS West Yorkshire Integrat... West Yorkshire and Harrogat...\n40 QWU   NHS Coventry and Warwickshi... Coventry and Warwickshire     \n41 QXU   NHS Surrey Heartlands Integ... Surrey Heartlands Health an...\n42 QYG   NHS Cheshire and Merseyside... Cheshire and Merseyside       \n\n\nThis gives us a perfect match!"
  },
  {
    "objectID": "blog/fuzzy_joining_tables.html#how-does-this-work",
    "href": "blog/fuzzy_joining_tables.html#how-does-this-work",
    "title": "Fuzzy joining tables using string distance methods",
    "section": "How does this work?",
    "text": "How does this work?\nRoughly, the way this matching algorithm works is it starts off and finds the edge with the greatest possible matching score, and pairs those two nodes together. It then removes those nodes (and edges to/from those nodes) from the graph, and repeats until all nodes are paired, or no edges remain.\nThis prevents the issue we initially saw, because a node can only be paired to one other node.\nThis algorithm works when we have a good set of weights to the edges. In fact, if you try running the string similarity function with some of the different algorithms that are available, such as the Levenshtein Distance, most give us bipartite matchings that aren’t correct.\nFor a more complete description, see the help page for the function (igraph::max_bipartite_match), and the Push-relabel maximum flow algorithm."
  },
  {
    "objectID": "blog/fuzzy_joining_tables.html#final-thoughts",
    "href": "blog/fuzzy_joining_tables.html#final-thoughts",
    "title": "Fuzzy joining tables using string distance methods",
    "section": "Final thoughts",
    "text": "Final thoughts\nHopefully this has been interesting to you and introduced some new and interesting techniques to play with. Both string-distance algorithms and graph theory are very powerful tools that crop up again and again in computer science, so are worth diving into if you are curious!\nThere is an obvious question of, are there easier approaches to this problem? In this case, we only had 42 options, which is probably quick enough to solve by hand in Excel by starting with two sorted columns and manually lining up the correct rows.\nHowever, if you had a similar problem with more options then the manual approach would quickly becoming tiring. It is worth noting that you should not blindly trust the results; In my original problem I scanned the results and confirmed that I got the results I was after. In this example we had the codes which allowed us to confirm the correct results\nI also came into this problem expecting there to be a perfect 1:1 mapping between both sets. If it isn’t guaranteed that constraint holds in your problem then you may need to treat the results more cautiously.\nThis post is also available as a quarto document."
  },
  {
    "objectID": "blog/welcome-to-leeds.html",
    "href": "blog/welcome-to-leeds.html",
    "title": "Welcome to Leeds",
    "section": "",
    "text": "Why create a group?\nTrying to learn R is tricky, there are so many ways to do things that I often ended up Googling everything and still not feeling like I was doing anything the right way.\nThen someone sent me a link to the NHS-R Community website where I found lots of interesting and helpful information. Plus there were R groups, but not one in Leeds. I was surprised by this given the density of health analysts and other R users in and around Leeds. There should definitely be a group in Leeds.\n\n\nHow to create a group?\nSo I ran my idea past a colleague to check if I was crazy. They agreed to help anyway and offered to email some contacts at the University of Leeds. I approached the Sheffield-R group and the NHS-R community (who also thought it was a good idea). We also connected with NHS Digital and NHS England and Improvement.\nThrough this process we were very lucky to find some clever individuals who had already thought about starting up a group in Leeds. This meant the group already had a website (R Users Leeds), Twitter, email and logo. This gave us a great starting point with a planning team covering Leeds Teaching hospitals NHS Trust, NHS Digital, University of Leeds, Leeds Beckett University and Sky Betting and Gaming. We are very lucky in Leeds to have access to both local and national NHS organisations, academic institutions and digital and technology companies.\n\n\n\nWhat do we want from a group?\nFrom the beginning it was important to think about the aims and ethos of the group which for Leeds are:\n\nCreate an open and inclusive community\nMeetings should be free\nOrganise meetings within and outside office hours to ensure everyone has the opportunity to participate.\nInclude the NHS, academia and other industries using R\n\n\n\nWhere will the group meet?\nAnother consideration was how to get venues especially at low or no cost. After making contacts I was assured that I could get things for free, I wasn’t too sure but thought that I should give it a go. This is going to be an ongoing task but I have been surprised by how many organisations want to help. Following a first planning meeting we managed to organise speakers and get a date and venue for the first meeting:\n5th December 2019, NHS Digital, Leeds\n\nReproducible wRiting with RMarkdown. Ed Berry, Senior Data Scientist, Sky Betting & Gaming\nEfficient WoRkflows getting more done with R. Robin Lovelace, University Academic Fellow, University of Leeds\nPutting the R into Reproducible Research. Anna Krystalli, Research Software Engineer, University of Sheffield\n\n\n\nR we nearly there yet ?\nOur first event is almost upon us so I suppose we have nearly reached the first destination. With tickets booking up quickly it is clear to see that there is a need for a group in Leeds to bring the analytical community together.Thank you to everyone who has supported us so far, hopefully this is just the beginning of the journey.\nIf you would like to speak at R Users Leeds please get in touch.\nYou can keep in touch with us on Twitter, GitHub and email.\nhttps://r-users-leeds.netlify.app/\nThis blog was written by Louise Hick, Real World Data Analyst, the Leeds Teaching Hospitals NHS Trust\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/we-need-you-share-your-covid-19-work-on-nhse-is-regular-mini-huddle-series.html",
    "href": "blog/we-need-you-share-your-covid-19-work-on-nhse-is-regular-mini-huddle-series.html",
    "title": "We need you: share your COVID-19 work on NHSE/I’s regular mini-huddle series",
    "section": "",
    "text": "COVID-19 has placed the value of data and analytics at the fore. This has meant an unprecedented focus on the work of health and care analysts, but also huge demand for local analytical services. The need to share and share alike is paramount. To support this, NHSE/I are holding a series of huddles and mini-huddles to allow analysts to share their COVID-19 work with a wider community.\n\n\n\n\n\n\nFutureNHS\n\n\n\n\n\nThe link to the FutureNHS recordings no longer works.\nThis is a platform that anyone can join but some workspaces are restricted in access https://future.nhs.uk/ and the DataAnalyticsCovid19 workspace redirects to AnalystX which may be open to general registration.\n\n\n\nWeekly huddles showcase work on a specific topic, and the back catalogue of recordings can be found on FutureNHS. The mini-huddles are a spin off series where speakers get into the detail behind a single topic, like geo-mapping or a dashboard. The team curating these huddles are keen to have a mini-huddle on COVID-19 analysis in R. If you are interested in sharing your analysis relating to COVID-19 using R, please get in touch with Sophie Hodges via email sophie.hodges5@nhs.net\nIf you haven’t already, please sign up to the COVID-19 data and analytics workspace here. The community is currently 6500-strong with members across the NHS, public health, local authorities, charities and commercial organisations. Mini-huddles usually draw an audience between 30 and 80 attendees so it’s a great opportunity to share your work, discuss findings and network with others working on similar issues.\nPlease get in touch if this sounds of interest. And a huge thank you from NHSE/I. The work being done by the analytical community is a key plank of the COVID-19 effort.\nSophie Hodges, NHS England and NHS Improvement - Project Manager – Graduate Analyst\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/publish-on-github.html",
    "href": "blog/publish-on-github.html",
    "title": "Publish on GitHub",
    "section": "",
    "text": "I had a question on this directly on how to publish and then another one popped up on the NHS-R Community Slack so, in the principle of DRY (Don’t Repeat Yourself) this is a short blog on how to publish (free!) slides, websites and reports through GitHub.\nWe publish all the NHS-R Community course slides through GitHub and the following refers to the Introduction to Quarto slides.\n\nSetting up on GitHub\nPublishing is through the Settings tab which is hidden unless you have rights to the repository.\nThere will be a Pages tab along the side which is where the settings can be changed https://docs.github.com/en/pages/getting-started-with-github-pages/creating-a-github-pages-site.\nThe NHS-R Community course repositories publish through the “deploy from a branch” setting with main and root on the branch but note that these have a redirect on the pages to the NHS-R web url so is called nhsrcommunity.com. Pages that are published through GitHub usually have github.io in the url name.\n\n\nFor Quarto files\nIf you are publishing a series of slides, as the format is often for NHS-R Community courses, if you add a blank file to the folder and call it _quarto.yml it acts like an engine to all the slides so you don’t have to repeat code. An example is https://github.com/nhs-r-community/intro-quarto/blob/main/_quarto.yml where the author is coded once here but appears on all the slides in this project folder. This isn’t needed for publishing but can make your code a bit less cluttered if you have many files to publish and means you only have to make changes in one place.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "",
    "text": "Article originally appeared on What a NHS-R Community Conference it was – simply wow! – Hutsons-hacks and was submitted by Gary Hutson – Senior Data Scientist.\nThe NHS-R Virtual Conference concluded this week and we have had a number of excellent speakers from the Health & Social Care sector, alongside working with key partners of interest. It kicked off with a full day jammed packed of speakers from all over the NHS. The second day we had international speakers from the US, Australia and other European countries.\nThe keynotes this year were provided by Professor Frank Harrell who did an excellent talk on “statistical mistakes to avoid” and Julia Silge @RStudio discussing “Preparing and processing text for Machine Learning”. I was lucky enough to chair the afternoon sessions, and it was great welcoming Julia to the stage and running the Q&A."
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#what-if-i-missed-it",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#what-if-i-missed-it",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "What if I missed it?",
    "text": "What if I missed it?\nNo worries if you missed the days, as we livestreamed it on YouTube for posterity."
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#day-one-watch-again",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#day-one-watch-again",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "Day One – Watch again",
    "text": "Day One – Watch again\n\n\nThere were so many talks to attend on this day and the quality was excellent, here are a few of the talks that stood out for me:\n\n09:30 – Professor Mohammed A Mohammed – NHS-R Conference Opening and Welcome\n11:45 – Colin Fay – ThinkR – discussing the role of accessibility in R\n14:25 – Sebastien Peytrignet – What can an online shopping algorithm teach us about coordinating outpatient care?\nI didn’t see all of day one, so this is just a provisional list"
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#day-two-watch-again",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#day-two-watch-again",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "Day Two – Watch again",
    "text": "Day Two – Watch again\n\n\nAnother stellar day of talks, but here if my selection:\n\n11:10 – Jacob Anhoej – discussing “Run Forest, run! Understanding variation and runs analysis\n12:00 – Simon and Chris discussing the new NHSRplotthedots package for statistical process control\n12:30 – Simon Moss and Richard Wood – the CCG use of PathSimR to support Covid-19 mass vaccination\n15:05 – Frank Harrell – Professor of Biostatistics talking about statistical mistakes to avoid\n16:05 – Jeroen Ooms – speaking about the r-universe project"
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#day-three-watch-again",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#day-three-watch-again",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "Day Three – watch again",
    "text": "Day Three – watch again\n\n\nI missed the morning, but I have watched back since and these are my picks, again there is too much content to wade through:\n\n09:30 – Gary Hutson – NHS-R Solutions and package funding process, with code! I also ran a workshop at the event. Watch this space for live recordings being uploaded to the NHS-R YouTube page.\n10:05 – Sukhmeet Panesar – connecting the data and analytic workforce and creating a social movement for good: the story of AnalystX\n11:05 Hugo Cosh and Zoe Strawbridge – Shiny doesn’t have to be scary. Discussing flexdashboard and how it is used in Public Health Wales\n11:25 – Dr Kate Bamford, Senior Data Surveillance Scientist, East Midlands Health Protection Team UK Health Security Agency Using R Markdown, Reactable and Crosstalk to create an interactive COVID-19 review tool\n12:45 – Adam Watkins – mapping public transport travel times – building on and enabling work of others\n15:05 – All the lightning talks were fantastic!\n16:00 – Julia Silge from RStudio talking about creating features for machine learning from text\n17:00 – Close by Chris Beeley – our co-chair of the NHS-R community"
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#workshops",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#workshops",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "Workshops",
    "text": "Workshops\nThe NHS-R community also held workshops to support understanding of our packages. These will all be streamed on YouTube and included sessions on Shiny, Tensorflow, TidyModels, NHSDataDictionaRy package, functional programming in R, alongside many others. Please keep an eye out for the NHS-R communities YouTube channell and website."
  },
  {
    "objectID": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#thanks-and-conclusion",
    "href": "blog/what-a-nhs-r-community-conference-it-was-simply-wow.html#thanks-and-conclusion",
    "title": "What a NHS-R Community Conference it was – simply wow!",
    "section": "Thanks and conclusion",
    "text": "Thanks and conclusion\nThanks to everyone involved in making the conference a success, and to those out there in the NHS-R community, please make a pledge:\nPlease pledge @NHSRCommunity on Twitter when you are making your pledge. It is time to say goodbye to the conference. It has been a rollercoaster this year and we will see you all again in 2022, as well as having webinars throughout the year.\nThe blog has been edited for NHS-R Style"
  },
  {
    "objectID": "blog/apha-blog-august-2023.html",
    "href": "blog/apha-blog-august-2023.html",
    "title": "AphA blog – August 2023",
    "section": "",
    "text": "Links are out now for the DataConnect23 and there is a lot to sign up to in the free week long event! It’s organised by the UK Government’s Central Digital and Data Office along with the Government Data Quality Hub and is open to public sector (NHS and Local Authorities) as well as Government.\nI’ll be doing a workshop promoting NHS-R Community and the work we do and people can get involved with so do sign up!"
  },
  {
    "objectID": "blog/apha-blog-august-2023.html#join-the-slack-groups",
    "href": "blog/apha-blog-august-2023.html#join-the-slack-groups",
    "title": "AphA blog – August 2023",
    "section": "Join the Slack groups",
    "text": "Join the Slack groups\nDetails are in the Open Analytics Resources and feel free to contact NHS-R Community via email at nhs.rcommunity@nhs.net"
  },
  {
    "objectID": "blog/moving-on-with-the-plan.html",
    "href": "blog/moving-on-with-the-plan.html",
    "title": "Moving on with the plan… And using R to do it! My personal experience…",
    "section": "",
    "text": "When Mohammed asked if I would be interested in doing a blog on how “we”, Information and Analytics in NHS Improvement, have been using R / R Studio I was a little apprehensive as I have never ‘blogged’ before, but I thought I would give it a try!\nSo here goes —- my first ever blog!\nMany of us will remember Garry Fothergill’s engaging ‘lightning talk’ at the NHS-R Conference back in October last year ‘So what’s your plan’. Garry gave us a synopsis of how NHS Improvement had been using R Studio to support the annual process (torture to some) of activity planning for 2018/19. The original concept, ignited by Paul Stroner, arose from a central frustration of ‘flat line’ unrealistic activity plans of the past. I am sure some of us have been guilty of that, I know I have on occasion in the past!\nSince the original piece of R work, the team have been looking at further developments to the approach that Garry and Paul had set the wheels in motion on, with particular reference to how it could be used to support the 2019/20 planning round more formally. Back in mid-October it was agreed that both NHS Improvement and NHS England would use the univariate modelling approach that Paul and Garry had been championing.\nAs part of this process the R code was reviewed and rewritten with some changes to methodologies in terms of validation processes (out of sample testing), applying adjustments for working and calendar days as well as models applied. The final R code was tested / Q&A’d by some of our esteemed NHS-R Community colleges and the overall approach was signed off by NHS Improvement’s Community of Practice for Statistics.\nAs part of our offer to support CCGs and Acute Providers, a specific organisational level R code was developed (the code we used centrally – pulled in all 148 Acute providers for over 15 activity lines, based on day weighted and unweighted historical data, so you can imagine the number of models). The R code has been widely shared with organisations on request but also posted on Future NHS site and is also available on our newly created Github account.\nI personally can’t take credit for this R code, we are lucky in the Information and Analytics team that we have a colleague who has extensive R programming background …… if I could physically plug into his R brain I would! It is this expertise (and Chris Beeley’s workshop at the NHS-R event) that have opened my eyes to the art of the possible in using R Shiny Apps. This has led us down the path of designing and creating an R Shiny App, which allows organisations to replicate their forecasted figures that have been centrally provided within the activity planning submission template over the internet. This tool can be used for any monthly data, all you need to do is make sure you have the upload data structured correctly, there is user help functionality included with the App – just click on the link below.\nhttps://nhsiadvancedanalytics.shinyapps.io/Forecasting_Support/ - Link no longer works\nI’m only at the start of my R journey, but I can already see the benefits of using it daily to support all aspects my reignited analytical life, so I’m excited about what the future holds! It’s a positive sign and a step in the right direction when these software programmes are being talked about by our NHS leaders, but what I am most enthused about is the will and the want to work collaboratively and share learning on all things R without judgement across organisations both internal and external to the NHS. So, I’m fully signed up to spreading the R message by being an active participant in any local R Working groups, presenting on R possibilities at different networking events whilst working as hard as I can on improving my own R skills. Watch this space, I may even take the leap and do some more blogs about my ‘R’ journey!\nThis blog was written by Vicki Cruze, Principal Analyst in the Performance Analysis Team at NHS England and NHS Improvement.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/histogram-with-auto-binning-in-ggplot2.html",
    "href": "blog/histogram-with-auto-binning-in-ggplot2.html",
    "title": "Histogram with auto binning in ggplot2",
    "section": "",
    "text": "# 20240224 Added for qmd to run\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nHistograms (with auto binning)\nAgain, we will use the mtcars dataset and use the fields in that to produce the chart, as we are doing this there is nothing to do on the data preparation side. That leaves us to have fun with the plot.\nBuilding the Histogram with auto binning\nI set up the plot, as per below:\n\nlibrary(ggplot2)\ntheme_set(theme_classic())\n\nI import the ggplot2 library and set my chart theme to a classic theme. The process next is to create the histogram plot and feed in the relevant data:\n\nplot &lt;- ggplot(mpg, aes(displ)) + scale_fill_brewer(palette = \"Blues\")\n\nI create a plot placeholder in memory so I can reuse this plot again and again in memory. This sets the aes layer equal to the displacement metric in the mtcars data frame. I then use the scale_fill_brewer command and select the palette to the Blues palette. A list of palettes can be found on the R Graph Gallery.\n\nThe next section uses the geom_histogram() geometry to force this to be a histogram:\n\nplot + geom_histogram(aes(fill=class),\n                      binwidth = .1,\n                      col=\"black\",\n                      size=.1) +\n  labs(title=\"Histogram with Auto Binning\",\n       caption=\"Produced by Gary Hutson\") + xlab(\"Displacement\")\n\nThe histogram uses the class of vehicle as the histogram fill, the binwidth is the width of the bins required, the colour is equal to black and the size is stipulated here. All that I then do is add the data labels to it and you have a lovely looking histogram built. This can be applied to any dataset. The output is as below:\n\n\n\n\n\n\n\n\nSpecifying binning values\nThe script can be simply changed in the histogram layer by adding the bins parameter:\n\nplot + geom_histogram(aes(fill=class),\n                   bins=5,\n                   col=\"black\",\n                   size=.1) +\n  labs(title=\"Histogram with Auto Binning\",\n       caption=\"Produced by Gary Hutson\") + xlab(\"Displacement\")\n\n\n\n\n\n\n\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutsons-Hacks.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/can-we-rely-on-synthetic-data-to-overcome-data-governance-issue-in-healthcare.html",
    "href": "blog/can-we-rely-on-synthetic-data-to-overcome-data-governance-issue-in-healthcare.html",
    "title": "Can we rely on synthetic data to overcome data governance issue in healthcare?",
    "section": "",
    "text": "I recently came across a term synthetic data. I start wondering what does it mean? I found that it is different from the dummy data, but in what ways and how is it different, I began to wonder?\nI become curious to find out more about it, as nowadays it is difficult to get hold of healthcare data (that is., NHS). The most prominent issues seem to link to data governance and access, as this information is personal sensitive data.\nI investigated methods for creating ‘synthetic data’ as a tool that might help to develop better prediction models, as data could be available for a much larger pool of people, who can tackle these data governance and other challenging healthcare issues.\nWhat is Synthetic data?\nThe goal is to generate a data set which contains no real units, therefore safe for public release and retains the structure of the data.\nIn other words, one can say that synthetic data contains all the characteristics of original data minus the sensitive content.\nSynthetic data is generally made to validate mathematical models. This data is used to compare the behaviour of the real data against the one generated by the model.\nHow we generate synthetic data?\nThe principle is to observe real-world statistic distributions from the original data and reproduce fake data by drawing simple numbers.\nConsider a data set with p variables. In a nutshell, synthesis follows these steps:\n\nTake a simple random sample of x1,obs and set as x1,syn\nFit model f(x2,obs|x1,obs) and draw x2,syn from f(x2,syn|x1,syn)\nFit model f(x3,obs|x1,obs , x2,obs ) and draw x3,syn  from f(x3,syn|x1,syn , x2,syn )\nAnd so on, until f(xp,syn|x1,syn , x2,syn , … , xp-1,syn)\n\nFitting statistical models to the original data and generating completely new records for public release.\nJoint distribution f(x1, x2, x3, …, xp) is approximated by a set of conditional distributions f(x2|x1).\nFor instance, we have the following original (real) data.\n\ntibble::tribble(\n      ~sex,                   ~edu, ~age, ~depress,\n  \"FEMALE\",   \"VOCATIONAL/GRAMMAR\",  57L,       6L,\n    \"MALE\",   \"VOCATIONAL/GRAMMAR\",  20L,       0L,\n  \"FEMALE\",   \"VOCATIONAL/GRAMMAR\",  18L,       0L,\n  \"FEMALE\", \"PRIMARY/NO EDUCATION\",  78L,      16L,\n  \"FEMALE\",   \"VOCATIONAL/GRAMMAR\",  54L,       4L,\n    \"MALE\",            \"SECONDARY\",  20L,       5L,\n  \"FEMALE\",            \"SECONDARY\",  39L,       2L,\n    \"MALE\",            \"SECONDARY\",  39L,       4L,\n  \"FEMALE\",            \"SECONDARY\",  43L,       0L,\n  \"FEMALE\",            \"SECONDARY\",  63L,       6L\n  )\n\n# A tibble: 10 × 4\n   sex    edu                    age depress\n   &lt;chr&gt;  &lt;chr&gt;                &lt;int&gt;   &lt;int&gt;\n 1 FEMALE VOCATIONAL/GRAMMAR      57       6\n 2 MALE   VOCATIONAL/GRAMMAR      20       0\n 3 FEMALE VOCATIONAL/GRAMMAR      18       0\n 4 FEMALE PRIMARY/NO EDUCATION    78      16\n 5 FEMALE VOCATIONAL/GRAMMAR      54       4\n 6 MALE   SECONDARY               20       5\n 7 FEMALE SECONDARY               39       2\n 8 MALE   SECONDARY               39       4\n 9 FEMALE SECONDARY               43       0\n10 FEMALE SECONDARY               63       6\n\n\nWe can generate synthetic data using the algorithm described above.\n\ntibble::tribble(\n      ~sex,                       ~edu, ~age, ~depress,\n    \"MALE\",     \"PRIMARY/NO EDUCATION\",  81L,      11L,\n  \"FEMALE\",                \"SECONDARY\",  75L,       9L,\n  \"FEMALE\",       \"VOCATIONAL/GRAMMAR\",  43L,       6L,\n  \"FEMALE\",       \"VOCATIONAL/GRAMMAR\",  65L,       3L,\n    \"MALE\", \"POST-SECONDARY OR HIGHER\",  17L,       3L,\n    \"MALE\",                \"SECONDARY\",  39L,       3L,\n    \"MALE\",                \"SECONDARY\",  35L,       1L,\n  \"FEMALE\",       \"VOCATIONAL/GRAMMAR\",  35L,       2L,\n    \"MALE\", \"POST-SECONDARY OR HIGHER\",  38L,       0L,\n    \"MALE\",       \"VOCATIONAL/GRAMMAR\",  25L,       0L\n  )\n\n# A tibble: 10 × 4\n   sex    edu                        age depress\n   &lt;chr&gt;  &lt;chr&gt;                    &lt;int&gt;   &lt;int&gt;\n 1 MALE   PRIMARY/NO EDUCATION        81      11\n 2 FEMALE SECONDARY                   75       9\n 3 FEMALE VOCATIONAL/GRAMMAR          43       6\n 4 FEMALE VOCATIONAL/GRAMMAR          65       3\n 5 MALE   POST-SECONDARY OR HIGHER    17       3\n 6 MALE   SECONDARY                   39       3\n 7 MALE   SECONDARY                   35       1\n 8 FEMALE VOCATIONAL/GRAMMAR          35       2\n 9 MALE   POST-SECONDARY OR HIGHER    38       0\n10 MALE   VOCATIONAL/GRAMMAR          25       0\n\n\nWe can compare the distribution of original data with synthetic data as follows:\n\n\nThese charts were created using the shiny app\n\nNational early warning score (NEWS) example in R:\n\nlibrary(NHSRdatasets)\nlibrary(dplyr)\n\ndf &lt;- NHSRdatasets::synthetic_news_data\n\ndf |&gt; \n  slice_head(n = 10)\n\n# A tibble: 10 × 12\n    male   age  NEWS  syst  dias  temp pulse  resp   sat   sup alert  died\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1     0    68     3   150    98  36.8    78    26    96     0     0     0\n 2     1    94     1   145    67  35      62    18    96     0     0     0\n 3     0    85     0   169    69  36.2    54    18    96     0     0     0\n 4     1    44     0   154   106  36.9    80    17    96     0     0     0\n 5     0    77     1   122    67  36.4    62    20    95     0     0     0\n 6     0    58     1   146   106  35.3    73    20    98     0     0     0\n 7     0    25     4    65    42  35.6    72    12    99     0     0     0\n 8     0    69     0   116    56  37.2    90    16    97     0     0     0\n 9     0    91     1   162    72  35.5    60    16    99     0     0     0\n10     0    70     1   132    96  35.3    67    16    97     0     0     0\n\n\n\nsummary(df)\n\n      male            age              NEWS             syst      \n Min.   :0.000   Min.   : 17.00   Min.   : 0.000   Min.   : 65.0  \n 1st Qu.:0.000   1st Qu.: 60.00   1st Qu.: 1.000   1st Qu.:118.0  \n Median :0.000   Median : 74.00   Median : 2.000   Median :134.0  \n Mean   :0.476   Mean   : 69.65   Mean   : 2.444   Mean   :135.7  \n 3rd Qu.:1.000   3rd Qu.: 84.00   3rd Qu.: 4.000   3rd Qu.:150.0  \n Max.   :1.000   Max.   :102.00   Max.   :12.000   Max.   :220.0  \n      dias             temp           pulse            resp      \n Min.   : 17.00   Min.   :33.10   Min.   : 40.0   Min.   :10.00  \n 1st Qu.: 63.00   1st Qu.:35.80   1st Qu.: 70.0   1st Qu.:16.00  \n Median : 74.00   Median :36.20   Median : 84.0   Median :18.00  \n Mean   : 74.63   Mean   :36.31   Mean   : 85.8   Mean   :18.39  \n 3rd Qu.: 84.00   3rd Qu.:36.70   3rd Qu.: 98.0   3rd Qu.:20.00  \n Max.   :124.00   Max.   :40.20   Max.   :200.0   Max.   :43.00  \n      sat              sup            alert            died     \n Min.   : 82.00   Min.   :0.000   Min.   :0.000   Min.   :0.00  \n 1st Qu.: 95.00   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.00  \n Median : 97.00   Median :0.000   Median :0.000   Median :0.00  \n Mean   : 96.38   Mean   :0.123   Mean   :0.071   Mean   :0.07  \n 3rd Qu.: 98.00   3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:0.00  \n Max.   :100.00   Max.   :1.000   Max.   :3.000   Max.   :1.00  \n\n\nGenerate the synthetic NEWS data using synthpop R package\n\nlibrary(synthpop)\n\nsyn_df &lt;- syn(df, seed = 4321)\n\nWarning: In your synthesis there are numeric variables with 5 or fewer levels: male, sup, alert, died.\nConsider changing them to factors. You can do it using parameter 'minnumlevels'.\n\nSynthesis\n-----------\n male age NEWS syst dias temp pulse resp sat sup\n alert died\n\n# synthetic data\nsyn_df$syn[1:10,]\n\n   male age NEWS syst dias temp pulse resp sat sup alert died\n1     1  56    1  126   84 35.7    72   17  98   0     0    0\n2     1  50    2  115   84 36.8    94   14  97   0     0    0\n3     0  74    6  143   86 36.5    82   21  93   0     0    0\n4     1  56    1  122   60 36.3    94   12  98   0     0    0\n5     1  52    0  153   89 36.2    78   12  96   0     0    0\n6     0  21    2  164   92 35.5    97   20  99   0     0    0\n7     0  37    1  101   57 35.6    76   15  98   0     0    0\n8     1  81    2  125   74 36.6    71   17  97   0     0    0\n9     1  67    5  182  103 37.1    95   18  94   1     0    0\n10    1  67    0  160   80 36.2    86   18  98   0     0    0\n\n\n\nsummary(syn_df$syn) \n\n      male           age              NEWS             syst      \n Min.   :0.00   Min.   : 17.00   Min.   : 0.000   Min.   : 65.0  \n 1st Qu.:0.00   1st Qu.: 60.00   1st Qu.: 1.000   1st Qu.:118.0  \n Median :0.00   Median : 74.00   Median : 1.000   Median :135.0  \n Mean   :0.47   Mean   : 69.99   Mean   : 2.414   Mean   :136.2  \n 3rd Qu.:1.00   3rd Qu.: 84.00   3rd Qu.: 4.000   3rd Qu.:150.2  \n Max.   :1.00   Max.   :102.00   Max.   :11.000   Max.   :219.0  \n      dias            temp           pulse             resp      \n Min.   : 17.0   Min.   :33.10   Min.   : 43.00   Min.   :12.00  \n 1st Qu.: 63.0   1st Qu.:35.80   1st Qu.: 70.00   1st Qu.:16.00  \n Median : 74.0   Median :36.20   Median : 83.00   Median :18.00  \n Mean   : 74.6   Mean   :36.26   Mean   : 85.04   Mean   :18.57  \n 3rd Qu.: 84.0   3rd Qu.:36.70   3rd Qu.: 97.00   3rd Qu.:20.00  \n Max.   :124.0   Max.   :40.20   Max.   :200.00   Max.   :43.00  \n      sat              sup            alert            died      \n Min.   : 82.00   Min.   :0.000   Min.   :0.000   Min.   :0.000  \n 1st Qu.: 95.00   1st Qu.:0.000   1st Qu.:0.000   1st Qu.:0.000  \n Median : 97.00   Median :0.000   Median :0.000   Median :0.000  \n Mean   : 96.45   Mean   :0.125   Mean   :0.059   Mean   :0.062  \n 3rd Qu.: 98.00   3rd Qu.:0.000   3rd Qu.:0.000   3rd Qu.:0.000  \n Max.   :100.00   Max.   :1.000   Max.   :3.000   Max.   :1.000  \n\n\n\nwrite.csv(\"synthetic_news_data.csv\")\n\n## \"\",\"x\"\n## \"1\",\"synthetic_news_data.csv\"\n\nFor more discussion about {synthpop} R package http://gradientdescending.com/generating-synthetic-data-sets-with-synthpop-in-r/\nSummary\nIn many ways, synthetic data reflects George Box’s observation that “all models are wrong, but some are useful” while providing a “useful approximation [of] those found in the real world,”\nThe connection between the clinical outcomes of a patient visits and costs rarely exist in practice, so being able to assess these trade-offs in synthetic data allow for measurement and enhancement of the value of care – cost divided by outcomes.\nSynthetic data is likely not a 100% accurate depiction of real-world outcomes, like cost and clinical quality, but rather a useful approximation of these variables. Moreover, synthetic data is constantly improving, and methods like validation and calibration will continue to make these data sources more realistic.\nBesides synthetic data used to protect the privacy and confidentiality of set of data, it can be used for testing fraud detection systems by creating realistic behaviour profiles for users and attackers. In machine learning, it can also be used to train and test models. The synthetic data can aid in creating a baseline for future testing or studies such as clinical trial studies.\nDr Muhammad Faisal and Gary Hutson\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/even-simpler-sql.html",
    "href": "blog/even-simpler-sql.html",
    "title": "Even Simpler SQL",
    "section": "",
    "text": "I’ve had some feedback on the last post, and rather than repeat the same thing multiple times, I’m going all @drob, and writing this instead.\nWhen I tweeted out the link to my post I gave it the tag line “why I’d rather write dplyr than SQL”.\nWhat I couldn’t fit in to the tweet was that this was based on the caveat that some of the SQL I have had to write has been incredibly complicated by the age / version / lack of functionality of the SQL database I was using, and the nature of the task at hand.\nIn those situations, being able to write dplyr to manipulate my data would have made my life a lot easier.\nHowever, I am not against SQL.\nFar from it, I love working with SQL and writing complex queries.\nThe more you learn, the more you understand what can be done with SQL, and it’s incredibly powerful.\nBut – there are definitely times when you think, “this would be a lot easier in R”."
  },
  {
    "objectID": "blog/even-simpler-sql.html#sql-is-great-and-you-should-definitely-learn-it",
    "href": "blog/even-simpler-sql.html#sql-is-great-and-you-should-definitely-learn-it",
    "title": "Even Simpler SQL",
    "section": "SQL is great, and you should definitely learn it",
    "text": "SQL is great, and you should definitely learn it\nAND"
  },
  {
    "objectID": "blog/even-simpler-sql.html#dplyr-is-great-and-you-should-definitely-learn-it.",
    "href": "blog/even-simpler-sql.html#dplyr-is-great-and-you-should-definitely-learn-it.",
    "title": "Even Simpler SQL",
    "section": "Dplyr is great, and you should definitely learn it.",
    "text": "Dplyr is great, and you should definitely learn it.\nThen you can decide which is best for the situation you are currently facing. In real life you wouldn’t need a hugely powerful database to wrangle 684 rows, and my main reason for using {dplyr} was that it was a small dataset and the resultant table was going to be assigned to {ggplot2} for plotting purposes."
  },
  {
    "objectID": "videos.html",
    "href": "videos.html",
    "title": "Videos",
    "section": "",
    "text": "To view our recordings of workshops, webinars, past conferences and more, visit our YouTube Channel\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#reports",
    "href": "index.html#reports",
    "title": "NHS-R Community Quarto website",
    "section": "Reports",
    "text": "Reports\n\n\n\n\n\n\n\n\n\n\nNHS-R 2025 Questionnaire Analysis\n\n\nThis report describes the analysis of the results of a community-wide NHS-R online questionnaire which was active in Spring 2025.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#blog",
    "href": "index.html#blog",
    "title": "NHS-R Community Quarto website",
    "section": "Blog",
    "text": "Blog\n\n\n\n\n\n\n\n\n\n\nPredictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions\n\n\n\n\n\n\nJoseph Mosley\n\n\nJun 14, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe importance of community in NHS data science\n\n\n\n\n\n\nClaire Welsh\n\n\nMar 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredictive Analytics within healthcare - Random Forest models for predicting length of stay\n\n\n\n\n\n\nJoseph Mosley\n\n\nFeb 27, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#books",
    "href": "index.html#books",
    "title": "NHS-R Community Quarto website",
    "section": "Books",
    "text": "Books\n\n\n\n\n\n\n\n\n\n\nHealth Inequalities\n\n\nThis project is a collection of information and knowledge related to analytical work on “Health Inequalities”, as performed in the NHS and the wider UK Health and Social…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNHS-R Way book\n\n\nEverything you need or want to know about NHS-R Community including: how to contribute and get involved, code styles, training preparation materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Analytics Resources\n\n\nUseful links for health and care analysts and data scientists.\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#supporters",
    "href": "index.html#supporters",
    "title": "NHS-R Community Quarto website",
    "section": "Supporters",
    "text": "Supporters"
  },
  {
    "objectID": "conference.html",
    "href": "conference.html",
    "title": "Conference",
    "section": "",
    "text": "We are pleased to announce that the RPYSOC 2025 conference will be held at the Wellcome Trust (215 Euston Road, London), offering both in-person and virtual attendance options!\nConference dates:\n\nThursday 13th November 2025\nFriday 14th November 2025\n\nRegistration to open shortly – so watch this space!\nTo submit an abstract, please complete this form.\nPlease note that the abstract submission deadline is Friday 5th September 2025. We would be honoured to incorporate your work into the conference.\nThank you for being part of the community during a time of change — we appreciate your engagement!\n\n\n\n Back to top"
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "NHS-R Community Quarto website",
    "section": "",
    "text": "Email: nhs.rcommunity@nhs.net\nGitHub: @nhs-r-community\nLinkedIn: NHS-R Community\nFosstodon (Mastodon): @NHSrCommunity@fosstodon.org\nBluesky: nhsrcommunity.bsky.social\nSlack: postcard.nhsrcommunity.com\n\nFor details on how NHS-R Community use social media can be found in the NHS-R Way book."
  },
  {
    "objectID": "contact.html#contact-us",
    "href": "contact.html#contact-us",
    "title": "NHS-R Community Quarto website",
    "section": "",
    "text": "Email: nhs.rcommunity@nhs.net\nGitHub: @nhs-r-community\nLinkedIn: NHS-R Community\nFosstodon (Mastodon): @NHSrCommunity@fosstodon.org\nBluesky: nhsrcommunity.bsky.social\nSlack: postcard.nhsrcommunity.com\n\nFor details on how NHS-R Community use social media can be found in the NHS-R Way book."
  },
  {
    "objectID": "accessibility.html",
    "href": "accessibility.html",
    "title": "Accessibility commitment",
    "section": "",
    "text": "Return to home page or about page.\nInspired from Silvia Canelón’s commitment to accessibility NHS-R Community welcomes feedback on the accessibility of this site and any links to its materials including code and training materials. We have used the Web Accessibility Evaluation Tool to manually check for accessibility errors and corrected where possible."
  },
  {
    "objectID": "accessibility.html#website-practices",
    "href": "accessibility.html#website-practices",
    "title": "Accessibility commitment",
    "section": "Website practices",
    "text": "Website practices\nThis site has been designed with the following features in mind:\n\nThe colour palette is from NHS branding.\nFonts are set to Arial which is an NHS font.\nAlternative text is used for all images.\n\nAny learning will be used to improve the site and will also be shared with the community more widely through book resources like our Open Analytics page where we have started to collect links that support greater Accessibility."
  },
  {
    "objectID": "accessibility.html#search-button",
    "href": "accessibility.html#search-button",
    "title": "Accessibility commitment",
    "section": "Search button",
    "text": "Search button\nThis is generated automatically through rendering a Quarto website and appears as an empty button.\nThere is an open issue on Quarto’s GitHub."
  },
  {
    "objectID": "accessibility.html#default-thumbnail-image-for-blogs---no-alternative-text",
    "href": "accessibility.html#default-thumbnail-image-for-blogs---no-alternative-text",
    "title": "Accessibility commitment",
    "section": "Default thumbnail image for blogs - no alternative text",
    "text": "Default thumbnail image for blogs - no alternative text\nThe thumbnails for default images in blogs do not have alternative text, this particularly affects the blogs brought across from the original WordPress site as new blogs have specific images added. In code this relates to the image-placeholder:.\nThere is an open issue on Quarto’s GitHub and an issue on the website GitHub."
  },
  {
    "objectID": "accessibility.html#empty-and-redundant-links",
    "href": "accessibility.html#empty-and-redundant-links",
    "title": "Accessibility commitment",
    "section": "Empty and redundant links",
    "text": "Empty and redundant links\nIn the blog thumbnail listing page images have missing links and redundant links."
  },
  {
    "objectID": "accessibility.html#poor-contrast-colours",
    "href": "accessibility.html#poor-contrast-colours",
    "title": "Accessibility commitment",
    "section": "Poor contrast colours",
    "text": "Poor contrast colours\nThere are elements on the pages where there is insufficient colour contrast:\n\nBlog listing where the following pages is listed as .... This is currently too light to view. This is an issue on the website’s GitHub."
  },
  {
    "objectID": "accessibility.html#missing-level-headers",
    "href": "accessibility.html#missing-level-headers",
    "title": "Accessibility commitment",
    "section": "Missing level headers",
    "text": "Missing level headers\nThere are elements on the pages where first and second level heading is missing."
  },
  {
    "objectID": "accessibility.html#youtube-video-links",
    "href": "accessibility.html#youtube-video-links",
    "title": "Accessibility commitment",
    "section": "YouTube video links",
    "text": "YouTube video links\nThere are links to YouTube videos in this website that currently only have the automated YouTube subtitles."
  },
  {
    "objectID": "accessibility.html#event-handlers-may-not-be-accessible",
    "href": "accessibility.html#event-handlers-may-not-be-accessible",
    "title": "Accessibility commitment",
    "section": "Event handlers may not be accessible",
    "text": "Event handlers may not be accessible\nThis website has used JavaScript events in Quarto and may not be accessible to both mouse and keyboard users."
  },
  {
    "objectID": "supporters.html",
    "href": "supporters.html",
    "title": "NHS-R Community Quarto website",
    "section": "",
    "text": "NHS-R Community has received support from various organisations over the years and those listed on the front page have provided financial support in the last 12 months.\nIf you wish to sponsor NHS-R Community for its activities or the conference, held in conjunction with NHS.pycom, please contact us at nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "supporters.html#supporters-of-nhs-r-community",
    "href": "supporters.html#supporters-of-nhs-r-community",
    "title": "NHS-R Community Quarto website",
    "section": "",
    "text": "NHS-R Community has received support from various organisations over the years and those listed on the front page have provided financial support in the last 12 months.\nIf you wish to sponsor NHS-R Community for its activities or the conference, held in conjunction with NHS.pycom, please contact us at nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "Training",
    "section": "",
    "text": "Warning\n\n\n\nLimited-capacity free online training, apply now to secure your place!\n\n\n🎉 We’re delighted to announce that, new for 2025, we are working in collaboration with Dr Pawel Orzechowski of the University of Edinburgh to offer a FREE 10-week distance-learning introductory course in NLP with R for NHS employees!\nThe course will run from 8th September to 20th November 2025. In this course, students will explore the fundamental concepts of natural language processing (NLP) and its role in current and emerging technologies.\nThis course has been adapted from a postgraduate-level offering from the University and will be run using their bespoke learning platform and a set of fortnightly Teams sessions, culminating in a project presented back to the class in the final session.\nTo see if you meet the pre-requisites for applying, get more information on the course, and to submit your application, please visit this online survey. Please note, places on the course are limited, and will be assigned on a first-come-first-served basis for those meeting the prerequisite criteria. Applications will close on 29th August, so hurry to secure your place!🏃‍♀️\nCourse dates: 8th September - course begins\nZoom meeting session dates (see invite email for Zoom links)\n\n24th September, 9:30:11:30 BST - Session 1\n8th October, 9:30:11:30 BST - Session 2\n22nd October, 9:30:11:30 BST - Session 3\n5th November, 9:30:11:30 BST - Session 4\n19th November, 9:30:11:30 BST - Final Session"
  },
  {
    "objectID": "training.html#nlp",
    "href": "training.html#nlp",
    "title": "Training",
    "section": "",
    "text": "Warning\n\n\n\nLimited-capacity free online training, apply now to secure your place!\n\n\n🎉 We’re delighted to announce that, new for 2025, we are working in collaboration with Dr Pawel Orzechowski of the University of Edinburgh to offer a FREE 10-week distance-learning introductory course in NLP with R for NHS employees!\nThe course will run from 8th September to 20th November 2025. In this course, students will explore the fundamental concepts of natural language processing (NLP) and its role in current and emerging technologies.\nThis course has been adapted from a postgraduate-level offering from the University and will be run using their bespoke learning platform and a set of fortnightly Teams sessions, culminating in a project presented back to the class in the final session.\nTo see if you meet the pre-requisites for applying, get more information on the course, and to submit your application, please visit this online survey. Please note, places on the course are limited, and will be assigned on a first-come-first-served basis for those meeting the prerequisite criteria. Applications will close on 29th August, so hurry to secure your place!🏃‍♀️\nCourse dates: 8th September - course begins\nZoom meeting session dates (see invite email for Zoom links)\n\n24th September, 9:30:11:30 BST - Session 1\n8th October, 9:30:11:30 BST - Session 2\n22nd October, 9:30:11:30 BST - Session 3\n5th November, 9:30:11:30 BST - Session 4\n19th November, 9:30:11:30 BST - Final Session"
  },
  {
    "objectID": "fellowship.html",
    "href": "fellowship.html",
    "title": "NHS-R Community Fellowships",
    "section": "",
    "text": "Our community is largely volunteer-led. We depend on our members to get involved in the running and decision making of NHS-R, so that it remains a useful set of ever-growing resources to support and encourage our membership.\nIn recognition of the significant contributions from members, the NHS-R Community awards the honorary title of Fellow for a set period of 3 years.\nFellowships are a great way to advertise your contributions, show your skill set and dedication to open source coding in healthcare, and embellish your CV with evidence of your community spirit.\nFor details on how to nominate yourself or someone else for a Fellowship, please visit the NHSR-Way book\n\n  \n  \n\n\n\nList of current NHS-R Fellows\n\n\nName\nOrganisation\nValid From\n\n\n\n\nAdrian Pratt\nNHS Arden & GEM CSU\n01/04/2025\n\n\nAnastasiia Zharinova\nNHS England\n01/04/2025\n\n\nAnya Ferguson\nThe Strategy Unit\n01/04/2025\n\n\nBianca O’Mahoney\nThe Strategy Unit\n01/01/2024\n\n\nCaileigh Solomenca\nThe Strategy Unit\n01/04/2025\n\n\nChris Beeley\nThe Strategy Unit\n01/04/2025\n\n\nChris Mainey\nNHS Birmingham and Solihull\n01/04/2025\n\n\nClaire Welsh\nThe Strategy Unit\n01/04/2025\n\n\nEllen Coughlan\nThe Health Foundation\n01/04/2025\n\n\nJoe Wilson\nNHS England\n01/04/2025\n\n\nLyn Howard\nNHS England\n01/04/2025\n\n\nNatasha Stephenson\nThe Strategy Unit\n01/04/2024\n\n\nPablo Leon-Rodenas\nNHS England\n01/04/2025\n\n\nPaul Johnson\nNHS South, Central and West\n01/04/2025\n\n\nSam Hollings\nNHS England\n01/04/2025\n\n\nSimon Wellesley-Miller\nNHS England\n01/04/2025\n\n\nStuart Howard\nBlackpool Teaching Hospitals NHS Foundation Trust\n01/04/2025\n\n\nTom Smith\nNottingham University Hospitals NHS Trust\n14/07/2025\n\n\nZoë Turner\nThe Strategy Unit and Nottinghamshire Healthcare NHS Foundation Trust\n01/04/2025\n\n\n\n*Organisations listed are at the time of award\n\n\n\nIf you have any questions about Fellowships, or would like to alter or remove your details from the Fellowship list above, please email us at nhs.rcommunity@nhs.net.\n\nThank you for your support!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "books/index.html",
    "href": "books/index.html",
    "title": "Books",
    "section": "",
    "text": "NHS-R Way book\n\n\n\ncode-of-conduct\n\ntraining\n\n\n\nEverything you need or want to know about NHS-R Community including: how to contribute and get involved, code styles, training preparation materials.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStatement on Tools\n\n\n\ndata-science\n\ninstallation\n\ngetting-started\n\n\n\nIn this book we’ve compiled a set of technical resources, links and write down our experiences of using open data science programs like R and Python within the NHS and…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpen Analytics Resources\n\n\n\ndata-science\n\nanalysis\n\nlinks\n\ntraining\n\n\n\nUseful links for health and care analysts and data scientists.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHealth Inequalities\n\n\n\ndata-science\n\nhealth-inequalities\n\n\n\nThis project is a collection of information and knowledge related to analytical work on “Health Inequalities”, as performed in the NHS and the wider UK Health and Social…\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  },
  {
    "objectID": "books/posts/open-analytics-resources/index.html",
    "href": "books/posts/open-analytics-resources/index.html",
    "title": "Open Analytics Resources",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "books/posts/statement-on-tools/index.html",
    "href": "books/posts/statement-on-tools/index.html",
    "title": "Statement on Tools",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "licence.html",
    "href": "licence.html",
    "title": "Licence",
    "section": "",
    "text": "This website, its content and code are released under a CC0 1.0 Universal.\nBlogs that are published through this site will also be under this license and can be referenced to personal sites if the author wishes.\n\n  \n\nFor more information on the licences used by NHS-R Community go to the chapter Style Guide for code in the NHS-R Way book.\n\n\n\n Back to top"
  },
  {
    "objectID": "books/posts/NHSR-way/index.html",
    "href": "books/posts/NHSR-way/index.html",
    "title": "NHS-R Way book",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "books/posts/health-inequalities/index.html",
    "href": "books/posts/health-inequalities/index.html",
    "title": "Health Inequalities",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "NHS-R Community News",
    "section": "",
    "text": "Closing date for applications to NLP with R course\nApplications to join our FREE 10-week distance-learning course on Natural Language Processing with R course will close on 29th August. This course is for NHS employees who are familiar with R but not with NLP. The course is being kindly provided by Dr Pawel Orzechowski of the University of Edinburgh. For more information on this course, click here.\n\n\n\nAbstract Call for RPYSOC2025 Conference!\n The RPYSOC2025 Conference will be held in London, on 13th and 14th November 2025. This event will be hybrid, so presentations can be delivered in person or virtually. For more information on the conference, see here.\nTo submit an abstract, please click here.\nAbstracts must be submitted by 5th September 2025.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "coffee-and-code.html",
    "href": "coffee-and-code.html",
    "title": "Coffee & Coding",
    "section": "",
    "text": "Please note:"
  },
  {
    "objectID": "coffee-and-code.html#august-2025",
    "href": "coffee-and-code.html#august-2025",
    "title": "Coffee & Coding",
    "section": "August 2025",
    "text": "August 2025\n📆 8th Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 21st Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx"
  },
  {
    "objectID": "coffee-and-code.html#september-2025",
    "href": "coffee-and-code.html#september-2025",
    "title": "Coffee & Coding",
    "section": "September 2025",
    "text": "September 2025\n📆 3rd Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 19th Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx"
  },
  {
    "objectID": "coffee-and-code.html#october-2025",
    "href": "coffee-and-code.html#october-2025",
    "title": "Coffee & Coding",
    "section": "October 2025",
    "text": "October 2025\n📆 2nd Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 15th Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 31st Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx"
  },
  {
    "objectID": "coffee-and-code.html#november-2025",
    "href": "coffee-and-code.html#november-2025",
    "title": "Coffee & Coding",
    "section": "November 2025",
    "text": "November 2025\n📆 13th Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 26th Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx"
  },
  {
    "objectID": "coffee-and-code.html#december-2025",
    "href": "coffee-and-code.html#december-2025",
    "title": "Coffee & Coding",
    "section": "December 2025",
    "text": "December 2025\n📆 12th Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx"
  },
  {
    "objectID": "conference24.html",
    "href": "conference24.html",
    "title": "NHS-R Community Quarto website",
    "section": "",
    "text": "21 and 22 November 2024, Hall 11, ICC Birmingham\nThe two open source communities, NHS-R Community and NHS.pycom, are hosting an in-person and virtually streamed annual conference we’ve called RPySOC (incorporating R, Py from Python and using the initials for Open Source Conference). We will have over 120 people in person with as many as double or triple that online. Both in person and virtual attendees are invited to post Q&As and have discussions with peers in our NHS-R Community Slack and which is available throughout the year for technical support, sharing useful information and connecting with others in our profession.\nWe have R and Python talks throughout two days in lightning (10 minutes) and plenary (20 minutes) slots and we will also have an in person event called an unconference, introduced to us through colleagues at The University of Edinburgh, which runs alongside the talks. This is where attendees put forward suggestions for discussion which others sign up to.\nTalks will be recorded and shared on YouTube and slides made available in our conference-2024 repo."
  },
  {
    "objectID": "conference24.html#tickets",
    "href": "conference24.html#tickets",
    "title": "NHS-R Community Quarto website",
    "section": "Tickets",
    "text": "Tickets\nIn-person attendance - full, but please email us at nhs.rcommunity@nhs.net to join the waiting list.\nVirtual attendance - tickets are available by completing this form."
  },
  {
    "objectID": "conference24.html#slack",
    "href": "conference24.html#slack",
    "title": "NHS-R Community Quarto website",
    "section": "Slack",
    "text": "Slack\nWe will be using a dedicated conference channel #in-person-conference-talks on Slack to take questions for our speakers. Last year this allowed both in-person and virtual attendees to ask questions and inspired discussions that lasted beyond the conference.\nThere will also be an unconference channel: #unconferencing-session.\nA link to join the NHS-R community Slack channel can be found in our postcard. More details on how we use Slack can be found in the NHS-R Way."
  },
  {
    "objectID": "conference24.html#programme",
    "href": "conference24.html#programme",
    "title": "NHS-R Community Quarto website",
    "section": "Programme",
    "text": "Programme\nTimings are subject to change even on the day\n\n\nDay one\nDay two\nUnconference\n\n\n\n\nCodefixed_points &lt;- c(\n  \"Break\",\n  \"Lunch\",\n  \"Registration and coffee\",\n  \"Welcome and Housekeeping\",\n  \"Closing of day\"\n)\n\ntibble::tribble(\n  ~Time, ~Title, ~Speaker,\n  \"09:00 – 09:45\", \"Registration and coffee\", \"\",\n  \"09:45 – 09:55\", \"Welcome and Housekeeping\", \"Zoë Turner\",\n  \"09:55 – 10:20\", \"RAPping my head against a databricks wall\", \"Louise Schreuders\",\n  \"10:20 – 10:35\", \"Reducing mental health inequalities among the BAME residents of Herefordshire and Worcestershire communities\", \"Oluwatimilehin Olabamiyo\",\n  \"10:35 – 11:00\", \"Simplifying development of AI applications integrated in EHRs with Health Chain\", \"Jennifer Jiang-Kells\",\n  \"11:00 – 11:15\", \"Beyond the dashboard: R for value added insights\", \"Nicola Farthing\",\n  \"11:15 – 11:45\", \"Break\", \"\",\n  \"11:45 – 12:00\", \"Reimagining NHS dashboards: an open-source approach with plotly-dash\", \"Jennifer Struthers\",\n  \"12:00 – 12:25\", \"Using Machine Learning and secondary care activity data to identify risk of cancer earlier\", \"Scarlett Kynoch\",\n  \"12:25 – 12:40\", \"The patient does not exist – generating synthetic patient data with Wasserstein GAN\", \"Simon Newey\",\n  \"12:40 – 13:05\", \"Streamlining machine learning development at the NHS via open-source tools\", \"Elias Altrabsheh and James Sibbit\",\n  \"13:05 – 13:07\", \"rainbowR\", \"Ella Kaye\",\n  \"13:07 – 13:55\", \"Lunch\", \"\",\n  \"13:55 – 14:10\",\n  \"What insights did Glasgow Scottish Ambulance Service (SAS) gain from combining multiple data sources about all chest pain patients from 2023? We'll present about the process and findings of a 1-year long MSc dissertation project.\", \"Katalin Koszegi\",\n  \"14:10 – 14:25\",\n  \"Predictive Modelling for health and social care capacity planning using open data\", \"Sebastian Fox\",\n  \"14:25 – 14:50\", \"To explain or predict: how different modelling objectives change how you use the same tools\", \"Chris Mainey\",\n  \"14:50 – 15:05\", \"Using Openxlsx2 to automate excel publications\", \"Ruth Keane\",\n  \"15:05 – 15:35\", \"Break\", \"\",\n  \"15:35 – 15:50\", \"What I learnt about (programming) languages by building bilingual websites\", \"Rosemary Walmsley\",\n  \"15:50 – 16:15\", \"Leveraging R to implement novel theoretical development in online ‘digital twin' simulation modelling\", \"Richard Wood\",\n  \"16:15 – 16:30\", \"Should I use your package\", \"Colin Gillespie\",\n  \"16:30 – 16:45\", \"Cracking open the TiN: how we build a one-stop statistics website using R, GitHub and BigQuery\", \"Mohan Del\",\n  \"16:45 – 17:10\", \"Closing of day\", \"\"\n) |&gt;\n  gt::gt() |&gt;\n  gt::cols_width(\n    Time ~ px(110),\n    Title ~ px(500),\n    Speaker ~ px(150)\n  ) |&gt;\n  gtExtras::gt_highlight_rows(rows = Title %in% fixed_points)\n\n\n\n\n\nTime\nTitle\nSpeaker\n\n\n\n09:00 – 09:45\nRegistration and coffee\n\n\n\n09:45 – 09:55\nWelcome and Housekeeping\nZoë Turner\n\n\n09:55 – 10:20\nRAPping my head against a databricks wall\nLouise Schreuders\n\n\n10:20 – 10:35\nReducing mental health inequalities among the BAME residents of Herefordshire and Worcestershire communities\nOluwatimilehin Olabamiyo\n\n\n10:35 – 11:00\nSimplifying development of AI applications integrated in EHRs with Health Chain\nJennifer Jiang-Kells\n\n\n11:00 – 11:15\nBeyond the dashboard: R for value added insights\nNicola Farthing\n\n\n11:15 – 11:45\nBreak\n\n\n\n11:45 – 12:00\nReimagining NHS dashboards: an open-source approach with plotly-dash\nJennifer Struthers\n\n\n12:00 – 12:25\nUsing Machine Learning and secondary care activity data to identify risk of cancer earlier\nScarlett Kynoch\n\n\n12:25 – 12:40\nThe patient does not exist – generating synthetic patient data with Wasserstein GAN\nSimon Newey\n\n\n12:40 – 13:05\nStreamlining machine learning development at the NHS via open-source tools\nElias Altrabsheh and James Sibbit\n\n\n13:05 – 13:07\nrainbowR\nElla Kaye\n\n\n13:07 – 13:55\nLunch\n\n\n\n13:55 – 14:10\nWhat insights did Glasgow Scottish Ambulance Service (SAS) gain from combining multiple data sources about all chest pain patients from 2023? We'll present about the process and findings of a 1-year long MSc dissertation project.\nKatalin Koszegi\n\n\n14:10 – 14:25\nPredictive Modelling for health and social care capacity planning using open data\nSebastian Fox\n\n\n14:25 – 14:50\nTo explain or predict: how different modelling objectives change how you use the same tools\nChris Mainey\n\n\n14:50 – 15:05\nUsing Openxlsx2 to automate excel publications\nRuth Keane\n\n\n15:05 – 15:35\nBreak\n\n\n\n15:35 – 15:50\nWhat I learnt about (programming) languages by building bilingual websites\nRosemary Walmsley\n\n\n15:50 – 16:15\nLeveraging R to implement novel theoretical development in online ‘digital twin' simulation modelling\nRichard Wood\n\n\n16:15 – 16:30\nShould I use your package\nColin Gillespie\n\n\n16:30 – 16:45\nCracking open the TiN: how we build a one-stop statistics website using R, GitHub and BigQuery\nMohan Del\n\n\n16:45 – 17:10\nClosing of day\n\n\n\n\n\n\n\n\n\n\nCodetibble::tribble(\n  ~Time, ~Title, ~Speaker,\n  \"09:00 – 09:45\", \"Registration and coffee\", \"\",\n  \"09:45 – 09:55\", \"Welcome and Housekeeping\", \"Zoë Turner\",\n  \"09:55 – 10:20\", \"The Reusability Crisis in Healthcare Analytics\", \"Rhian Davies\",\n  \"10:20 – 10:35\", \"Shift staffing via task load prediction\", \"Marcos Fabietti\",\n  \"10:35 – 10:50\", \"Unleashing the power of pathway simulation\", \"Sammi Rosser\",\n  \"10:50 – 11:15\", \"Break\", \"\",\n  \"11:15 – 11:40\", \"New generic tests for cancer – with R is a clinical scientists best friend\", \"Joe Shaw\",\n  \"11:40 – 11:55\", \"Beyond automation: a shiny app to maximise analytical impact routine reporting narrative\", \"Laura Birks\",\n  \"11:55 – 12:10\", \"Sharpening my Python skills through self-development of web scraping bank complaints data\", \"Kenneth Quan\",\n  \"12:10 – 12:25\", \"GitHub as a team sport\", \"Matt Dray\",\n  \"12:25 – 13:10\", \"Lunch\", \"\",\n  \"13:10 – 13:25\", \"Presenting fingertips in data in a more friendly format\", \"Rachel Brown\",\n  \"13:25 – 13:50\", \"A method to apply temporal graph analysis on electronic patient record data to explore healthcare professional patient interaction intensity\", \"John Booth\",\n  \"13:50 – 14:05\", \"Deploying a Shiny app with Docker in a Raspberry Pi\", \"Pablo León Ródenas\",\n  \"14:05 – 14:20\", \"Estimating flexible hazard rates for C diff recurrence from electronic health records using the SplinHazard Regression package and other methods in R\", \"Elisabeth Dietz\",\n  \"14:20 – 14:45\", \"Assessment of patient feedback using Natural Language Processing (NLP) and textual data analysis in R\", \"Ana Singh\",\n  \"14:45 – 15:15\", \"Break\", \"\",\n  \"15:15 – 15:40\", \"Forged in the fire: agile project management lessons from the frontline\", \"Chris Beeley\",\n  \"15:40 – 15:55\", \"Community Talk – Turing Way\", \"Sophia Batchelor\",\n  \"15:55 – 16:05\", \"Community Talk -NHS.Pycom\", \"Alex Cheung\",\n  \"16:05 – 16:15\", \"Closing talk NHS-R Community and raffle\", \"Zoë Turner\",\n  \"16:15 – 16:20\", \"Raffle\", \"\",\n  \"16:20\", \"Closing of the day\", \"\"\n) |&gt;\n  gt::gt() |&gt;\n  gt::cols_width(\n    Time ~ px(110),\n    Title ~ px(500),\n    Speaker ~ px(150)\n  ) |&gt;\n  gtExtras::gt_highlight_rows(rows = Title %in% fixed_points)\n\n\n\n\n\nTime\nTitle\nSpeaker\n\n\n\n09:00 – 09:45\nRegistration and coffee\n\n\n\n09:45 – 09:55\nWelcome and Housekeeping\nZoë Turner\n\n\n09:55 – 10:20\nThe Reusability Crisis in Healthcare Analytics\nRhian Davies\n\n\n10:20 – 10:35\nShift staffing via task load prediction\nMarcos Fabietti\n\n\n10:35 – 10:50\nUnleashing the power of pathway simulation\nSammi Rosser\n\n\n10:50 – 11:15\nBreak\n\n\n\n11:15 – 11:40\nNew generic tests for cancer – with R is a clinical scientists best friend\nJoe Shaw\n\n\n11:40 – 11:55\nBeyond automation: a shiny app to maximise analytical impact routine reporting narrative\nLaura Birks\n\n\n11:55 – 12:10\nSharpening my Python skills through self-development of web scraping bank complaints data\nKenneth Quan\n\n\n12:10 – 12:25\nGitHub as a team sport\nMatt Dray\n\n\n12:25 – 13:10\nLunch\n\n\n\n13:10 – 13:25\nPresenting fingertips in data in a more friendly format\nRachel Brown\n\n\n13:25 – 13:50\nA method to apply temporal graph analysis on electronic patient record data to explore healthcare professional patient interaction intensity\nJohn Booth\n\n\n13:50 – 14:05\nDeploying a Shiny app with Docker in a Raspberry Pi\nPablo León Ródenas\n\n\n14:05 – 14:20\nEstimating flexible hazard rates for C diff recurrence from electronic health records using the SplinHazard Regression package and other methods in R\nElisabeth Dietz\n\n\n14:20 – 14:45\nAssessment of patient feedback using Natural Language Processing (NLP) and textual data analysis in R\nAna Singh\n\n\n14:45 – 15:15\nBreak\n\n\n\n15:15 – 15:40\nForged in the fire: agile project management lessons from the frontline\nChris Beeley\n\n\n15:40 – 15:55\nCommunity Talk – Turing Way\nSophia Batchelor\n\n\n15:55 – 16:05\nCommunity Talk -NHS.Pycom\nAlex Cheung\n\n\n16:05 – 16:15\nClosing talk NHS-R Community and raffle\nZoë Turner\n\n\n16:15 – 16:20\nRaffle\n\n\n\n16:20\nClosing of the day\n\n\n\n\n\n\n\n\n\nThe unconference was a popular addition to the conference last year. People came forward with ideas and questions inspiring some great discussions on topics ranging from Quarto to statistical tests to NHS-R Community branding. We will be trying out unconferencing again and look forward to seeing you there.\nDay one - we will take suggestions for informal group discussions which people can vote for or say they can turn up to.\nDay two - we will split the room so that people can move between the speaker sessions and the unconference.\nFor more information on unconferencing we have a podcast talk with Pawel Orzechowski from the University of Edinburgh who introduced the idea of unconferencing to us and a blog by Ben Murch on how it went at the 2023 conference.\nRPySOC24 Unconference sessions:\n\nCodetibble::tribble(\n  ~Time, ~Title,\n  \"09:55\", \"NLP techniques and practical usage questions/discussions \",\n  \"09:55\", \"CPD from entry-level to C10 – whats worth doing questions/discussions\",\n  \"10:20\", \"RAP chat\",\n  \"10:20\", \"Analytical bugbears - what makes you go grrr!\",\n  \"11:15\", \"I know R, should I learn Python? I know Python, should I learn R?\",\n  \"11:15\", \"Structuring a BI Team to allow different skillsets to flourish\",\n  \"11:15\", \"Data literacy in relation to linked data sets (Sudlow review): who needs it? And where do we start?\",\n  \"11:15\", \"Data scientists vs data analysts: are we different?\",\n  \"11:40\", \"Deploying streamlit/shiny/plotly dash apps securely\",\n  \"11:40\", \"How do we balance using cool AI tools with our environmental responsibilities\",\n  \"13:10\", \"Do everything in github\",\n  \"13:10\", \"Writing tests into my code – tips please!\",\n  \"13:50\", \"Methods in github sustainability\",\n  \"13:50\", \"Practically introducing R training within our organisation\",\n  \"13:50\", \"How can we better use data to annual plan in an acute hospital\",\n  \"14:20\", \"Quarto reporting\",\n  \"14:20\", \"How to convince managers that we need data science\",\n  \"14:20\", \"System dynamics simulation\",\n\n) |&gt;\n  gt::gt() |&gt;\n  gt::cols_width(\n    Time ~ px(110),\n    Title ~ px(500),\n  )\n\n\n\n\n\nTime\nTitle\n\n\n\n09:55\nNLP techniques and practical usage questions/discussions\n\n\n09:55\nCPD from entry-level to C10 – whats worth doing questions/discussions\n\n\n10:20\nRAP chat\n\n\n10:20\nAnalytical bugbears - what makes you go grrr!\n\n\n11:15\nI know R, should I learn Python? I know Python, should I learn R?\n\n\n11:15\nStructuring a BI Team to allow different skillsets to flourish\n\n\n11:15\nData literacy in relation to linked data sets (Sudlow review): who needs it? And where do we start?\n\n\n11:15\nData scientists vs data analysts: are we different?\n\n\n11:40\nDeploying streamlit/shiny/plotly dash apps securely\n\n\n11:40\nHow do we balance using cool AI tools with our environmental responsibilities\n\n\n13:10\nDo everything in github\n\n\n13:10\nWriting tests into my code – tips please!\n\n\n13:50\nMethods in github sustainability\n\n\n13:50\nPractically introducing R training within our organisation\n\n\n13:50\nHow can we better use data to annual plan in an acute hospital\n\n\n14:20\nQuarto reporting\n\n\n14:20\nHow to convince managers that we need data science\n\n\n14:20\nSystem dynamics simulation"
  },
  {
    "objectID": "webinars-and-workshops.html",
    "href": "webinars-and-workshops.html",
    "title": "Webinars and Workshops",
    "section": "",
    "text": "Please note:"
  },
  {
    "objectID": "webinars-and-workshops.html#august-2025",
    "href": "webinars-and-workshops.html#august-2025",
    "title": "Webinars and Workshops",
    "section": "August 2025",
    "text": "August 2025\n📆 13th Webinar: “An Improved I Chart with Jacob Anhøj and Mohammed Mohammed” + 13:00 - 14:00 (GMT) + 🔗 Registration Link"
  },
  {
    "objectID": "webinars-and-workshops.html#september-2025",
    "href": "webinars-and-workshops.html#september-2025",
    "title": "Webinars and Workshops",
    "section": "September 2025",
    "text": "September 2025\n📆 11th Webinar: “Complexity Made Approachable: Rethinking R-Based Modelling for Healthcare with AM-Smart Tools with Brian Castellani, Corey Schimpf and Chris Caden” + 13:00 - 14:00 (GMT) + 🔗 Registration Link\n📆 23rd & 24th Workshop: “Introduction to R and RStudio with Dr Claire Welsh”\n\n09:00 - 13:00 (GMT)\n🔗 Registration Link"
  },
  {
    "objectID": "webinars-and-workshops.html#october-2025",
    "href": "webinars-and-workshops.html#october-2025",
    "title": "Webinars and Workshops",
    "section": "October 2025",
    "text": "October 2025\n📆 14th & 15th Workshop: “Intermediate R with Dr Claire Welsh”\n\n09:00 - 13:00 (GMT)\n🔗 Registration Link"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About NHS-R Community",
    "section": "",
    "text": "The NHS-R Community started in 2018 to promote the use of R in the NHS and the community has grown rapidly ever since. It is a community that is broader than the NHS as members come from public sector organisations across the UK, including Local Authorities and Civil Service, as well as academics and voluntary sector people who have an interest in healthcare. Whilst R is a core language for support by the community, there is always support for data science tools more generally, particularly where they cross over with R."
  },
  {
    "objectID": "about.html#how-to-contribute-to-our-website",
    "href": "about.html#how-to-contribute-to-our-website",
    "title": "About NHS-R Community",
    "section": "How to contribute to our website",
    "text": "How to contribute to our website\nFor advice on how to suggest updates or changes to any of our content, please see the NHS-R Way Book"
  },
  {
    "objectID": "about.html#special-thanks",
    "href": "about.html#special-thanks",
    "title": "About NHS-R Community",
    "section": "Special thanks",
    "text": "Special thanks\nTo the creators of Quarto and to those who publish their code to share their knowledge particularly the website Real World Data Science and Silvia Canelón’s personal site.\nBoth were instrumental in building this site as they share the code from their website but have also contributed to the NHS-R Community Conferences:\n\nBrian Tarran opened the NHS-R Community Conference in 2023 with his talk Forging community links: NHS-R, the Royal Statistical Society and Real World Data Science.\nSilvia Canelón ran a two-part Xaringan (including CSS) workshop at the 2020 NHS-R Conference - Part 1 and Part 2. And as part of this workshop Silvia kindly created {NHSRtheme} which contains the code for the NHS Branding colours used in this site."
  },
  {
    "objectID": "events.html",
    "href": "events.html",
    "title": "Events and Training",
    "section": "",
    "text": "ConferenceWebinars and WorkshopsCoffee & Coding\n\n\n\nWe are pleased to announce that the RPYSOC 2025 conference will be held at the Wellcome Trust (215 Euston Road, London), offering both in-person and virtual attendance options!\nConference dates: Thursday 13th November 2025 Friday 14th November 2025\nRegistration to open shortly – so watch this space!\nTo submit an abstract, please complete this form.\nPlease note that the abstract submission deadline is Friday 05th September 2025. We would be honoured to incorporate your work into the conference.\nThank you for being part of the community during a time of change—we appreciate your engagement!\n\n\nPlease note:\n\nWorkshop attendance is restricted to employees of the NHS, public sector, civil servants, voluntary sector, charities and academia, or who are retirees from any of these sectors.\nWebinars are free to all to attend, and recordings are stored on our YouTube Channel\n\n\nAugust 2025\n📆 13th Webinar: “An Improved I Chart with Jacob Anhøj and Mohammed Mohammed” + 13:00 - 14:00 (GMT) + 🔗 Registration Link\n\n\n\nSeptember 2025\n📆 11th Webinar: “Complexity Made Approachable: Rethinking R-Based Modelling for Healthcare with AM-Smart Tools with Brian Castellani, Corey Schimpf and Chris Caden” + 13:00 - 14:00 (GMT) + 🔗 Registration Link\n📆 23rd & 24th Workshop: “Introduction to R and RStudio with Dr Claire Welsh”\n\n09:00 - 13:00 (GMT)\n🔗 Registration Link\n\n\n\n\nOctober 2025\n📆 14th & 15th Workshop: “Intermediate R with Dr Claire Welsh”\n\n09:00 - 13:00 (GMT)\n🔗 Registration Link\n\n\n\n\nPlease note:\n\nCoffee & Code sessions are a chance for like-minded coders to get together and chat, share tips, code snippets, and watch show-and-tells about useful or new R stuff. These sessions are run by Simon Wellesley-Miller (NHS England), and recordings are made available here.\n\n\nAugust 2025\n📆 8th Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 21st Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n\n\n\nSeptember 2025\n📆 3rd Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 19th Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n\n\n\nOctober 2025\n📆 2nd Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 15th Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 31st Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n\n\n\nNovember 2025\n📆 13th Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n📆 26th Coffee and Coding:\n\n14:00 - 15:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n\n\n\nDecember 2025\n📆 12th Coffee and Coding:\n\n11:00 - 12:00 (GMT)\n🔗 Meeting Link\nMeeting ID: 331 945 876 23\nPasscode: Po2nEx\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nhs.rcommunity@nhs.net. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at nhs.rcommunity@nhs.net. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Covenant Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by [Mozilla’s code of conduct enforcement ladder][https://github.com/mozilla/inclusion].\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations."
  },
  {
    "objectID": "r-packages.html",
    "href": "r-packages.html",
    "title": "R packages",
    "section": "",
    "text": "The NHS-R community has created and are developing R packages to support analytics in health and care.\nContributions are very welcome. We appreciate code, documentation, bug fixes and designs for hexes. Please feel free to tackle any of the issues in the package GitHub repository or create a new issue. If you have other code or ideas for new packages, please let us know.\n       \n\n\n\n Back to top"
  },
  {
    "objectID": "blog/apha-april-blog.html",
    "href": "blog/apha-april-blog.html",
    "title": "AphA April 2023 Blog",
    "section": "",
    "text": "First published for the AphA April 2023 newsletter:\nThis is a new look for the updates from NHS-R Community as we’ve had a few changes recently in the NHS-R Community support and this is a great opportunity for us to share what we’ve been working on and our plans for the future – which you can be a part of if you haven’t already found us!\nThe NHS-R Community started out with funding but that was for a fixed period which came to an end last year. Thankfully the community has grown to such an extent that the value of what we can achieve was recognised by the Strategy Unit, part of NHS Midlands and Lancashire Commissioning Support Unit, and two roles were created with NHS-R Community support in the job description. Those roles were filled by Chris Beeley and me, Zoë Turner, as Data Scientists.\nOur roles are not solely supporting NHS-R Community, but like we did when we worked in Nottinghamshire Healthcare NHS Foundation Trust, our dedication to the NHS-R Community, supporting data science learning and being open pervade all that we do. Chris is leading the newly formed Data Science Team in the Strategy Unit which is a privilege to be a part of. I worked with Chris, as many people know, in the Clinical Development Unit in Nottinghamshire, using R and publishing our code and methodology in the open. I found publishing things incredibly difficult at first, not because it’s technically hard to do; it takes time to learn but not impossible. It was more the culture of sharing openly was new and uncomfortable. I had a lot of anxiety about what I could publish and if I’d make mistakes but, crucially, when I made mistakes, I was part of a supportive team which helped me make corrections and gave me the encouragement to keep practising.\nAnd it’s with this experience that I’ll be concentrating on supporting analysts and data scientists across health and social care to learn and practice new skills in data science. I’ll be developing and delivering training as part of my role in the Strategy Unit on a consultancy basis but all materials will be published through the NHS-R Community GitHub repository under an open licence. We regularly have volunteers from the community deliver the free Introduction to R and R Studio training events and I hope to extend this in the coming year to include using Git with RStudio for version control and open publishing of code.\nNHS-R Community will also have a presence at the first national analytics conference for health and social care HACA2023 at Birmingham Medical School on 11-12 July 2023 which has had a huge number of submitted abstracts to it so do sign up now!\nAnd as we have done since 2018, we’ll be putting on the NHS-R Community (hybrid) Conference this year with talks planned for 17-18 October 2023 at Edgbaston Cricket Ground in Birmingham. Abstracts will be out for contributing to very soon and we’ll also be looking for people to run workshops which will planned over a week with dates to be confirmed soon. All workshops, like last year, will be virtual so people can join in from across the UK. After the success of NHS-R Community’s conference last year, we’ll be including talks and workshops for Python.\nWe have grown so much as a community with so much material I’ll be spending some of my time to helping curate. We have a website NHS-R Community, a GitHub repository, a YouTube channel, write blogs and create regular Podcasts but we are mindful it can be overwhelming knowing where to start with finding the NHS-R Community and perhaps also to contributing to it …\n… so I will end with the one link you need to find the people who make up the community which is the Slack workspace. It’s open to anyone who is interested in R (and other languages) and healthcare. We post all our updates and news regularly in this space and I hope to see you there sometime but, if you want to contact us via email, we can be found at nhs.rcommunity@nhs.net.\nThis blog is also available in its original quarto script on GitHub, comments on this blog can be made here or as a GitHub issue.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-conference-was-it-worth-it.html",
    "href": "blog/nhs-r-conference-was-it-worth-it.html",
    "title": "NHS-R Conference: was it worth it?",
    "section": "",
    "text": "Soon after I started in this role I was offered several free places to a conference on Big Data. It was aimed at Healthcare and with Big Data in the title it sounded very exciting. Sadly, like many things around ‘Big Data’ it was baffling. There were a few good, high-level, presentations but it turned out the conference was funded by private companies and they had bought the opportunity to showcase their wares. I didn’t learn anything practical and I would be hard pressed to recall anything from the event now. All in all, it was a disappointment.\nTime passed and I tentatively started messing around with R. Not being someone who can learn well from books and videos, I learn better from someone telling me what they’ve written, I went to a couple of free R training events, one of which was run by NHS-R. At that training there was talk of an NHS-R conference and I couldn’t help but get that familiar tingle of excitement. Yet, in the back of my head now was the nagging feeling that it might be like that Big Data conference. After all, these things are not easy to set up, I was still overwhelmed by what R could do and still just learning and, well, how good could it really be?\nOptimism won out, along with the fact that the conference was free and a few colleagues were going so I could get a lift. I had nothing to lose. I didn’t even have to build a case to go for my managers let alone myself. At worst I could expect a day out and a free coffee, maybe a biscuit. It was settled. I would even take my own lunch.\nIt’s been a fair number of weeks since the event, which is a pretty good time to evaluate something like this. I always find that there is a buzz after any conference or meeting. The buzz of course can be as much negative as positive energy so it’s best to wait a few days before reviewing, I think. Plus, I’m not that used to writing down my thoughts. After all of my doubts then, did it live up to my vague and overly excitable expectations? Oh yes, it did. In buckets.\nI’ve worked with some intelligent and creative people in the NHS and it turns out these weren’t isolated pockets of great people doing great work. The NHS, and other organisations close to it, are chock full of these people. We had speakers from Public Health, universities, foundations, private consultancies, Acute and Mental Health Trusts. In fact, I know it’s called NHS-R but it’s not an exclusive club and nor should it be.\nWe were shown finished pieces of work in R as well as the code itself which I really enjoyed. I’m an analyst and I like to see the logic and the code as that’s where I work – in the detail. Best of all though was the networking. I met a number of people I knew either from courses or from their blogs/R packages. Unlike the ‘Big Data’ conference where I had to track someone down on LinkedIn to ask about one of the presentations, and sadly heard nothing, I have contacted some of the speakers through Twitter and email and had conversations. There is a sense that everyone is in this together and that competitive edge you sometimes get in IT is notably missing.\nThe other great thing about this conference was that it was free and that should not be underestimated. Being free, it attracted those non-management, just starting out in R, not even statisticians and only did maths to GCSE level people like me. The NHS, like many other public-sector organisations, are feeling the pinch and more than ever we need support and training for the hidden side of the NHS – its data people. And it’s those people I hope you will see at the next NHS-R conference showing their achievements and sharing their enthusiasm, because, whilst this was a training/conference event, really it was the coming together of a community.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/asking_for_help.html",
    "href": "blog/asking_for_help.html",
    "title": "How to ask for help",
    "section": "",
    "text": "To enable others to help you, you’ll need to include a reprex, or reproducible example - a minimal example of the data and code you’re using, so they can recreate the problem."
  },
  {
    "objectID": "blog/asking_for_help.html#setting-things-up",
    "href": "blog/asking_for_help.html#setting-things-up",
    "title": "How to ask for help",
    "section": "Setting things up",
    "text": "Setting things up\nCtrl + F7 in RStudio will open a new source column which you can use to gather together the information you want to share in your request for help.\n\nlibrary(dplyr)\nlibrary(datapasta)\n\nYou’ll often want to include a sample of your dataframe rather than the whole thing, particularly if it’s very large.\n\nfresh_data &lt;- NHSRdatasets::LOS_model |&gt; dplyr::slice_sample(n = 20) # This produces a sample dataframe of 20 randomly selected rows. slice_sample(prop=0.2) will randomly select 20% of rows"
  },
  {
    "objectID": "blog/asking_for_help.html#sharing-your-data",
    "href": "blog/asking_for_help.html#sharing-your-data",
    "title": "How to ask for help",
    "section": "Sharing your data",
    "text": "Sharing your data\nThe base R function dput() produces the code to recreate your data sample.\n\nfresh_data |&gt; dput()\n\nor another alternative is to use df_paste() from the datapasta package."
  },
  {
    "objectID": "blog/asking_for_help.html#editing-the-dataframe",
    "href": "blog/asking_for_help.html#editing-the-dataframe",
    "title": "How to ask for help",
    "section": "Editing the dataframe",
    "text": "Editing the dataframe\nYou may need to edit the data before sharing it to anonymise it or remove confidential information. You could manually edit the output from dput() or df_paste.\nAn alternative way to modify your data for sharing\n\n# create a dataframe of organisation names and aliases\nOrganisation &lt;- unique(NHSRdatasets::LOS_model$Organisation)\nlength(Organisation) # checks the length of the vector Organisation\nalias &lt;- letters[1:10] # if you want to automate this you could use letters[1:length(Organisation)]\ndf &lt;- data.frame(Organisation, alias) # combines the 2 vectors into a dataframe\n\nJoin the dataframe we just created to the LOS_model dataframe, select the columns we want & take a sample 10 rows, before piping it into dput() to create the code we can share.\n\ndplyr::left_join(NHSRdatasets::LOS_model, df) |&gt;\n  select(Age, alias, Death) |&gt;\n  slice_sample(n = 10) |&gt;\n  dput()"
  },
  {
    "objectID": "blog/asking_for_help.html#what-should-you-include",
    "href": "blog/asking_for_help.html#what-should-you-include",
    "title": "How to ask for help",
    "section": "What should you include?",
    "text": "What should you include?\n\nlibraries used\nyour code and an explanation of what you’re trying to achieve\nthe smallest dataset that still works\nerror messages\nwhat solutions you’ve already tried\ncheck that the code you’re sharing works exactly like your original code with the issue. Sometimes you’ll realise for yourself what the solution is at this point!\ncheck that the code you’re sharing works exactly like your original code with the issue.\n\nDon’t include:\n\nanything confidential\nany code that isn’t essential to the part you’re asking about"
  },
  {
    "objectID": "blog/asking_for_help.html#where-can-you-ask-for-help",
    "href": "blog/asking_for_help.html#where-can-you-ask-for-help",
    "title": "How to ask for help",
    "section": "Where can you ask for help?",
    "text": "Where can you ask for help?\nNHS-R Community Slack forum\nThe Posit forum, hosted by the company that makes and maintains RStudio"
  },
  {
    "objectID": "blog/local-public-health-joins-the-party.html",
    "href": "blog/local-public-health-joins-the-party.html",
    "title": "Local Public Health joins the paRty",
    "section": "",
    "text": "Having recently attended an R meetup in Birmingham, hearing of various user groups and hackathons that take place around certain technologies, I was getting a feeling that there was an increasing desire in the public health community to learn more about R and modern data science tools and techniques. I wondered whether there would be interest in a data science and R user group for the West Midlands public health intelligence community. I thought I’d raise the idea at the West Midlands Public Health Intelligence Group (WMPHIG) when I attended the quarterly meeting, but another attendee beat me to it and doing so confirmed there was some interest. I volunteered to arrange the first user group and Public Health England (PHE) kindly offered assistance.\nBetween us we setup a date and venue for the first meeting at the PHE offices in Birmingham and I was pleased to hear from Nicola at PHE that “…Tickets are selling like hotcakes! “\nNot knowing exactly how the group would best work, we suggested a loose structure for the meeting with the following discussion points:\n\nHow this group should work\n\nAssess current levels of knowledge/experience\n\nR training requirements\n\nR learning methods\n\nPublic health R use examples (including Fingertips R) & wider use examples\n\nWhat could be done with R / What else do people want to do with R\n\nChallenges and issues people have experienced/are experiencing\n\nPossible joint projects that might benefit all members\n\nWe ended up staying reasonably on topic, but there was plenty of useful and engaging discussion around the topics of data science and R. The was a nice mix of novice and more advanced R users (though no one admitted to being an expert 😉 ) in the group. Many of those who were more advanced had fairly recently been novice users. Whilst the more advanced users were able to share their experiences of their learning journeys, others were able to contribute on how we might develop use of data science and R in Public Health Intelligence. I was also impressed with some of the examples of R use that were shared with the group by analysts who have only been using it for a relatively short time. A key point shared was though R may seem a bit daunting at first, its worth jumping in and getting your analytical hands dirty!\nA number of attendees had also managed to attend the NHS-R Community Conference and shared positive experiences of the day and the knowledge they’d picked up.\nEveryone appeared to agree that R and other modern data science tools/methods can offer a lot to public health intelligence. There also appeared to be a desire to work together and help each other out on this learning journey. With that spirit in mind, we have agreed to share code and other useful information on K-Hub (https://khub.net/) and another meeting is going to be arranged for next quarter.\nThanks to all that attended and contributed and to PHE for helping with the organisation.\nThis blog was written by Andy Evans, Senior Officer at Birmingham City Council.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/tracking-and-getting-download-statistics-for-your-r-packages.html",
    "href": "blog/tracking-and-getting-download-statistics-for-your-r-packages.html",
    "title": "Tracking and getting download statistics for your R packages",
    "section": "",
    "text": "I had the privilege of tapping into the R package funding stream to fund my first, and not last, CRAN package entitled NHSDataDictionaRy.\nThe motivation for the package was to provide a consistent way to scrape the live NHS Data Dictionary website. The aim was to allow the lookups to always be up to date and to allow R users in the NHS to quickly get key NHS and medical lookup and reference files. This package also provides other generic web scraping functions that can be utilised with other websites.\nSlight digression – if you have a package idea that you think needs to be developed, then please fill in an NHS-R package proforma to put your proposal to the central team at NHS-R for consideration.\nWho is using my package?\nThe motivation for this quick package wrapper was to find a way to track how often my package had been downloaded from CRAN, as I am a statistician by trade and have also worked in performance analysis, so I like to monitor the downloads from a sanity and return on investment viewpoint. In addition, I am rather curious to know if the package is being successful.\nThe first thing to do is to load the {dlstats} library in to my simple wrapper function to allow for the packages to be summarised in terms of downloads per month and to date. The package list creates three components in the R list, these are:\n\nPlot of downloads over time\nA tibble (a fancy tidy data frame) of the downloads per package per month, if multiple packages are passed to the vector then you will get multiple results you can interrogate\nDownloads_to_date this is a summary of all the downloads to date\nCreating the function wrapper\nThe function wrapper looks as below:\n\n# Load these libraries\nlibrary(ggplot2)\nlibrary(dlstats)\nlibrary(tibble)\n\n# Create the wrapper function\npackage_trackeR &lt;- function(packages) {\n  # Create the downloads for the package\n  dl &lt;- dlstats::cran_stats(c(packages))\n  # Create the plot\n  plot &lt;- ggplot(\n    dl,\n    aes(end, downloads, group = package)\n  ) +\n    geom_line(aes(color = package), linetype = \"dashed\") +\n    geom_point(aes(shape = package, color = package)) +\n    theme_minimal()\n  plot &lt;- plot + xlab(\"Download date\") +\n    ylab(\"Number of downloads\")\n  # Create a list for multiple returns\n  returns_list &lt;- list(\n    \"download_df\" = as_tibble(dl),\n    \"downloads_to_date\" = sum(dl$downloads),\n    \"downloads_plot\" = plot\n  )\n\n  return(returns_list)\n}\n\nTo decompose what this is doing:\n\nThe dl variable uses the {dlstats} package to download a vector of packages. The vector is denoted by the `c()` wrapper, this just allows you to pass multiple packages (as string inputs) to the function\nThe plot creates a plot of all the downloads for the chosen package(s) and displays them on a line chart.\nThe output of this returns the artefacts as stated in the precursory section.\nUsing the function\nTo use, or call, the new function we instantiate or utilise it, as below:\n\n# Call the new function\ntracking &lt;- package_trackeR(c(\"NHSDataDictionaRy\"))\n\nI simply now pass my vector of packages to the function and this returns the following associated outputs.\n\ntracking$download_df\n\n# A tibble: 54 × 4\n   start      end        downloads package          \n   &lt;date&gt;     &lt;date&gt;         &lt;int&gt; &lt;fct&gt;            \n 1 2021-01-01 2021-01-31       129 NHSDataDictionaRy\n 2 2021-02-01 2021-02-28       526 NHSDataDictionaRy\n 3 2021-03-01 2021-03-31       502 NHSDataDictionaRy\n 4 2021-04-01 2021-04-30       155 NHSDataDictionaRy\n 5 2021-05-01 2021-05-31       484 NHSDataDictionaRy\n 6 2021-06-01 2021-06-30       452 NHSDataDictionaRy\n 7 2021-07-01 2021-07-31       571 NHSDataDictionaRy\n 8 2021-08-01 2021-08-31       529 NHSDataDictionaRy\n 9 2021-09-01 2021-09-30       369 NHSDataDictionaRy\n10 2021-10-01 2021-10-31       546 NHSDataDictionaRy\n# ℹ 44 more rows\n\ntracking$downloads_plot\n\n\n\n\n\n\ntracking$downloads_to_date\n\n[1] 15825\n\n\nTo close…\nI hope this simple wrapper can be useful for tracking your packages when they get to CRAN and provide some much needed reassurance that what you are developing is being used in the wild.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/aiming-for-a-wrangle-free-or-reduced-world.html",
    "href": "blog/aiming-for-a-wrangle-free-or-reduced-world.html",
    "title": "Aiming for a wrangle-free (or reduced) world",
    "section": "",
    "text": "I work as a Data Scientist at Public Health England. I am part of a small team that have a role in trying to modernise how we “do” data. I have been an analyst in one way or another for most of my working life. In my role as an analyst, as with most analysts, my biggest focus was on the accuracy of my outputs, but I’ve always got frustrated very quickly with repetitive tasks, which are all too common for analytical roles. In fact, I remember when these frustrations first began. It was during my Masters when, as part of my dissertation, my supervisor asked me to draw a map using ArcMap software. I hadn’t had any previous experience of the software before this moment. Instead of asking for help, which I should have done, I went away and tried to import the files I was given. What I didn’t know was that I didn’t have access to the correct licence to import the files I was using. I thought I was doing something wrong. I did manage to open the files in Excel though and I could see they contained coordinates. I could see that if I interpolated the coordinates of the points, I could create a file that I would be able to upload into ArcMap and it would look like what I was aiming for - the problem was that there were millions of coordinates and they were split over multiple files! This is the moment in my life where I discovered “Record Macro”. I managed to record a few instances of what I wanted to do, and then manipulated the recorded code to repeat the task for everything. I felt very smug going to my supervisor the following week and handing him a map with pink, yellow and red blobs illustrating height contours of a water basin. To say he wasn’t impressed would be an understatement. He pulled up a map on his screen to show me what it should look like. His screen essentially showed what Google Satellite now provides us. My smugness quickly turned to self-doubt.\nIn many ways this example is typical of my experience as an analyst. I have received data in many ways, from people or through systems and databases, but to manipulate (or wrangle, as it is commonly called now) those data to what my manager wanted to see would take a number of days. The data may contain a table for each month, where each month was a different tab in a spreadsheet. If I was lucky each tab would be formatted identically, but more often than not there would be different numbers of columns or rows (sigh). Sometimes, one month might (helpfully) have an extra blank row at the top or maybe some merged cells. I would sit there bringing all those data into one place thinking of myself as part of a sandwich assembly line, picking up all the raw ingredients, assembling them in the right way for somebody else to enjoy. I really wanted to enjoy it! Surely this can be done better and faster. How did analysts on detective shows instantly get the information the senior detective required at the tap of a few keys?\nI first used R 3 years ago. R has completely changed the way that I see data. It has formalised all my previous frustrations. It has words for things that I have thought but could never explain. It encourages data to be “done” properly. Before R I had never heard of an analytical pipeline (I realise this isn’t exclusively an R thing). Everything I had done was about getting data, spending time wrangling it, analysing it and finally presenting it to someone else (for their enjoyment). R gave me R Markdown. Here I could do all of these steps in one script. There was no need for me to write a Word document to sit alongside my Excel workbook to explain where I got the data from, what tabs 2 to 7 do, and why I’ve hard-coded 34.84552 in cell D4. There was no need for me to write step by step instructions for how to draw a bar chart on one axis and a line chart on another within the same graph. The ability to become transparent in my workings was ideal for my lazy nature as the description is written in the code. Not only was my working transparent, it was also completely reproducible. If someone else had access to the same data as me, they could run my script and it would produce the same outputs.\nMy biggest revelation though was being introduced to tidy data. This was my game changer. I had often heard the quote that analysts spent 80% of their time manipulating data and 20% analysing it. That chimed with me. As is written in the paper referenced above:\n“tidy datasets are all alike but every messy dataset is messy in its own way”\nAs the paper describes, tidy data has three features:\nEach variable forms a column. Each observation forms a row. Each type of observational unit forms a table.\nIt is hard to describe or appreciate this really until you think about the dataset you’re working with. Are you really struggling to get it into the format you need to make it easy to work with? The example the paper provides can be seen below:\n\nlibrary(gt)\nlibrary(tidyr)\nlibrary(dplyr)\n\nuntidy_data &lt;- tibble::tribble(\n  ~person, ~treatmentA, ~treatmentB,\n  \"John Smith\",          NA,          2L,\n  \"Jane Doe\",         16L,         11L,\n  \"Mary Johnson\",          3L,          1L\n)\n\n\ngt(untidy_data)\n\n\n\n\n\nperson\ntreatmentA\ntreatmentB\n\n\n\nJohn Smith\nNA\n2\n\n\nJane Doe\n16\n11\n\n\nMary Johnson\n3\n1\n\n\n\n\n\ntidy_data &lt;- untidy_data |&gt; \n  pivot_longer(cols = c(\"treatmentA\", \"treatmentB\"),\n               names_to = \"treatment\",\n               values_to = \"result\") |&gt; \n  arrange(treatment)\n\ngt(tidy_data)\n\n\n\n\n\nperson\ntreatment\nresult\n\n\n\nJohn Smith\ntreatmentA\nNA\n\n\nJane Doe\ntreatmentA\n16\n\n\nMary Johnson\ntreatmentA\n3\n\n\nJohn Smith\ntreatmentB\n2\n\n\nJane Doe\ntreatmentB\n11\n\n\nMary Johnson\ntreatmentB\n1\n\n\n\n\n\n\nFigure 1, the table on above illustrates an untidily formatted table. The table below presents the same data but in tidy format\nAs an analyst, working with tidy data is a rare pleasure. Analytical tasks become seamless as it allows you to use the tidyverse package. Summarising data for groups within your dataset or creating models based on subgroups are an additional one or two lines of understandable code rather than 20 to 30.\nI look forward to a world where tidy data becomes the norm. In this world analysts will be spending 80% of their time analysing the data. We will be using data in a timely fashion, and it will be informing decision making even more than it currently does. We will be combining different datasets to create fuller pictures for the decisions we are informing. We will be learning new techniques for analysing the data rather than new techniques for manipulating them. Wrangling will become a thing of the past and most importantly, we will get to enjoy the sandwich that we’ve made.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-community-conference-organisers-perspective.html",
    "href": "blog/nhs-r-community-conference-organisers-perspective.html",
    "title": "NHS-R Community Conference – organisers’ perspective",
    "section": "",
    "text": "WoW – that’s the best description we can come up with for our third NHS-R Conference 2020 which took place virtually during first two weeks of November 2020.\n\nOver a thousand (n=1043) registrations from 39 countries in 5 continents. 15 amazing workshops and 41 inspiring sessions delivered by 62 speakers. But it is not only about the numbers – we were blown away by the breadth, quality, diversity and imagination of the speakers and our delegates.\n\n\n“It was excellently organised and it was inspiring to hear from so many talented and passionate analysts!”\n\nLisa Cummins, Financial Strategy Analyst, Waltham Forest and East London (WEL) CCG\nNewcomers ask what is R? A nice, clear question but as the conference program shows R is a “general integrated” data science tool that offers practical solutions for a range of data science tasks from mapping covid-19 outbreaks to automate 3000 letters to patients. But it is not just the NHS: conference also featured some amazing talks from various sectors and organisations. From Financial Times and Virgin Media to British Heart Foundation and DHSC: R and open source is the future of the transparent, shareable, and excellent analytics.\n\n“Absolutely fantastic effort by the NHS-R Community. I have seen health systems in 3 Continents and NHS is simply the best and NHS-R is an extension!!”\n\nNighat Khan, Researcher, Edinburgh University\nThe NHS-R Conference offers a pulse check on the use of R in the NHS and the trend is clear. Our first people had 120 registrations, our second – 300. This year we had more than 1,000 registrations, 600 participants attended live and more than 300 re-watched later. If you missed the conference, you still have a chance to catch up. All the talks are recorded and are available on our YouTube channel: please have a look at talks and workshops playlists. They will also be uploaded to our website. Conference materials are being uploaded to our github page.\n\n“Thank you to all the organisers & contributors, it was fantastic & I’ve learned so much!”\n\nLyn Howard, Clinical Network Coordinator, NWC Clinical Network\nOur annual survey had some key data – still 1/3 of analysts regard themselves as novices. Please consider running a workshop for the NHS-R Community and we will do whatever we can to support you!\n\n“The conference was a huge success and I though really highlighted some of the great work that’s going on around the country with R.”\n\nDr Richard Issitt, Clinical Research Fellow, Great Ormond Street Hospital\nOur new venture is as partners with Hexitime. There was a sticky notes session on MIRO – below is a picture of the board – so much energy to mobilise knowledge in our community! On Hexitime we have an “NHS-R Wishing Well” for people to suggest ideas to the community.\n\nThe NHS-R Conference has become the premier data science conference in the NHS and care calendar. It has grown to this because of the engagement, passion and giving of our community members who contribute to together with support from RStudio, MANGO and Jumping Rivers. Every “little” action count’s and you can participate in making the healthcare analytics better.\n\nJoin us on Hexitime\nPlease write a Blog post! You can see the examples on this website, just send a ready version to the nhs.rcommunity@nhs.net\nWe run monthly webinars. If you have something to share, please email us at nhs.rcommunity@nhs.net\nWe have a section in the AphA magazine which contains a short R-profile. If you want to be featured, please drop us a line.\n\nAs you can imagine there was a lot of underwater paddling to support the conference – so a massive thanks to all NHS-R Conference Team and a special mention for Anastasiia who carried the conference on her shoulders with a lovely smile and lovely sense of humour.\nSee you next year!\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-book-club.html",
    "href": "blog/nhs-r-book-club.html",
    "title": "NHS-R Book Club",
    "section": "",
    "text": "“Good friends, good books, and a sleepy conscience; this is a happy life.” – Mark Twain.\nSince lockdown earlier this year, I have had more time to do thing I’ve never had the chance to do before. Every year my new years resolution is to read as many books as I can by the end of the year. That new year resolutions keeps rolling over into next year like the national lottery grand prize.\nWhat we all are going through this year is unprecedented. Anxiety is provoked and this has been the psychological impact of this pandemic many of us can resonate with. As working from home has shared the same meaning as living at work, taking a break is much needed. For me, I very much enjoy learning new skills and have grabbed a few good books.\nHave you ever read the same paragraphs over and over again and time seems to stand still? I can google, tweet or even stackoverflow my confusions and questions from concepts I can’t grasp but since I am a member of NHS-R Community. I built up my courage and dropped a message in a #random channel asking for a reading buddy. And the feedback was welcoming and just like that, our NHS-R Book club was formed.\n\nI was reading The Art of Statistics: Learning from Data by David Spiegelhalter which has become the first book we chose as a group for the book club.  An hour fortnightly virtual get together and reflect on our learning. What I like most from the past two sessions is the diversity of the group yet we all share common interests, which are applying statistical techniques in answering scientific questions.\nI encouraged anyone who likes reading to join because if you like learning, you do not have to learn anything alone. Finding a safe space, a mentor, a buddy whatever label you choose to call. Or I have a better idea, join our #book-club on NHS-R Slack.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-newscast-15th-august.html",
    "href": "blog/nhs-r-newscast-15th-august.html",
    "title": "NHS-R newscast 15th August 2022",
    "section": "",
    "text": "NHS-R conference 2022 – Keynotes: Heather Turner from R Ladies, Jess Morley, and others. Python track. Free to all UK public sector. Fully hybrid remote/ in person\nRstudioconf. Alice Walsh talk on creativity, quilting, and R. Really useful analogy: do you want to be a pizza delivery business, or a plumber?  Pizza delivery – choose from a picture, it arrives and looks like it should. We do a lot of dashboard pizza – asked for a chart here, there, summary table, no creativity. Plumber is domain expert. Turns up and asks questions about the problem. Brings tools and fittings to fix, but doesn’t focus on them. Focus on understanding the customer’s problem. Then gets on with work, and trusted to deliver result.\nSoftware carpentry and linting.\nRStudio becomes Posit to broaden focus to Python and VS Code\nVS Code for R. Live share. Git lens. GitHub co pilot https://twitter.com/_ColinFay/status/1556656199885529090?s=20&t=V-MENMBfJLbBbpLGVh4cVA\nWe had some feedback from the drop in that it should be easier to find training materials. GitHub can be quite confusing for beginners and in particular it is actively blocked in some NHS Trusts. We need to have a top level “training” view on the main website which has clear signposting to what all the courses are and where to find them. We will take this feedback away and try to work on it."
  },
  {
    "objectID": "blog/nhs-r-newscast-15th-august.html#bullet-summary-of-the-most-recent-nhs-r-podcast",
    "href": "blog/nhs-r-newscast-15th-august.html#bullet-summary-of-the-most-recent-nhs-r-podcast",
    "title": "NHS-R newscast 15th August 2022",
    "section": "",
    "text": "NHS-R conference 2022 – Keynotes: Heather Turner from R Ladies, Jess Morley, and others. Python track. Free to all UK public sector. Fully hybrid remote/ in person\nRstudioconf. Alice Walsh talk on creativity, quilting, and R. Really useful analogy: do you want to be a pizza delivery business, or a plumber?  Pizza delivery – choose from a picture, it arrives and looks like it should. We do a lot of dashboard pizza – asked for a chart here, there, summary table, no creativity. Plumber is domain expert. Turns up and asks questions about the problem. Brings tools and fittings to fix, but doesn’t focus on them. Focus on understanding the customer’s problem. Then gets on with work, and trusted to deliver result.\nSoftware carpentry and linting.\nRStudio becomes Posit to broaden focus to Python and VS Code\nVS Code for R. Live share. Git lens. GitHub co pilot https://twitter.com/_ColinFay/status/1556656199885529090?s=20&t=V-MENMBfJLbBbpLGVh4cVA\nWe had some feedback from the drop in that it should be easier to find training materials. GitHub can be quite confusing for beginners and in particular it is actively blocked in some NHS Trusts. We need to have a top level “training” view on the main website which has clear signposting to what all the courses are and where to find them. We will take this feedback away and try to work on it."
  },
  {
    "objectID": "blog/the-first-ever-train-the-trainer-class-2019.html",
    "href": "blog/the-first-ever-train-the-trainer-class-2019.html",
    "title": "The first ever train the trainer class 2019",
    "section": "",
    "text": "The Strategy Unit, AphA (Association of Professional Healthcare Analysts) and NHS-R joined forces to deliver the first ever train the trainer (TTT) class on the 11-12th December 2019 in Birmingham. We allocated spaces for two AphA members per branch and in the end we had 18 people from across the UK NHS, including clinical and non-clinical staff who worked as commissioners, providers or national NHS bodies.\nAs this was the first ever TTT course, we were quite unsure how things would turn out, especially because teaching R is not easy. One key issue is that trainers have to think about the background of novices to R – this can range from no programming experience at all to years of SQL experience. Two other challenges with R are that there are many ways of doing things (for example using base R versus tidyverse) and installation can be tricky for some.\nTo address these challenges, we designed the course to incorporate considerable emphasis on evidence based teaching practice, plenty of learning by doing and sharing underpinned by the use of liberating structures techniques to capture the voice of delegates. The aim was to give delegates the skills, confidence and capability to deliver an introduction to R for those in healthcare, based on a field tested slide deck.\nThe class was fabulous – they were engaged and enthusiastic, learned a great deal from each other and the facilitators and made great progress.\n\nIndeed it was inspiring to see how NHS staff can work effectively together, despite being from different organisations. Our first class have self-organised into an action learning set who will: train others, learn from each other, feedback their learning into the NHS-R Community, and have an annual TTT Class of 2019 reunion!\nWhilst there are areas where we would refine the TTT, we were pleased with the first iteration and feel confident that we have a committed and capable set of trainers who can deliver the Introduction to R for those in healthcare at locations near you.\nIf you are interested in hosting an Introduction to R training session or being involved in future iterations of the TTT, please get in touch via nhs.rcommunity@nhs.net and we will see what can be done (no promises).\nThis blog was written by Professor Mohammed A Mohammed, Principal Consultant at the Strategy Unit\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/dont-repeat-yourself-functions.html",
    "href": "blog/dont-repeat-yourself-functions.html",
    "title": "Don’t Repeat Yourself!",
    "section": "",
    "text": "One of the greatest benefits of using R over spreadsheets is that it’s very easy to re-use and repurpose code, for example if we need to produce the same chart over and over again, but for different cuts of the data.\nLet’s imagine that we are trying to create a plot for arrivals to A&E departments using the ae_attendances dataset from the {NHSRdatasets} package.\nCreating our first plot\nFirst we want to create a plot for all of England’s A&E departments over the last 3 financial years.\n\nlibrary(tidyverse)\nlibrary(NHSRdatasets)\n\nae_attendances %&gt;%\n  group_by(period) %&gt;%\n  # summarise at is a shorthand way of writing something like\n  #   summarise(column = function(column))\n  # first you specify the columns (one or more) in the vars() function (short\n  # for variables), followed by the function that you want to use. You can\n  # then add any additional arguments to the function, like below I pass\n  # na.rm = TRUE to the sum function.\n  summarise_at(vars(attendances), sum, na.rm = TRUE) %&gt;%\n  ggplot(aes(period, attendances)) +\n  geom_point() +\n  geom_line() +\n  scale_x_date(date_breaks = \"6 months\", date_labels = \"%b-%y\") +\n  labs(\n    x = \"Month of Attendance\",\n    y = \"Number of Attendances\",\n    title = \"Attendances to A&E Departments by Month\",\n    subtitle = \"All A&E departments in England\"\n  )\n\n\n\n\n\n\n\nCreating a second plot\nNow, what if we wanted to run this for just a single trust? We could copy and paste the code, then add in a filter to a specific trust.\n\n# of course, you would usually more specifically choose which organisation we\n# are interested in! Selecting the first organisation for illustrative purposes.\n# The pull function grabs just the one column from a data frame, we then use\n# head(1) to select just the first row of data, and finally ensure that we\n# convert this column from a factor to a character\nfirst_org_code &lt;- ae_attendances %&gt;%\n  pull(org_code) %&gt;%\n  head(1) %&gt;%\n  as.character()\n\nae_attendances %&gt;%\n  filter(org_code == first_org_code) %&gt;%\n  group_by(period) %&gt;%\n  summarise_at(vars(attendances), sum) %&gt;%\n  ggplot(aes(period, attendances)) +\n  geom_point() +\n  geom_line() +\n  scale_x_date(date_breaks = \"6 months\", date_labels = \"%b-%y\") +\n  labs(\n    x = \"Month of Attendance\",\n    y = \"Number of Attendances\",\n    title = \"Attendances to A&E Departments by Month\",\n    subtitle = paste(\"org_code =\", first_org_code)\n  )\n\n\n\n\n\n\n\nSo, what changed between our first plot and the second? Well, we’ve added a line to filter the data, and changed the subtitle, but that’s it. The rest of the code is repeated.\nCreating yet another copy of the first plot\nLet’s say we want to run this code again and create a plot for another organisation. So again, let’s copy and paste.\nBut perhaps at this point we also decide that we want the label’s on the y-axis to use comma number formatting, we want to change the dots and lines to bars, and we want to colour the bars in NHS Blue.\n\n# the scales package has nice functions for neatly formatting chart axes\nlibrary(scales)\n\n# again, just selecting an organisation for illustrative purposes only.\n# This time, we use tail instead of head to select the final row\nsecond_org_code &lt;- ae_attendances %&gt;%\n  pull(org_code) %&gt;%\n  tail(1) %&gt;%\n  as.character()\n\nae_attendances %&gt;%\n  filter(org_code == second_org_code) %&gt;%\n  group_by(period) %&gt;%\n  summarise_at(vars(attendances), sum) %&gt;%\n  ggplot(aes(period, attendances)) +\n  geom_col(fill = \"#005EB8\") +\n  scale_x_date(date_breaks = \"6 months\", date_labels = \"%b-%y\") +\n  scale_y_continuous(labels = comma) +\n  labs(\n    x = \"Month of Attendance\",\n    y = \"Number of Attendances\",\n    title = \"Attendances to A&E Departments by Month\",\n    subtitle = paste(\"org_code =\", second_org_code)\n  )\n\n\n\n\n\n\n\nNow, we want to go back and change the rest of the plots to have the same look and feel. Well, you will have to go back up and change those plots individually, which when there’s just 3 plots then so what? It’s easy enough to go back and change those!\nBut what if it’s 300 plots? Or, what if those 3 plots are in 3 different places in a very large report? What if those 3 plots are in separate reports? What if it wasn’t just a handful of lines code we are adding but lots of lines?\nCreating functions\nThis is where we should start to think about extracting the shared logic between the different plot’s into a function. This is sometimes called “DRY” for “Don’t Repeat Yourself”. Where possible we should aim to eliminate duplication in our code.\nIn R it’s pretty simple to create a function. Here’s a really simple example:\n\nmy_first_function &lt;- function(x) {\n  y &lt;- 3 * x\n  y + 1\n}\n\nThis creates a function called my_first_function: you assign functions just like any other variable in R by using the &lt;- assignment operator. You then type the keyword function which is immediately followed by a pair of parentheses. Inside the parentheses you can name “arguments” that the function takes (zero or more), then finally a set of curly brackets, { and }, which contain the code you want to execute (the function’s body).\nThe functions body can contain one or more lines of code. Whatever line of code is executed last is what is returned by the function. In the example above, we first create a new variable called y, but we return the value of y + 1.\nThe values that we create inside our function (in this case, y) only exist within the function, and they only exist when the function is called (so subsequent calls of the function don’t see previous values).\nWe can then simply use our function like so:\n\nmy_first_function(3)\n\n[1] 10\n\n\nWhich should show the value “10” in the console.\nConverting our plot code to a function\nThe first thing we should look to do is see what parts of the code above are identical, which parts are similar but change slightly between calls, and which parts are completely different.\nFor example, in our plot above, each example uses the same data summarisation, and the same call to {ggplot}. We slightly changed how we were displaying our charts (we started off with geom_point() and geom_line(), but changed to geom_col() in the third plot). Let’s go with the chart used in the third version as our base plot.\nThe subtitle’s differ slightly between the 3 plots, but we could extract this to be an argument to the function. So my first attempt at converting this plot to a function might be:\n\nae_plot &lt;- function(data, subtitle) {\n  data %&gt;%\n    group_by(period) %&gt;%\n    summarise_at(vars(attendances), sum) %&gt;%\n    ggplot(aes(period, attendances)) +\n    geom_col(fill = \"#005EB8\") +\n    scale_x_date(date_breaks = \"6 months\", date_labels = \"%b-%y\") +\n    scale_y_continuous(labels = comma) +\n    labs(\n      x = \"Month of Attendance\",\n      y = \"Number of Attendances\",\n      title = \"Attendances to A&E Departments by Month\",\n      subtitle = subtitle\n    )\n}\n\nWe can now create our first 3 plots as before:\n\nae_plot(ae_attendances, \"All A&E departments in England\")\n\n\n\n\n\n\n\n\n# as ae_plot's first argument is the data, we can use the %&gt;% operator to pass in the data like so:\nae_attendances %&gt;%\n  filter(org_code == first_org_code) %&gt;%\n  ae_plot(paste(\"org_code =\", first_org_code))\n\n\n\n\n\n\n\n\nae_attendances %&gt;%\n  filter(org_code == second_org_code) %&gt;%\n  ae_plot(paste(\"org_code =\", second_org_code))\n\n\n\n\n\n\n\nNow, we’ve managed to remove most of the duplication in our code! If we decide we no longer like the blue points and line we can easily change the function, or if we want to switch to a bar chart instead of the line chart we only have to update the code once; when we re-run our code all of the plots will change.\nOf course, this leads to it’s own problems: what if we want 3 charts to have blue points but one use red? We could either add a colour argument to the function, or we could remove the logic which adds the points and lines to the chart but does everything else: then we could just add the points on at the end (or, create a red function and a blue function; each function would first call the main function before doing their own stuff).\nIn Summary\nFunctions allow us to group together sections of code that are easy to reuse, they make our code easier to maintain, because we only have to update code in one place, and they reduce errors by limiting the amount of code we have.\nFurther Reading\nThis file was generated using RMarkdown, you can grab the .Rmd file as a GitHub gist.\nHopefully this has been a useful introduction to functions, if you are interested in learning more then the R4DS book has an excellent chapter on functions.\nOnce you have mastered writing functions then you might want to read up on tidyeval: this allows you to write functions like you find in the {tidyverse} where you can specify the names of columns in dataframes.\nYou may also want to have a go at object orientated programming, which is covered in the Advanced R book.\nAny time you see yourself copying and pasting code try to remember, Don’t Repeat Yourself!\nTom is a Senior Data Scientist working at the Strategy Unit.\nThis blog has been edited for NHS-R Style\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-community-member-profile.html",
    "href": "blog/nhs-r-community-member-profile.html",
    "title": "Member profile - Robin Hinks",
    "section": "",
    "text": "R is a powerful tool for manipulating health and care data and a lot can be learned from sharing our experiences of using R with others. We bring to you an NHS-R profile from one of our Community members, to share their insider knowledge of using R…\nHow did you first meet R?\nWhile working as a civil service analyst, where I was encouraged to undertake self-directed learning in R to conduct statistical and geographic analysis.\n\nWhat sort of things do you use R for and what do you love about R?\n\nThrough previous roles – where I have done quality assurance and validation of other research teams’ work – I know the value of well-documented analytical process, and the dangers of poor record keeping! I love how R allows you to keep all your analysis, research notes and – through R Markdown – reporting in one place.\n\nWhat do you hate about R?\n\nI have a qualitative research background and using R has been my first real exposure to code development. While I found the move from, say, SPSS’ ‘point and click’ environment easy enough, I have found it difficult to get my head round the wider world of code development that surrounds r: learning about pulls, commits, splits and the like has been challenging!\n\nWhat are your top tips for using R?\n\nStart by coding up some tasks you already have a process for elsewhere – for example automating some data transformations you’ve previously done in a programme like SPSS or Excel. Working out how to translate a task into R’s environment is a lot easier that starting from a blank slate.\n\nCan you please name a project where you have used R? Briefly describe what this involves.\n\nHealth data and statistics are reported against a range of geographies that do not easily match up with the political geographies our members might seek to influence – for example parliamentary constituencies or local authorities. I’ve used R to develop look up tables between different geographic areas; and using the leaflet package visually map different geographic area,\ndeveloping simple choropleth and point maps for internal insight work.\nThis blog has been formatted to remove Latin Abbreviations\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/the-nhs-r-community-and-hexitime-our-1st-month-of-collaborating.html",
    "href": "blog/the-nhs-r-community-and-hexitime-our-1st-month-of-collaborating.html",
    "title": "The NHS-R Community and Hexitime – Our 1st month of collaborating",
    "section": "",
    "text": "Last month at our annual event we announced an exciting partnership between the NHS-R Community and the collaboration platform Hexitime. At the conference many of you joined an interactive session where we shared some of the challenges we face at work and how we might be able to mobilise our network to help each other tackle them.\nThe workshop clearly sparked some useful reflection and great ideas, because within an hour 50 members of the community had already registered and begun to upload their offers, requests and challenges!\nWe wanted to explore everybody’s efforts a month later and understand how the partnership was progressing. We are delighted to be able to share with you the following map of NHS-R members supporting each other and benefiting from the platform. The map audits our first 3 weeks together.\n\n\n\nNHS-R community members collaborating in action\n\n\nSo, first reflections;\n\nThere is scope to use the opportunity as much or as little as you need. Some members have already notched up over 10 exchanges in the first few weeks, and this can include earning credits by helping others as much as spending them to bring in support you need.\nNHS-R members are starting to benefit from skill exchanges outside the community. Support with literature reviews, board level Statistical Process Control (SPC) reporting, career coaching and understanding system length of stay modelling are some of the examples where NHS-R members are receiving such support for free.\nMembers are starting to create exchange ‘chains’, where credits of support are flowing between members who may not know each other. This is quite powerful and will really help us to mobilise the network in the future. So, what may feel like a small offer of help is probably contributing to a much larger value chain in the network.\nThe map shows about 20 active NHS-R community members reaching out and sharing their skills through the platform. They’ve had a productive 3 weeks – but imagine what this would look like with a few hundred NHS-R members using it!\n\nWe also want to introduce Beth Taylor to the community, who will be supporting our partnership moving forwards:\n\nI would be keen to hear about any interesting exchanges you have made on Hexitime to share with the rest of the community – get in touch with your stories! I am looking forward to utilising Hexitime to develop my own skills in R.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/success-story-lydia-briggs.html",
    "href": "blog/success-story-lydia-briggs.html",
    "title": "Success story - Dr Lydia Briggs, Data Scientist, GOSH",
    "section": "",
    "text": "What was the problem/challenge you were trying to address?\nHospital surgical teams regularly hold meetings to discuss cases and events that have happened in their departments. The patient data to be discussed can be complicated and varied, requiring analysts to manually go through many notes, reports and summaries in the electronic patient record system. By developing an extraction process and incorporating generalizable coding with R, this lengthy and manual process can be automated and can save valuable hours of analyst time.\n\n\nHow has R helped you? Any particular libraries/products/packages you found the most useful?\nR is a useful tool for this project as it allows for easy linkage of different datasets at the patient level and allows for informative visualisation. For example, a patient who is currently in the cardiology ward can easily be linked to their hospital admission history, previous procedures, laboratory results and so on and the breadth of this information can be shown in an interactive time line plot using ggplot2 and plotly. As the data is extracted on a weekly basis, a {targets} pipeline has been beneficial in saving processing time and in compartmentalising the features and functions.\n\n\nWhat is the result?\nWe developed a targets pipeline which outputs a markdown report and a shiny app which displays the features required by the hospital department. A shiny app allows the department to interact with the data through selecting features and plotly timeline visualisations. Due to the ease of joining datasets together using R, more information can be provided and presented in the department meetings in an easy to understand format.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/cqc-data.html",
    "href": "blog/cqc-data.html",
    "title": "CQC directory data vignette",
    "section": "",
    "text": "The following was submitted to the NHSRdatasets package in 2020 but was too big as data to be accepted by the package (CRAN has size limits). However, the vignette that accompanied the data may be of interest and so is copied here as a blog. The data is also pulled from the GitHub Pull Request."
  },
  {
    "objectID": "blog/cqc-data.html#cqc-directory",
    "href": "blog/cqc-data.html#cqc-directory",
    "title": "CQC directory data vignette",
    "section": "CQC Directory",
    "text": "CQC Directory\nThis vignette explains how to use the cqc_load dataset in R, and also details where it comes from and how it is generated.\nThe data is sourced from the Care Quality Commission and its archive is available on Google Drive.\nThe data contains care provider locations and ratings as published monthly in 2019.\nThe dataset contains data on locations and providers:\n\n\nlocation: location ID, name, ODS code, telephone number, web address, sector, inspection directorate, region, local authority, CCG, street address, parliamentary constituency, and coordinates.\n\nprovider: provider ID, name, company/charity registration, telephone number, web address, sector, inspection directorate, region, local authority, street address, and parliamentary constituency. code for the organisation that this activity relates to\n\npub_date: The date that particular data point was published\n\nFirst let’s load some packages and the dataset and show the first 10 rows of data.\n\n\n\n\n\n\nData location\n\n\n\n\n\nThe following code has had all references to the NHSRdatasets package removed as the data wasn’t added to it.\n\n\n\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\nlibrary(magrittr)\n\n# Although the data isn't in NHSRdatasets it can be downloaded from the Pull Request using this code\n\nload(url(\"https://github.com/MHWauben/NHSRdatasets/raw/274688bc727e800d5c40f06f86be38f866167818/data/cqc_directory.rda\"))\n\n# format for display\ncqc_load %&gt;%\n  # show the first 10 rows\n  head(10) %&gt;%\n  # format as a table\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation_id\nlocation_hsca_start_date\ncare_home\nlocation_name\nlocation_ods_code\nlocation_telephone_number\nregistered_manager_note_where_there_is_more_than_one_manager_at_a_location_only_one_is_included_here_for_ease_of_presentation_the_full_list_is_available_if_required\nlocation_web_address\ncare_homes_beds\nlocation_type_sector\nlocation_inspection_directorate\nlocation_primary_inspection_category\nlocation_latest_overall_rating\npublication_date\nlocation_region\nlocation_local_authority\nlocation_onspd_ccg_code\nlocation_onspd_ccg\nlocation_commissioning_ccg_code\nlocation_commissioning_ccg\nlocation_street_address\nlocation_address_line_2\nlocation_city\nlocation_county\nlocation_postal_code\nlocation_latitude\nlocation_longitude\nlocation_parliamentary_constituency\nbrand_id\nbrand_name\nprovider_companies_house_number\nprovider_charity_number\nprovider_id\nprovider_name\nprovider_hsca_start_date\nprovider_type_sector\nprovider_inspection_directorate\nprovider_primary_inspection_category\nprovider_ownership_type\nprovider_telephone_number\nprovider_web_address\nprovider_street_address\nprovider_address_line_2\nprovider_city\nprovider_county\nprovider_postal_code\nprovider_local_authority\nprovider_region\nprovider_latitude\nprovider_longitude\nprovider_parliamentary_constituency\nprovider_nominated_individual_name\npub_date\n\n\n\n1-1000210669\n2013-12-12\nY\nKingswood House Nursing Home\nVM4G6\n01424716303\n*\nNA\n22\nSocial Care Org\nAdult social care\nResidential social care\nRequires improvement\n2018-08-04\nSouth East\nEast Sussex\nE38000076\nNHS Hastings and Rother CCG\nNA\nNA\n21-23 Chapel Park Road\nNA\nSt Leonards On Sea\nEast Sussex\nTN37 6HR\n50.857239\n0.561998\nHastings and Rye\nBD398\nBRAND Innomary\n08558638\nNA\n1-877912132\nInnowood Limited\n2013-12-12\nSocial Care Org\nAdult social care\nResidential social care\nOrganisation\n02089079713\nNA\n62 Northwick Avenue\nNA\nHarrow\nMiddlesex\nHA3 0AB\nBrent\nLondon\n51.579057\n-0.320762\nBrent North\nSaluguti, Vikas\n2019-01-01\n\n\n1-1000312641\n2013-10-18\nN\nHuman Support Group Limited - Sale\nVN30H\n01619429490\nBuckley, Michelle\nwww.humansupportgroup.co.uk\n0\nSocial Care Org\nAdult social care\nCommunity based adult social care services\nGood\n2018-11-17\nNorth West\nTrafford\nE38000187\nNHS Trafford CCG\nNA\nNA\n59 Cross Street\nNA\nSale\nCheshire\nM33 7HF\n53.427726\n-2.323301\nAltrincham and Sale West\nBD409\nBRAND Human Support Group\n03513906\nNA\n1-101693918\nThe Human Support Group Limited\n2010-10-01\nSocial Care Org\nAdult social care\nCommunity based adult social care services\nOrganisation\n01619429487\nwww.humansupportgroup.co.uk\nCraig House, 33 Ballbrook Avenue\nDidsbury\nManchester\nLancashire\nM20 3JG\nManchester\nNorth West\n53.425083\n-2.235246\nManchester, Withington\nMason, Glen\n2019-01-01\n\n\n1-1000401911\n2013-11-04\nY\nLittle Haven\nVL05L\n02086974246\nMuriuki, Martin\nNA\n15\nSocial Care Org\nAdult social care\nResidential social care\nGood\n2018-04-11\nLondon\nLewisham\nE38000098\nNHS Lewisham CCG\nNA\nNA\n133 Wellmeadow Road\nNA\nLondon\nNA\nSE6 1HP\n51.441104\n0.002335\nLewisham East\n-\n-\n04174819\nNA\n1-101666779\nElizabeth Peters Care Homes Limited\n2010-10-01\nSocial Care Org\nAdult social care\nResidential social care\nOrganisation\n02086985296\nNA\n14 Canadian Avenue\nNA\nLondon\nNA\nSE6 3AS\nLewisham\nLondon\n51.440913\n-0.02297\nLewisham East\nMuriuki, Martin\n2019-01-01\n\n\n1-1000587219\n2013-11-04\nY\nHighlands Borders Care Home\nVM44K\n01392491261\nMartin, Fiona\nNA\n18\nSocial Care Org\nAdult social care\nResidential social care\nGood\n2017-07-20\nSouth West\nDevon\nE38000129\nNHS Northern, Eastern and Western Devon CCG\nNA\nNA\n22 Salutary Mount\nHeavitree\nExeter\nDevon\nEX1 2QE\n50.721541\n-3.508053\nExeter\n-\n-\n06612312\nNA\n1-101693962\nHighlands Care Home Limited\n2010-10-01\nSocial Care Org\nAdult social care\nResidential social care\nOrganisation\n01392431122\nwww.highlandscarehome.co.uk\n56 St Leonards Road\nNA\nExeter\nDevon\nEX2 4LS\nDevon\nSouth West\n50.718254\n-3.521576\nExeter\nZhang, Danadanqi\n2019-01-01\n\n\n1-1000711804\n2013-12-12\nY\nBelmont Grange Nursing and Residential Home\nVM4GQ\n01913849853\nUrwin, Kelly-Ann\nNA\n30\nSocial Care Org\nAdult social care\nResidential social care\nGood\n2017-03-01\nNorth East\nCounty Durham\nE38000116\nNHS North Durham CCG\nNA\nNA\nBroomside Lane\nNA\nDurham\nCounty Durham\nDH1 2QW\n54.786148\n-1.52816\nCity of Durham\n-\n-\n07470626\nNA\n1-241243645\nPerfect Care Limited\n2011-10-05\nSocial Care Org\nAdult social care\nResidential social care\nOrganisation\n01388420145\nwww.perfectcare.co.uk\n10-12 High Street\nNA\nSpennymoor\nCounty Durham\nDL16 6DB\nCounty Durham\nNorth East\n54.69881\n-1.601821\nBishop Auckland\nMoran, Barbara\n2019-01-01\n\n\n1-1001764404\n2013-10-11\nN\nEverycare Midsussex\nVNA9N\n01444244770\nParsons, Katie\nwww.everycare.co.uk/midsussex\n0\nSocial Care Org\nAdult social care\nCommunity based adult social care services\nGood\n2018-10-20\nSouth East\nWest Sussex\nE38000083\nNHS Horsham and Mid Sussex CCG\nNA\nNA\n191-193 London Road\nNA\nBurgess Hill\nNA\nRH15 9RN\n50.956288\n-0.139426\nMid Sussex\n-\n-\n08547200\nNA\n1-962890981\nCura Muneris Limited\n2013-10-11\nSocial Care Org\nAdult social care\nCommunity based adult social care services\nOrganisation\n01444244770\nNA\n191-193 London Road\nNA\nBurgess Hill\nNA\nRH15 9RN\nWest Sussex\nSouth East\n50.956288\n-0.139426\nMid Sussex\nDimelow, David\n2019-01-01\n\n\n1-1001764472\n2013-10-28\nN\nCherish UK Ltd\nVN2E0\n01253766888\nStockell, Sam\nwww.cherishuk.co.uk\n0\nSocial Care Org\nAdult social care\nCommunity based adult social care services\nGood\n2017-10-12\nNorth West\nBlackpool\nE38000015\nNHS Blackpool CCG\nNA\nNA\n8 Skyways Commercial Centre\nBlackpool Business Park, Amy Johnson Way\nBlackpool\nLancashire\nFY4 3RS\n53.777788\n-3.025486\nBlackpool South\n-\n-\n05435700\nNA\n1-101680050\nCherish UK Limited\n2011-01-25\nSocial Care Org\nAdult social care\nCommunity based adult social care services\nOrganisation\n01253766888\nwww.cherishuk.co.uk\n8 Skyways Commercial Centre\nBlackpool Business Park, Amy Johnson Way\nBlackpool\nLancashire\nFY4 3RS\nBlackpool\nNorth West\n53.777788\n-3.025486\nBlackpool South\nWatson, Wendy\n2019-01-01\n\n\n1-1001764512\n2013-10-14\nN\nOptical Express - Bluewater Clinic\nNA\n08000232020\nLeadley, Robert\nwww.opticalexpress.co.uk/store/kent-bluewater-shopping-centre.html\n0\nIndependent Healthcare Org\nHospitals\nAcute hospital - Independent specialist\nNA\nNA\nSouth East\nKent\nE38000043\nNHS Dartford, Gravesham and Swanley CCG\nNA\nNA\nUnit L40, Lower Thames Walk\nBluewater\nGreenhithe\nKent\nDA9 9SJ\n51.438086\n0.271001\nDartford\nBD142\nBRAND Optical Express\nSC161469\nNA\n1-983441221\nOptical Express Limited\n2013-10-14\nIndependent Healthcare Org\nHospitals\nAcute hospital - Independent specialist\nOrganisation\n01236723300\nwww.opticalexpress.com\nThe Avenue\nCliftonville\nNorthampton\nNothamptonshire\nNN1 5BT\nNorthamptonshire\nEast Midlands\n52.236378\n-0.877334\nNorthampton South\nSpellman, Mary\n2019-01-01\n\n\n1-1001765343\n2013-10-14\nN\nOptical Express - Cambridge Clinic\nNA\n08000232020\nLeadley, Robert\nwww.opticalexpress.co.uk/store/cambridge-petty-cury.html\n0\nIndependent Healthcare Org\nHospitals\nAcute hospital - Independent specialist\nNA\nNA\nEast of England\nCambridgeshire\nE38000026\nNHS Cambridgeshire and Peterborough CCG\nNA\nNA\n39-41 Petty Cury\nNA\nCambridge\nCambridgeshire\nCB2 3NB\n52.205264\n0.1203\nCambridge\nBD142\nBRAND Optical Express\nSC161469\nNA\n1-983441221\nOptical Express Limited\n2013-10-14\nIndependent Healthcare Org\nHospitals\nAcute hospital - Independent specialist\nOrganisation\n01236723300\nwww.opticalexpress.com\nThe Avenue\nCliftonville\nNorthampton\nNothamptonshire\nNN1 5BT\nNorthamptonshire\nEast Midlands\n52.236378\n-0.877334\nNorthampton South\nSpellman, Mary\n2019-01-01\n\n\n1-1001875873\n2013-10-14\nN\nOptical Express - Leeds (Albion Street) Clinic\nNA\n08702202020\nSaward, Louise\nwww.opticalexpress.co.uk/store/leeds-airedale-house.html\n0\nIndependent Healthcare Org\nHospitals\nAcute hospital - Independent specialist\nNA\nNA\nYorkshire and The Humber\nLeeds\nE38000225\nNHS Leeds CCG\nNA\nNA\n6th Floor, Airedale House\n77-85 Albion Street\nLeeds\nWest Yorkshire\nLS1 5AW\n53.798527\n-1.545292\nLeeds Central\nBD142\nBRAND Optical Express\nSC161469\nNA\n1-983441221\nOptical Express Limited\n2013-10-14\nIndependent Healthcare Org\nHospitals\nAcute hospital - Independent specialist\nOrganisation\n01236723300\nwww.opticalexpress.com\nThe Avenue\nCliftonville\nNorthampton\nNothamptonshire\nNN1 5BT\nNorthamptonshire\nEast Midlands\n52.236378\n-0.877334\nNorthampton South\nSpellman, Mary\n2019-01-01\n\n\n\n\n\nWe can calculate the total number of each type of rating at each datapoint like this:\n\nrat_overtime &lt;- cqc_load %&gt;%\n  group_by(pub_date, location_latest_overall_rating) %&gt;%\n  summarise(number = dplyr::n()) %&gt;%\n  filter(!is.na(location_latest_overall_rating)) %&gt;%\n  ungroup() %&gt;%\n  mutate(pub_date = as.Date(pub_date))\n\nrat_overtime %&gt;%\n  pivot_wider(\n    id_cols = location_latest_overall_rating,\n    names_from = pub_date,\n    values_from = number\n  ) %&gt;%\n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlocation_latest_overall_rating\n2019-01-01\n2019-02-01\n2019-03-01\n2019-04-01\n2019-05-01\n2019-06-01\n2019-07-01\n2019-08-01\n2019-09-01\n2019-10-01\n2019-11-01\n2019-12-01\n\n\n\nGood\n25000\n25015\n25059\n25176\n25192\n25395\n25549\n25676\n25816\n25923\n25937\n26083\n\n\nInadequate\n366\n363\n378\n384\n395\n394\n399\n410\n419\n425\n436\n434\n\n\nOutstanding\n1163\n1181\n1205\n1251\n1282\n1302\n1322\n1358\n1391\n1410\n1442\n1464\n\n\nRequires improvement\n4013\n4027\n4036\n4017\n3975\n3940\n3944\n3969\n3972\n3997\n4047\n4086\n\n\n\n\n\nWe can now plot, for each month, the proportion of locations that received each rating:\n\nggplot(rat_overtime, aes(pub_date, number, fill = location_latest_overall_rating)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  theme_minimal() +\n  labs(\n    x = \"Data publication date\",\n    y = \"Proportion\",\n    fill = \"CQC rating\",\n    title = \"CQC ratings as published\",\n    caption = \"Source: Care Quality Commission\"\n  )\n\n\n\n\n\n\n\nAlthough the raw numbers show increases in each rating, this is primarily driven by more and more locations becoming available: proportionally, performance has remained fairly stable at a national level.\nWe can further break this down by region to better understand what is happening:\n\nregion_ratings_overtime &lt;- cqc_load %&gt;%\n  group_by(pub_date, location_region, location_latest_overall_rating) %&gt;%\n  summarise(number = dplyr::n()) %&gt;%\n  filter(!is.na(location_latest_overall_rating) & !(location_region %in% c(\"Unspecified\", \"(pseudo) Wales\"))) %&gt;%\n  ungroup() %&gt;%\n  mutate(pub_date = as.Date(pub_date))\n\nggplot(region_ratings_overtime, aes(pub_date, number, fill = location_latest_overall_rating)) +\n  geom_bar(stat = \"identity\", position = \"fill\") +\n  theme_minimal() +\n  facet_wrap(~location_region) +\n  labs(\n    x = \"Data publication date\",\n    y = \"Proportion\",\n    fill = \"CQC rating\",\n    title = \"CQC ratings as published\",\n    caption = \"Source: Care Quality Commission\"\n  )\n\n\n\n\n\n\n\nMore interesting patterns emerge now: some regions have growing proportions of ‘Good’ or ‘Outstanding’ locations, whereas others are seeing increases in locations requiring improvement."
  },
  {
    "objectID": "blog/cqc-data.html#how-many-new-care-homes-are-being-opened",
    "href": "blog/cqc-data.html#how-many-new-care-homes-are-being-opened",
    "title": "CQC directory data vignette",
    "section": "How many new care homes are being opened?",
    "text": "How many new care homes are being opened?\nWith an ageing population, there is increasing demand for care home beds. Is supply keeping up?\n\ncarehome_openings &lt;- cqc_load %&gt;%\n  mutate(location_hsca_start_date = as.Date(location_hsca_start_date)) %&gt;%\n  filter(care_home == \"Y\" & location_hsca_start_date &gt;= \"2014-01-01\" &\n    !(location_region %in% c(\"Unspecified\", \"(pseudo) Wales\")) &\n    pub_date == max(as.Date(pub_date))) %&gt;%\n  mutate(location_month = floor_date(location_hsca_start_date, unit = \"month\")) %&gt;%\n  group_by(location_month) %&gt;%\n  summarise(\n    number = n(),\n    beds = sum(as.integer(care_homes_beds))\n  ) %&gt;%\n  mutate(\n    location_year = as.factor(format(floor_date(location_month, unit = \"year\"), \"%Y\")),\n    month = format(location_month, \"%m\")\n  )\n\nggplot(carehome_openings, aes(x = month, y = number, colour = location_year, group = location_year)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nIt is likely that the 2019 timeseries drops off due to a delay between provision starting and CQC reporting data. However, overall the number of new care homes opening is not increasing! Perhaps each care home is getting bigger; how many new beds are becoming available?\n\nggplot(carehome_openings, aes(x = month, y = beds, colour = location_year, group = location_year)) +\n  geom_line() +\n  theme_minimal()\n\n\n\n\n\n\n\nThis is a bit more encouraging: in 2016 and 2017, there was a good uptick in new care home beds. However, 2018 did not perform quite as well. Still, 2019 appears to be going in the right direction!"
  },
  {
    "objectID": "blog/two-way-business-intelligence-partnering-shiny-and-sql-to-capture-insights-in-performance-reporting.html",
    "href": "blog/two-way-business-intelligence-partnering-shiny-and-sql-to-capture-insights-in-performance-reporting.html",
    "title": "Two-Way Business Intelligence – Partnering Shiny and SQL to Capture Insights in Performance Reporting",
    "section": "",
    "text": "Performance and operational reporting is one of the key functions of the Information team at Worcestershire Acute Hospitals NHS Trust. All acute Trusts are monitored on a variety of performance related metrics, and regular reporting on performance against these key metrics is essential to support operational teams in achieving their targets.\nOur role as an Information team includes regularly reviewing operational and performance reporting, and highlighting key trends, variations or anomalies in the data to operational colleagues. Causes of variation can then be identified and documented as part of our reporting, allowing us to build up insight into our performance data.\nOne method of building up this collective insight is simple: the Information team reviews performance reporting, notices a trend or point of interest in the data and raises it with operation colleagues, either by email or in a performance review meeting. The reasoning behind the variation is investigated and contextual information around the causes is provided by operational colleagues. This is then cascaded by email to all relevant stakeholders so that, going forward, the point of interest in the data is understood.\nThere are some inherent problems with this traditional method of collecting insight around reporting. If collected insight is not shared with all stakeholders, or new colleagues join the Trust, not all of the report audience have the same context when reviewing the data. Add in the human factor of forgetfulness when multitasking and monitoring dozens of key performance metrics, and teams may find themselves repeatedly querying the same previously explained variations in performance.\nImplementing R as one of our business as usual tools, particularly the Shiny package, provides an alternative way of collecting and preserving insight into operational performance reporting. Below is an example of a report in Shiny, showing some dummy data on the Two-Week-Wait Referral to First Appointment cancer performance metric. In this instance, performance data has been charted using the SPC methodology, which makes outliers and variation easy to identify.\n\nAs an operational manager reviewing this report, I may know that the outlier at April 2018 was caused by a shortage of Two-Week-Wait clinic capacity because a clinician was on annual leave. This report has been set up so that the performance chart recognises click inputs. Double clicking on a data point triggers a modal dialog pop-up to appear. This modal reminds me of which data point I clicked on, and offers the opportunity to record a short comment explaining the variation.\n\nWhen I then click Okay, the modal dialogue box is dismissed, and the SPC chart refreshes. Behind the scenes, Shiny has sent an instruction to a SQL table in our data warehouse, where the captured comment has been stored along with information about which data point it relates to, and who has recorded the comment. When the SPC chart refreshes, my new comment is presented as an annotation which is available not just to me, but to all colleagues who view the report.\n\nColleagues from corporate support teams such as information, finance or workforce have immediate access to the reasons behind changes in the data. When presented at performance review meetings, the report contains added insight which can be understood even if the relevant operational manager is absent or leadership of a department has changed. It also prevents teams from wasting time querying or investigating variations in data which have been reviewed before.\nCombining dynamic and intuitive user interfaces crafted in Shiny with data storage in SQL allows for valuable contextual insight to be captured and retained. In practice, our team have used this combination in setting up performance monitoring reports for activity levels during Phase 3 of Restoration of Services following the COVID-19 outbreak in March-2020. Part of restoration planning included modelling a number of interventions required to rebuild activity to historic levels. As Phase 3 progresses, the delivery of these interventions needs to be tracked to ensure that activity can be delivered. We used Shiny and SQL to set up an Intervention Tracking report, which displays all intended interventions and allows operational teams to record updates on delivery, building up a timeline of progress towards achieving their activity targets. In this way R allows one-way operational performance reporting to become a two-way exchange of insight.\nThe example code for the report above is available on my github at https://github.com/chrisreading01/2waybiexample.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/success-story-kate-cheema.html",
    "href": "blog/success-story-kate-cheema.html",
    "title": "Success story - Kate Cheema, Director of Health Intelligence, British Heart Foundation",
    "section": "",
    "text": "Tell us a bit about yourself, your team and how your story begins?\nThe Health Intelligence team at the British Heart Foundation just about still qualifies as a ‘new’ team at the stately age of 2-and-a-bit years. Our remit is to collate and analyse a wide range of data related to cardiovascular disease and its treatment and outcomes across the UK and use this to provide critical insight and context to the BHF’s charitable mission work. Some of our work is population focussed, understanding the patterns and trends in CVD prevalence and incidence. Some is more system focussed, getting under the skin of variation in care for CVD patients and learning more about how patients are impacted by service change. And still more is supporting particular programmes of work, such as the National Defibrillator Network or our community mobilisation projects. You’ll see our numbers in BHF adverts and occasionally one of us gets wheeled out to talk stats on local radio. Lots to do! We’re a small team, just 5.5 people, serving a large organisation so we need to work smart.\n\n\nWhat was the problem/challenge you were trying to address?\nCVD is a complex and varied group of illnesses and the data describing it can be very nuanced. Pair that with an organisation that needs to use it for comms and media work, as well as informing strategy and programmes of work and the result is the need for a hybrid model of delivery that does the basics brilliantly, allows for easy ‘self serve’ and frees up time to support colleagues with the complex stuff. So our challenge was to build a library of core resources for internal use, as automated as possible and presented in an accessible format for all colleagues to use.\n\n\nHow R helped you? Any particular libraries/products/packages you found the most useful?\nR has been invaluable in the whole project to date (still lots to do!) but two specific resources spring to mind. Firstly, the Tidyverse suite of packages has been invaluable in streamlining, and making repeatable, our data reshaping. Much of the publicly available data we use is downloaded from websites and is (ahem) not exactly in a useful format. Having standardised, and generally very simple, reshaping scripts to reuse across the team has saved hours of time, not to mention to ability to automate the download in the first place (kudos to ::curl::).\nSecondly, we have utilised R Markdown extensively in the automated production of simple off-the-(Sharepoint)-shelf PDF based reports. Accessible to all, impossible to break (famous last words) these generally take the form of a key set of data visualisations of a specific topic, usually rendered using ggplot2 but also using network visualisation packages (::network::, ::igraph::, ::tidygraph:: ) and n-gram analysis of text data (::tidytext::) where required.\n\n\nWhat is the result?\nThe beginnings of a library accessible to all in the BHF and a decent chunk of time saved. A couple of our reports are scheduled to run and publish automatically with zero intervention from the team outside of checking it’s there. We’ve used R in other standalone projects (for example forecasting work, network analysis as part of an evaluation project) too. We’re in the process of improving our R capability, in terms of skills and in terms of infrastructure, so this is really just the start of the story for us.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-community-conference-view-from-the-the-chartered-society-of-physiotherapy-csp.html",
    "href": "blog/nhs-r-community-conference-view-from-the-the-chartered-society-of-physiotherapy-csp.html",
    "title": "NHS-R Community Conference: view from the The Chartered Society of Physiotherapy (CSP)",
    "section": "",
    "text": "I recently went to the second annual NHS-R conference, run by the NHS-R Community that’s promoting the use of R throughout the NHS. R being the free statistics and computer science package that has rapidly grown in popularity in academia and industry in recent years.\nThis growth has been driven by R’s users, who collaboratively develop and share solutions to everyday challenges. This community now has an increasing number of NHS staff.\n“Whether you’re trying to automate staff reports, analyse patient surveys, or produce interactive visualisations of clinical activity: the chances are someone has already produced a shareable solution using R, or is looking to collaborate with peers to do so”\nGetting involved with the NHS-R community is a great way to join this crowd.\nIt is imperative that physiotherapy services are able to demonstrate their success and impact. Why not apply the same analytical and evaluative mindset we use with our patients to our data and get involved now?\nWith NHS trusts, CCGs and others dipping into the computer-science toolbox when embarking on service redesigns – to forecast future demand for a service, for example – increasing trade unionists’ and clinicians’ use of R could improve the scrutiny of decision making processes.\nLikewise, with R supporting some stunning data visualisation tools, it could be a key tool for members making the case for physiotherapy in their local area.\nIntroductory courses in statistical packages like R can get you started, and then communities like NHS-R can help maximise use within physiotherapy.\nThis blog was shared by Frontline magazine: https://www.csp.org.uk/frontline/article/using-r-nhs\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html",
    "href": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html",
    "title": "A run chart is not a run chart is not a run chart",
    "section": "",
    "text": "Run charts are simple and powerful tools that help discriminate between random and non-random variation in data over time – for example, measures of healthcare quality.\nRandom variation is present in all natural processes. In a random process we cannot know the exact value of the next outcome, but from studying previous data we may predict the probability of future outcomes. So, a random process is predictable. Non-random variation, on the other hand, appears when something new, sometimes unexpected, starts to influence the process. This may be the result of intended changes made to improve the process or unintended process deterioration. The ability to tell random from non-random is crucial in quality improvement. One way of achieving this is runs analysis.\nIn general, a run is defined as a sequence of like items that is preceded and followed by one or more items of another type or by no item. Items can be heads and tails, odd and even numbers, numbers above and below a certain value, and so on.\nRuns analysis is based on knowledge of the natural distributions and limits of run lengths and the number of runs in random processes. For example, if we toss a coin 10 times and get all heads (1 run of length 10), we would think that something non-random is affecting the game. Likewise, if a run chart includes a run of 10 data points on the same side of the centre line, we would start looking for an explanation.\n\n\n\nFigure 1: Run charts from random numbers. A: random variation. B: non-random variation in the form of an upwards shift introduced after data point number 16 and identified by unusually long and few runs and signalled by a red, dashed centre line. See text for details on how to identify non-random variation.\n\n\nSpecifically, a run chart may be regarded as a coin tossing game where the data points represent heads and tails depending on their position above and below the centre line (ignoring data points that fall directly on the centre line). If the process is random, the data points will be randomly distributed around the centre (Figure 1A). A shift in process location will affect the distribution of data points and will eventually present itself by non-random patterns in data, which can be identified by statistical tests (Figure 1B).\nSwed and Eisenhart studied the expected number of runs in random sequences. If the number of runs is too small or too large, it is an indication that the sequence is not random (1). To perform Swed and Eisenhart’s runs test one must either do rather complicated calculations or look up the limits for the expected number of runs in tables based on the total number of runs in the sequence and the number of items of each kind. Simplified tables for use with run charts have been developed for up to 60 data points (2). For more than 60 data points, the limits can be calculated using the normal approximation of the runs distribution function (3). For example, in a run chart with 24 data points, the expected number of runs (95% prediction limits) is 8-18.\nChen proposed an alternative to Swed and Eisenhart’s method. Instead of counting the number of runs, Chen counts the number of shifts in the sequence, that is when a value of one kind is followed by a value of another kind, which is one less than the number of runs (4). To avoid confusing Chen’s shifts in sequence with shifts in process location, I use the term crossings.\nIn run charts, crossings are easily counted by counting the number of times the graph crosses the median line. If the process is random, the chance of crossing or not crossing the median line between two adjacent data points is fifty-fifty. Thus, the total number of crossings has a binomial distribution, b(n−1,0.5), where n is the number of data points and 0.5 is the success probability.\nWe should consider whether we are interested in both sides of the distribution, too few and/or too many crossings. By nature, a shift in process location following process improvement or deterioration will result in fewer crossings than expected. But unusually many crossings (oscillation) is also a sign of non-random variation, which will appear if data are negatively autocorrelated, that is, if any high number tends to be followed by a low number and vice versa. However, oscillation is not an effect of the process shifting location, but most likely a result of a poorly designed measure or sampling issues (5 p175). Chen recommends using one-sided tests suited for the purpose of the analysis, that is whether one is interested in detecting shifts or oscillations (4).\nFor example, for a run chart with 24 data points we could choose the lower fifth percentile of the cumulative binomial distribution of 23 trials with a success probability of 0.5 as our critical value for the lower limits of crossings. This is easily calculated in R using the qbinom() function, qbinom(p = 0.05, size = 24 - 1, prob = 0.5) = 8, that is fewer than 8 crossings would be unusual and suggest that the process is shifting. In Figure 1B non-random variation in the form of a shift is identified by the fact that the chart has only 6 crossings when at least 8 would be expected from 24 random numbers.\nThe number of crossings (and runs) is inversely related to the lengths of runs. All things being equal, fewer crossings give longer runs and vice versa. Therefore, a test for unusually long runs is also commonly used as a means to identify shifts. A simple example is the “classic” rule of thumb of a run of 8 or more data points on the same side of the centre line. But just like the expected number of crossings, the expected length of the longest run depends on the total number of data points. In a run chart with, say, 100 data points, we should not be surprised to find a run of 8.\nThe distribution of longest runs has been described in detail by Schilling (6–8). The expected length of the longest run either above or below the median is log2(n), where n is the total number of data points, excluding data points that fall directly on the centre line. Approximately 95% of the longest runs are predicted to be within ±3 of the expected value. For the purpose of detecting a shift, we are interested in the upper prediction limit for longest run, which is log2(n)+3 (rounded to the nearest integer). For example, in a run chart of 24 data points, the upper 95% prediction limit for the longest run is round(log2(24) + 3) = 8, that is a run of more than 8 indicates a shift. Figure 1B has an unusually long run of 9 consecutive data points on the same side of the centre line.\nA trend is a special form of a run, where like items are defined as data points that are bigger or smaller than the preceding one. The trend test was developed by Olmstead who provided tables and formulas for the probabilities of trends of different lengths depending on the total number of data points (9). For example, with less than 27 data points in total, the chance of having a trend of 6 or more data points going up or down is less than 5%. Note that Olmstead defines a trend as the number of jumps rather than the number of data points that surround the jumps.\nIn summary, there are (at least) four unusual run patterns that may be used to identify non-random variation in run charts:\n\nToo many runs\n\nToo few runs\n\nToo long runs\n\nToo long trends\n\nThe selection of rules and the choice of critical values to define too many, too few and too long have significant influence on the statistical properties of run charts. This is the subject of the following sections."
  },
  {
    "objectID": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html#appendix-critical-values-for-longest-run-and-number-of-crossings",
    "href": "blog/a-run-chart-is-not-a-run-chart-is-not-a-run-chart.html#appendix-critical-values-for-longest-run-and-number-of-crossings",
    "title": "A run chart is not a run chart is not a run chart",
    "section": "Appendix: Critical values for longest run and number of crossings",
    "text": "Appendix: Critical values for longest run and number of crossings\n![Screenshot of critical values for longest run and number of crossings](img/appendix-1-critic"
  },
  {
    "objectID": "blog/code-snippets-regular-expressions.html",
    "href": "blog/code-snippets-regular-expressions.html",
    "title": "Code snippets – regular expressions",
    "section": "",
    "text": "Inspired by conversations on the NHS-R Slack where code answers are lost over time (it’s not a paid account), and also for those times when a detailed comment in code isn’t appropriate but would be really useful, this blog is part of a series of code snippet explanations."
  },
  {
    "objectID": "blog/code-snippets-regular-expressions.html#metrics",
    "href": "blog/code-snippets-regular-expressions.html#metrics",
    "title": "Code snippets – regular expressions",
    "section": "Metrics",
    "text": "Metrics\nFor this particular analysis metrics 02 to 09 use a denominator of GP population count of over 16 years old whilst all the others use prevalence of Coronary Heart Disease (CHD). For quickness, it’s possible to list out each metric and its corresponding calculation but that does make code less readable, particularly as the metrics included have gaps in the numbering:\n\nactivity_by_type_decile_stg %&gt;%\n  mutate(metric02_total_ratio = metric02_total / list_size_total * 1000) %&gt;%\n  # skip 03 and skip 04\n  mutate(metric05_total_ratio = metric05_total / list_size_total * 1000) %&gt;%\n  mutate(metric06_total_ratio = metric06_total / list_size_total * 1000) %&gt;%\n  mutate(metric07_total_ratio = metric07_total / list_size_total * 1000) %&gt;%\n  mutate(metric08_total_ratio = metric08_total / list_size_total * 1000) %&gt;%\n  mutate(metric09_total_ratio = metric09_total / list_size_total * 1000) %&gt;%\n  # Above metrics 2 - 9 use GP population count as denominator rather than CHD prevalence\n  mutate(metric10_total_ratio = metric10_total / metric01_total * 1000) %&gt;%\n  mutate(metric11_total_ratio = metric11_total / metric01_total * 1000)\n# and so on\n\nThe first downside of this is that metrics may be added later and so get missed in future runs of the scripts. That means that 03 and 04 would have no corresponding metric_total_ratio column and it won’t necessarily be obvious or stand out that they are missing.\nThe other downside is readability is poor because fewer lines of code is less of a wall of text and so easier to debug. However, such shorter code can be confusing and not self explanatory unless you are familiar with it:\n\nlibrary(glue)\n\n# Metrics 2 - 9 listed as use GP list size 16+ as denominator\n# else uses CHD prevalence\nby_list_total &lt;- c(\n  \"02\", \"05\",\n  \"06\", \"07\",\n  \"08\", \"09\"\n)\n\nby_list_total\n\n[1] \"02\" \"05\" \"06\" \"07\" \"08\" \"09\"\n\n# Produces a string to be used in an if(else) in later code to match against\n# the metric numbers listed in by_list_total\nby_list_regex &lt;- glue(\"^metric({paste(by_list_total, collapse = '|')})_total$\")\n\nby_list_regex\n\n^metric(02|05|06|07|08|09)_total$\n\n\nAnd this is where the blogs like this can be used to explain parts of the code in more detail!\n\nby_list_total is creating a vector (a collection of same things, in this case text/strings) of the metrics which is used in by_list_regex. Writing it this way makes it easier to see what metrics are included, missed or incorrect. It’s not a wall of text and can be scan read really quickly.\nby_list_regex also creates a vector which uses by_list_total. Together they are important for a later search in the metric names.\n\nTranslating the string output ^metric(02|05|06|07|08|09)_total$ into English would be a bit like:\n\nAt the beginning (^) of the text look for the word metric followed by 02 or (|) 05 or (|) 06 or 07 (|) or 08 (|) or 09, then an underscore and the word total which come at the end ($) of the string.\n\nThe magic of a string like this occurs in later code when it’s used in the ifelse() function:\n\n# The wide data is first converted to longer, \n# \"tidy\" data before adding a new \n# total_column based on the metric name\nactivity_long &lt;- activity_type_decile |&gt;\n  pivot_longer(\n    cols = c(starts_with(\"metric\"), -metric01_total),\n    names_to = \"metric_name\",\n    values_to = \"metric_total\"\n  ) |&gt;\n  mutate(\n    total_column = ifelse(\n      str_detect(metric_name, by_list_regex),\n      list_size_total,\n      metric01_total\n    ))\n\nI usually use case_when() but when the options are binary, like in this case where it’s one population or another, then the ifelse() function is a bit neater."
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "",
    "text": "Hi! This blog post acts as a follow up to my previous entry to the NHS-R Community website, regarding the use of random forest models to predict a patient’s length of stay. This time, the aim was to classify a patient as either “at risk” or “not at risk” of falling as an inpatient, utilising the XG Boost technology available within R. The classification depended on a variety of predictor variables, agreed with a senior digital nursing colleague working within the same acute trust as me. The intention behind this project was to see how accurately a patient could be classified into a fall risk category, using the agreed predictor variables. The goals and proposed methodology of this project were established after discussions with the previously mentioned senior digital nursing colleague, in which we set out to determine what the most appropriate way to approach this task would be regarding a clinical perspective, variable selection and potential clinical use cases. The accuracy of the eventually completed model was assessed through predictions on a testing data set, where the model was used to classify patients as either at risk of falling or not. This prediction was then compared with the actual fact of whether each specific patient did experience a fall during their inpatient spell or not. The testing data set for this accuracy assessment was a section of the primary data set used for this project, as the primary data set was split into both a training and testing section.\nAs mentioned within my previous predictive analytics post (linked above), this blog post is not intended to be a comprehensive guide on how to approach a machine learning task within R, best practice for utilising XG Boost or a complete overview of all of the relevant considerations necessary for such a project. This blog post is intended to act as an overview of how I approached this specific project, that hopefully potential readers might find interesting. The considerations and methods discussed within this blog post are specific to the context of this project only. If you are thinking about undertaking a similar machine learning project, you should seek the relevant guidance to do so."
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#introduction",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#introduction",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "",
    "text": "Hi! This blog post acts as a follow up to my previous entry to the NHS-R Community website, regarding the use of random forest models to predict a patient’s length of stay. This time, the aim was to classify a patient as either “at risk” or “not at risk” of falling as an inpatient, utilising the XG Boost technology available within R. The classification depended on a variety of predictor variables, agreed with a senior digital nursing colleague working within the same acute trust as me. The intention behind this project was to see how accurately a patient could be classified into a fall risk category, using the agreed predictor variables. The goals and proposed methodology of this project were established after discussions with the previously mentioned senior digital nursing colleague, in which we set out to determine what the most appropriate way to approach this task would be regarding a clinical perspective, variable selection and potential clinical use cases. The accuracy of the eventually completed model was assessed through predictions on a testing data set, where the model was used to classify patients as either at risk of falling or not. This prediction was then compared with the actual fact of whether each specific patient did experience a fall during their inpatient spell or not. The testing data set for this accuracy assessment was a section of the primary data set used for this project, as the primary data set was split into both a training and testing section.\nAs mentioned within my previous predictive analytics post (linked above), this blog post is not intended to be a comprehensive guide on how to approach a machine learning task within R, best practice for utilising XG Boost or a complete overview of all of the relevant considerations necessary for such a project. This blog post is intended to act as an overview of how I approached this specific project, that hopefully potential readers might find interesting. The considerations and methods discussed within this blog post are specific to the context of this project only. If you are thinking about undertaking a similar machine learning project, you should seek the relevant guidance to do so."
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#data-set-preparation-and-variable-selection",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#data-set-preparation-and-variable-selection",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "Data Set Preparation and Variable Selection",
    "text": "Data Set Preparation and Variable Selection\nFor this project, the variables used were discussed with the previously mentioned senior digital nursing colleague prior to the development and testing of the model. The features were chosen as it was thought that they would act as ideal predictor variables for whether or not an inpatient may experience a fall. The final predictor variables used in the model were as follows, “Sex”, “Non-Elective Stay”, “Dementia Diagnosis”, “Current Delirium”, “Parkinson’s Diagnosis”, “Current Low Blood Pressure”, “Previous Fall Flag” and “Age Category”. A positive to using these particular variables was that they were all readily available in internal electronic patient record systems.\n\n\n\n\n\n\nWhile these predictor variables were the final ones utilised within this model, they were not the only ones considered from the start of the project. This is discussed in more detail later, within the section “Performance and Challenges”.\n\n\n\nIn order to transform and clean this data into an appropriate format, a SQL query was used to retrieve all of the relevant fields from the necessary electronic patient record systems and wrangle this into the data set that would eventually be used in the project. Next, this data was saved as a CSV file and used to train the model within the primary R script.\nAs the input for this specific model needed to be numeric, “one hot encoding” was once again used to create a numeric column for each value within the categorical variables. This encoding was performed within the R script itself."
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#model-creation",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#model-creation",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "Model Creation",
    "text": "Model Creation\nThis model was created in R using the following packages, “tidyverse”, “xgboost” and “caret”. The primary data set was initially imported into R as a CSV, this file is referred to in the example script below as “CSV File”. The data was then encoded using the function “dummyVars”, with any irrelevant columns then removed. For this project, the data was split into a training section containing 70% of the initial data and a testing section containing 30% of the initial data.\nFollowing this, the optimal number of rounds and the maximum depth of the model were established. This was done through gradual adjustments to each of these values, with improvements and drops in performance and prediction quality noted. In the example script provided below, “x” would be replaced by the depth and “y” would be replaced by the number of rounds. Once the model was trained, the accuracy and recall was then assessed through a confusion matrix. As this was a heavily imbalanced data set, where a much larger proportion of patients did not experience a fall compared to those that did, the threshold for classification needed to be altered from the default value in order to receive meaningful and useful predictions. Within the example script, “t” would be replaced by the specific threshold value determined to be optimal for classification. For this project, the threshold was eventually decided through a process of gradual alterations, with improvements and drops within the accuracy and recall scores of the model noted.\n\n\n\n\n\n\nAs mentioned within my previous predictive analytics post, I established a seed within the R script using the set.seed() function, in order to ensure that the results were reproducible.\n\n\n\nThe example script showing how this project was approached within R is shown below.\n\n#LOAD LIBRARIES\nlibrary(tidyverse)\nlibrary(xgboost) \nlibrary(caret) \n\n#SET SEED SO THAT RESULTS ARE REPRODUCIBLE\nset.seed(200)\n\n#IMPORT DATA\nResults &lt;- read.csv(\"CSV File\", header=T, stringsAsFactors=T)\n\n#CHECK INITIAL DATA\nglimpse(Results)\n\n#ENCODE DATA\ndmy &lt;- dummyVars(\" ~ .\", data = Results)\n\nencoded_data &lt;- data.frame(predict(dmy, newdata = Results))\n\nencoded_data$Fall.Y &lt;- as.factor(encoded_data$Fall.Y)\n\nformatted_data &lt;- subset(encoded_data, select = -c(Fall.N))\n\n#CHECK FORMATTED DATA\nglimpse(formatted_data)\n\n#DATA SPLIT INTO TRAINING (70%) AND TESTING (30%)\nparts = createDataPartition(formatted_data$Fall.Y, p = 0.7, list = F)\ntrain = formatted_data[parts, ]\ntest = formatted_data[-parts, ]\n\n#PREDICTOR AND RESPONSE - TRAINING\ntrain_x = data.matrix(train[, -21])\ntrain_y = data.matrix(train[,21])\n\n#PREDICTOR AND RESPONSE - TESTING\ntest_x = data.matrix(test[, -21])\ntest_y = data.matrix(test[, 21])\n\n#FINAL TRAINING AND TESTING\nxgb_train = xgb.DMatrix(data = train_x, label = train_y)\nxgb_test = xgb.DMatrix(data = test_x, label = test_y)\n\n#SAMPLE MODEL\n#X WOULD BE THE MAXIMUM DEPTH\n#Y WOULD BE THE NUMBER OF ROUNDS\ndepth &lt;- x\nrounds &lt;- y\nfinal = xgboost(data = xgb_train, max.depth = depth, nrounds = rounds, verbose = 0, objective='binary:logistic')\n\n#MAKE FINAL PREDICTIONS ON TEST DATA\npred_test = predict(final, xgb_test)\n\n#ASSESS ACCURACY\n#T WOULD BE THE IDEAL THRESHOLD FOR CLASSIFICATION, DETERMINED AFTER TESTING\nthreshold &lt;- t\na = as.factor(test_y)\nb = as.factor(if_else(pred_test &gt;= threshold, 1, 0))\n\nCONFUSION_MATRIX &lt;- table(Test_Value = a, Prediction_Value = b)\nCONFUSION_MATRIX"
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#performance-and-challenges",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#performance-and-challenges",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "Performance and Challenges",
    "text": "Performance and Challenges\nAs the data set behind this model was quite heavily imbalanced, one of the solutions used to attempt to overcome this was to manually alter the balance of the training section of the data set through “oversampling” and “undersampling”. While this did result in reasonably high accuracy scores, it is believed that an unintended consequence of this was that “overfitting” took place. This conclusion was reached as predictions in testing were not generally in line with what was expected, despite the reasonably high accuracy scores. Due to this, the original data set balance was used, with a lowered threshold for classification (as previously mentioned). One of the key variables believed to have contributed to the overfitting issue was age, when used in a continuous, numeric format. Performance was generally better when age was altered to present as a categorical field, for example, 0-18 rather than a specific numeric value. While the issue of overfitting seemed to be a lot less prevalent by the end of the project, as predictions were more consistent with what would be expected, it is important to note that this does not mean there was absolutely no overfitting present within the model still.\nAs previously mentioned, certain variables originally considered were eventually removed from the data set and not included within the final version of the model. This is because they were determined to generally not be good predictor variables within the context of this specific data set and model. They may have actually been hindering the overall performance of the model. The variables removed were “Current Dizziness”, “Current Elevated Heart Rate”, “Current Visual Impairment” and “Current Incontinence”. While these variables were determined to not be good predictors within this particular project, they could still potentially be useful within another setting and context for the same purpose of predicting fall risk.\nFor the variables utilised within the final model, a chart is shown below visualising the feature importance of all those still included. The plot was created using the package “ggplot2”. The R script for this plot is also shown below, the script presumes that the model has already been created, as the model “final” from the previous script shown is referenced.\n\n#LOAD LIBRARIES\nlibrary(ggplot2)\n\n#FEATURE IMPORTANCE\nmodel_feature_importance &lt;- xgb.importance(model = final)\nmodel_feature_importance &lt;- model_feature_importance %&gt;% arrange(desc(Gain))\n\n#FEATURE IMPORTANT PLOT - GGPLOT2\nggplot(model_feature_importance, aes(x = reorder(Feature, Gain), y = Gain)) +\ngeom_bar(stat = \"identity\", fill = \"red\") +\ncoord_flip() +\nlabs(title = \"Feature Importance (Gain) Within Model\", x = \"Variable\", y = \"Feature Importance\") +\ntheme_light()\n\nPlot Of Feature Importance\n\nThe performance of the XG Boost model was compared with a Logistic Regression model and a Random Forest model, both also created in R using the same data set. Overall, the XG Boost model seemingly returned a better accuracy and gave higher quality predictions with this specific data set than either of the other classification methods.\nAs an inpatient fall could be considered to be rather awkward to predict, in that multiple patients that did not fall could theoretically be admitted with the exact same values as a patient that did fall, a goal of achieving an overall accuracy and recall score of 70% was set for this specific project. Precision, while very important for some projects, was not deemed to be a priority over recall and overall accuracy for this specific project. This is because the impact of a false positive (being a patient predicted to fall that did not actually fall) would be more acceptable in the context of this project than a false negative (being a patient that did fall but was predicted not to). The final accuracy score for this project was 69.39%, while the recall score was 67.83%."
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#interactivity-for-users",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#interactivity-for-users",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "Interactivity For Users",
    "text": "Interactivity For Users\nSimilarly to the previously mentioned Random Forest prediction project, it was thought that users would need a way to simply interact with the model to input values and receive their predictions. For this, I created a shiny app that would allow users to utilise check boxes to input the various predictor values for a hypothetical patient. The app would then produce a fall risk classification based on these predictor values using the model. In theory, this app would be hosted and accessed internally within a trust, for relevant staff to use for predictions. Below, a screenshot is shown of an example of the final output of the app based on the predictor values entered. In this case, the model predicted that the patient would be a fall risk.\nScreenshot Of App"
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#considerations-and-notes",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#considerations-and-notes",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "Considerations and Notes",
    "text": "Considerations and Notes\n. If such a model were to ever be used for clinical purposes, it would require further validation to absolutely ensure that the performance was to a high standard and that predictions were meaningful. As previously mentioned, there may have still been overfitting present within the final version of this model, even if this was to a lesser extent than it would have otherwise been if the relevant corrections and amendments had not been made.\n. While I opted to use an XG Boost model created specifically within R for this project, it should be noted that alternative classification methods and other tech stacks could have also been used to develop a similar model.\n. When utilising patient data for such a model, there are obvious information governance considerations necessary. The data set used to create this model was limited to only variables that were utilised for predictions, no unnecessary information was extracted. Additionally, this data was wrangled, stored and analysed on the trust’s internal network only.\n. My role within my organisation is not explicitly that of a “Data Scientist” or primarily a machine learning focused role, my current title is “Information Development Analyst”. While I have a keen interest in R, machine learning and the potential applications of this within the NHS, I am definitely not an authority in this area of healthcare analytics. The methods used for this undertaking are specific to the goals and context of this project only. If you are considering creating something similar within your own organisation, you should seek the relevant guidance to do so.\n. Once the age variable had been altered to present in a categorical format, rather than as a continuous numeric value, it may have been more appropriate to utilise “ordinal encoding” rather than “one hot encoding” for this specific variable. However, a version of the model was created with this applied as an alternative and this did not appear to impact the performance of the model. Nevertheless, I plan to eventually create a second version of the model with this considered, along with more recent data included, as the time period for admissions within the data set used for this project was the 1st January 2021 - 31st December 2024.\n. Throughout this blog post, I have continually referred to a senior digital nursing colleague who had valuable input into this project from a clinical perspective, particularly for considering potential clinical use cases and initial variable selection. Thanks Hannah for your contribution to this project!"
  },
  {
    "objectID": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#final-thoughts",
    "href": "blog/Predictive_Analytics_Within_Healthcare-XGBoost.html#final-thoughts",
    "title": "Predictive Analytics within healthcare - XG Boost models for inpatient fall risk predictions",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nThank you for taking the time to read this post! I hope you have found it as interesting as I found undertaking the project itself.\nThere are certainly untapped applications for prediction models, similar to the one described in this post, to be created and implemented throughout the NHS and healthcare analytics more generally for a variety of clinical and non-clinical use cases. Some organisations may already have large amounts of resources dedicated to the development and implementation of internal prediction models, while others may not have started to explore the possibilities and potential benefits of this yet. Each organisation looking to implement predictive analytics would likely benefit in unique ways, depending on where attention and resources were focused. Wherever these models are implemented, it is obviously imperative that they are done so with care and extensively checked before they are allowed to influence any potential decisions within a healthcare organisation. As this technology has become more accessible through open source tech stacks (such as R and Python) and more digestible through the increased availability of online resources, I am personally excited to see any further developments, particularly within the NHS and wider healthcare space."
  },
  {
    "objectID": "blog/animated-population-pyramids-in-r-part-1.html",
    "href": "blog/animated-population-pyramids-in-r-part-1.html",
    "title": "Animated Population pyramids in R: part 1",
    "section": "",
    "text": "Update to data\n\n\n\n\n\nSince the blog was published populations statistics are available from 2018 and CCG names have changed. The blog text refers to the previous estimates from 2016 and three CCGs but the code is from 2018 and for only 2 CCGs.\n\n\n\nEven in my relatively short experience of working as healthcare analyst, I have massively used population pyramids to describe the local population and how it may change according to ONS population projections. So, I decided to try animated pyramids in R. The overall process for me includes:\n1. Wrangle data a bit to make it ready for {ggplot2}.\n2. Build 1 pyramid and see how it will look.\n3. Create animation with 25 pyramids for period 2016 – 2041 using different animation packages and compare them.\nIn this part I will consider only first 2 steps.\nChange the data\nI probably should have said earlier, but I am not an expert in R (actually, I feel like I’m still a perpetual novice). On data wrangling stage, I created datasets for almost each step of the data transformation. It made easier for me to check for errors but made my code a bit ugly.\n\nlibrary(readxl)\nurl &lt;- \"https://www.ons.gov.uk/file?uri=/peoplepopulationandcommunity/populationandmigration/populationprojections/datasets/clinicalcommissioninggroupsinenglandtable3/2018based/table3.xls\"\ndestfile &lt;- \"table3.xls\"\ncurl::curl_download(url, destfile)\n\nmales &lt;- read_excel(destfile, sheet = \"Males\", skip = 6) |&gt; dplyr::mutate(gender = \"Male\")\nfemales &lt;- read_excel(destfile, sheet = \"Females\", skip = 6) |&gt; dplyr::mutate(gender = \"Female\")\ndf &lt;- rbind(males, females)\n\nI used the open-source data from the Office for National Statistics. It has population estimates for mid-2016 and population projections by age and gender for England and CCGs. I separately saved worksheets ‘Females’ and ‘Males’ and loaded them in RStudio. I then just added gender column in both datasets and combined this two data sets by rbind(). Overall, data wrangling process should make this:\n##    year `AGE GROUP` gender population totalyears percentage\n## 1  2016       0-4 Female       41.1     1167.5   3.520343\n## 2  2016     05-09 Female       39.5     1167.5   3.383298\n## 3  2016     10-14 Female       36.4     1167.5   3.117773\n## 4  2016     15-19 Female       39.6     1167.5   3.391863\n## 5  2016     20-24 Female       49.6     1167.5   4.248394\n## 6  2016     25-29 Female       44.2     1167.5   3.785867\n## 7  2016     30-34 Female       39.8     1167.5   3.408994\n## 8  2016     35-39 Female       38.1     1167.5   3.263383\n## 9  2016     40-44 Female       35.6     1167.5   3.049251\n## 10 2016     45-49 Female       38.5     1167.5   3.297645\nfrom this:\n\nread_excel(destfile, sheet = \"Females\") |&gt; head(10)\n\n# A tibble: 10 × 29\n   2018-based subnational …¹ ...2  ...3     ...4    ...5    ...6    ...7    ...8\n   &lt;chr&gt;                     &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1 Table 3: 2018-based Subn… &lt;NA&gt;  &lt;NA&gt;       NA      NA      NA      NA      NA\n 2 Females by 5 year age gr… &lt;NA&gt;  &lt;NA&gt;       NA      NA      NA      NA      NA\n 3 &lt;NA&gt;                      &lt;NA&gt;  &lt;NA&gt;       NA      NA      NA      NA      NA\n 4 Figures in units (to one… &lt;NA&gt;  &lt;NA&gt;       NA      NA      NA      NA      NA\n 5 &lt;NA&gt;                      &lt;NA&gt;  &lt;NA&gt;       NA      NA      NA      NA      NA\n 6 CODE                      AREA  AGE …    2018    2019    2020    2021    2022\n 7 E92000001                 Engl… 0-4   1630474 1607341 1586496 1560569 1539989\n 8 E92000001                 Engl… 5-9   1719932 1728416 1728980 1724499 1703291\n 9 E92000001                 Engl… 10-14 1596348 1637137 1677733 1710446 1744902\n10 E92000001                 Engl… 15-19 1506704 1502982 1514822 1545190 1590329\n# ℹ abbreviated name: ¹​`2018-based subnational population projections`\n# ℹ 21 more variables: ...9 &lt;dbl&gt;, ...10 &lt;dbl&gt;, ...11 &lt;dbl&gt;, ...12 &lt;dbl&gt;,\n#   ...13 &lt;dbl&gt;, ...14 &lt;dbl&gt;, ...15 &lt;dbl&gt;, ...16 &lt;dbl&gt;, ...17 &lt;dbl&gt;,\n#   ...18 &lt;dbl&gt;, ...19 &lt;dbl&gt;, ...20 &lt;dbl&gt;, ...21 &lt;dbl&gt;, ...22 &lt;dbl&gt;,\n#   ...23 &lt;dbl&gt;, ...24 &lt;dbl&gt;, ...25 &lt;dbl&gt;, ...26 &lt;dbl&gt;, ...27 &lt;dbl&gt;,\n#   ...28 &lt;dbl&gt;, ...29 &lt;dbl&gt;\n\n\nLet’s see it step by step:\nFor the simplicity, I left only area I need for now. In 2016, Birmingham and Solihull CCG were three different CCGs.\n\n# df1 &lt;- subset(persons, df$AREA == \"NHS Birmingham CrossCity CCG\" | df$AREA == \"NHS Birmingham South and Central CCG\" | df$AREA == \"NHS Solihull CCG\")\n\n# CCG names have changed since first posting of blog\ndf1 &lt;- subset(df, df$AREA == \"NHS Birmingham and Solihull CCG\" | df$AREA == \"NHS Sandwell and West Birmingham CCG\")\n\nMy data still has columns for each year separately, so I created column ‘year’ and changed data structure\n\n\n\n\n\n\nSuperseded function gather()\n\n\n\n\n\ngather()has been superseded in {tidyr} part of {tidyverse} and it a message may appear to suggest using pivot_longer(). The code will still run but there will be no development or maintenance for this function.\n\n\n\n\nlibrary(tidyr)\n# df2 &lt;- gather(df1, \"year\", \"population\", 4:29)\ndf2 &lt;- pivot_longer(df1, cols = 4:29,\n                    names_to = \"year\",\n                    values_to = \"population\")\n\nprint(df2, n = 10)\n\n# A tibble: 2,080 × 6\n   CODE      AREA                            `AGE GROUP` gender year  population\n   &lt;chr&gt;     &lt;chr&gt;                           &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;      &lt;dbl&gt;\n 1 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2018      19007 \n 2 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2019      18803 \n 3 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2020      18578.\n 4 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2021      18219 \n 5 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2022      17923.\n 6 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2023      17780.\n 7 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2024      17794 \n 8 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2025      17790.\n 9 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2026      17802.\n10 E38000144 NHS Sandwell and West Birmingh… 0-4         Male   2027      17841.\n# ℹ 2,070 more rows\n\n\nBoring but important bits: aggregate data by year, age band and gender, change population column to numeric format and drop the row ‘All Ages’ to not accidentally include it in our plot\n\ndf2$population &lt;- as.numeric(df2$population)\ndf3 &lt;- aggregate(population ~ `AGE GROUP` + gender + year, data = df2, FUN = sum)\ndf3 &lt;- df3[df3$`AGE GROUP` != \"All ages\", ]\n\nNow, let’s calculate percentages. For standard population pyramids percentages are calculated from total population for the year, so we should calculate this value, add to the table and calculate percentage for each gender-age band pair.\n\ntotalyear &lt;- aggregate(population ~ year, data = df3, FUN = sum)\ndf4 &lt;- merge(x = df3, y = totalyear, by = \"year\", all.x = TRUE)\ncolnames(df4)[colnames(df4) == \"population.y\"] &lt;- \"totalyears\"\ncolnames(df4)[colnames(df4) == \"population.x\"] &lt;- \"population\"\ndf4$percentage &lt;- df4$population / df4$totalyears * 100\n\n# This version of import requires the age groups to be a factor to be ordered:\ndf4$`AGE GROUP` &lt;- factor(df4$`AGE GROUP`, \n                          levels = \n                            c(\"0-4\",\n                          \"5-9\",   \n                          \"10-14\",\n                          \"15-19\",\n                          \"20-24\",\n                          \"25-29\",\n                          \"30-34\", \n                          \"35-39\", \n                          \"40-44\", \n                          \"45-49\", \n                          \"50-54\",\n                          \"55-59\", \n                          \"60-64\",\n                          \"65-69\",\n                          \"70-74\",\n                          \"75-79\",\n                          \"80-84\",\n                          \"85-89\",\n                          \"90+\" ))\n\nTo draw population pyramids in Excel, I always used negative values for one of the genders and then changed the legend. I used the same logic for R\n\ndf4 &lt;- transform(df4, percentage = ifelse(gender == \"Male\", - df4$population / df4$totalyears * 100, percentage))\n\nLast but not least, I notices ‘X’ in front of the year. Let’s remove it!\nCommented out this last section as it removes the first two characters and this data import does not have the X added\n\n# df4$year &lt;- substr(df4$year, 2, 5) \n\nDrawing pyramid\nNow, when our data looks tidy and ready, we can move to the the most exciting part – using {ggplot2}. The main thing in this process are: build bar chart, flip axes and use the theme we would like. I could not resist and used The Strategy Unit colours!\n\nlibrary(ggplot2)\nggplot(subset(df4, df4$year == \"2018\"), \n       aes(x = `AGE GROUP`, \n           y = percentage, \n           fill = gender)) +   # Fill column\n  geom_bar(stat = \"identity\", width = .85) +   # draw the bars\n  scale_y_continuous(breaks = seq(-5,5, length.out = 11),\n                     labels = c('5%','4%', '3%', '2%', '1%', '0', '1%','2%','3%','4%','5%')) +\n  coord_flip() +  # Flip axes\n  labs(title = \"Birmingham and Solihull population\", \n       y = \"percentage of population\", \n       x = \"Age group\") +\n  theme(plot.title = element_text(hjust = .5),\n        axis.ticks = element_blank(),\n        panel.background = element_blank(), \n        strip.background = element_rect(colour=\"white\", \n                                        fill=\"white\"), \n        strip.text.x = element_text(size = 10)) +   # Centre plot title\n  scale_fill_manual(values = c(\"goldenrod2\", \"gray32\")) + ###colours of Strategy Unit+ \n  facet_grid(. ~ year)\n\n\n\n\n\n\n\nTo be continued…\nAs I previously said, my main aim of this exercise was to learn R animation and compare different packages. So far I have used packages {magick} and {gganimate} and am happy to share results in the next part. Please do not hesitate to leave your comment and suggest any other packages for creating animation, I want to test them all!\nThis blog has been edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html",
    "href": "blog/diverging-dot-plot-and-lollipop-charts-plotting-variance-with-ggplot2.html",
    "title": "Diverging Dot Plot and Lollipop Charts – Plotting Variance with ggplot2",
    "section": "",
    "text": "Creating the Dot Plot Variance chart\nThe data preparation was used in the previous blog entitled: Diverging Bar Charts – Plotting Variance with ggplot2.\n\n# 20240222 Added for qmd to run\nlibrary(ggplot2)\ntheme_set(theme_classic())\ndata(\"mtcars\") # load data\n\nmtcars$CarBrand &lt;- rownames(mtcars) # Create new column for car brands and names\n\nmtcars$mpg_z_score &lt;- round((mtcars$mpg - mean(mtcars$mpg))/sd(mtcars$mpg), digits=2)\n\nmtcars$mpg_type &lt;- ifelse(mtcars$mpg_z_score &lt;= 0, \"below\", \"above\")\n\nmtcars &lt;- mtcars[order(mtcars$mpg_z_score), ] #Ascending sort on Z Score\nmtcars$CarBrand &lt;- factor(mtcars$CarBrand, levels = mtcars$CarBrand)\n\nRefer to that if you need to know how to create the data prior to this tutorial.\nSetting up the Dot Plot Variance chart\n\nlibrary(ggplot2)\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n  geom_point(stat='identity', aes(col=mpg_type), size=6) +\n  scale_color_manual(name=\"Mileage (deviation)\",\n                     labels = c(\"Above Average\", \"Below Average\"),\n                     values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n  geom_text(color=\"white\", size=2) +\n  labs(title=\"Diverging Dot Plot (ggplot2)\",\n       subtitle=\"Z score showing Normalised mileage\", caption=\"Produced by Gary Hutson\") +\n  ylim(-2.5, 2.5) +\n  coord_flip()\n\nThis is very similar to the previous plot we created in the previous post, however there are a few differences. The main difference is that we use a geom_point() geometry and set the colour of the points based on whether the said point deviates above and below the average. In addition, we use the geom_text() to set the colour of the text in the points to white and specify the size of the text. The final difference is that I have added a Y limit (ylim) range of -2.5 standard deviation to positive 2.5 standard deviations.\nRunning this block of code, along with the data preparation code, will give you a chart that looks as below:\n\n\n\n\n\n\n\n\nCreating the Diverging Lollipop Chart\nThe code below shows how to build the diverging lollipop chart in R and ggplot2:\n\nggplot(mtcars, aes(x=CarBrand, y=mpg_z_score, label=mpg_z_score)) +\n  geom_point(stat='identity', aes(col=mpg_type), size=6) +\n  scale_color_manual(name=\"Mileage (deviation)\",\n                     labels = c(\"Above Average\", \"Below Average\"),\n                     values = c(\"above\"=\"#00ba38\", \"below\"=\"#0b8fd3\")) +\n  geom_segment(aes(y = 0,\n                   x = CarBrand,\n                   yend = mpg_z_score,\n                   xend = CarBrand),\n               color = \"black\") +\n  geom_text(color=\"white\", size=2) +\n  labs(title=\"Diverging Lollipop Chart\",\n       subtitle=\"Z score for normalised mileage\",\n       caption=\"Produced by Gary Hutson\") +\n  ylim(-2.5, 2.5) + coord_flip() + theme(panel.grid.major = element_blank(), panel.grid.minor =\n  element_blank())\n\nSimilar geometries are used here. What has been added here is the geom_segment() this shows how the line segments need to be added. The starting y is equal to 0 on the Y scale and the starting x is the first car by the car brand. Similarly, the end of the x (xend) is also the CarBrand.\nThe only other difference is to add a theme constraint to the end of the code to turn off the major and minor grid lines, this is achieved by setting the panel.grid.major and panel.grid.minor equal to element_blank().\nThe completed graph and plot is shown below:\n\n\n\n\n\n\n\n\nThere – we now have some lovely looking charts that can be put into a report to report on variance between categorical variables.\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutsons-Hacks.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/simpler-sql-with-dplyr.html",
    "href": "blog/simpler-sql-with-dplyr.html",
    "title": "Simpler SQL with {dplyr}",
    "section": "",
    "text": "Comparing {dplyr} with SQL nested queries\nFollowing on from my last post, where I demonstrated R to some first time R users, I want to do a wee comparison of {dplyr} V SQL, so that folks, particularly those in the NHS who might be R curious, can see just what the fuss is about.\nTo do so I want to recap on the example I showed at the AphA Scotland event.\nThis,in turn goes back to some work I’ve been doing with Neil Pettinger, where we are looking at ways to visualise patient flow.\nThis relies on a spreadsheet that Neil originally put together. Part of my demo was to explain how to recreate the visualisation in R, but I also showed some of the data transformation steps carried out using {dplyr} and some fellow {tidyverse} helpers.\nIn this post I want to focus on that a but further, by showing the SQL code I would write to arrive at the same end result.\nIn order to do this I imported Neil’s spreadsheet (which I’ve uploaded - with Neil’s permission to the repo RowOfDots) to into a SQL Server table (by using the built in import wizard, for a quick but not reproducible way of ingesting the data).\nHere’s how that looks:\n\nNB - ALL patient names are entirely made up.\nAs a reminder, for this task we need to create a column that mimics Excel’s floor function and reduces the MovementDateTime field to the nearest 15 mins. We also want to get a count of how many patient were either moving IN or OUT during each 15 minute segment of the day.\n\nSELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME]\nGO\n\nYou’d need to replace the database and table names to suit. I’m not going to explain the code for flooring the datetime field - just know that it works, but you may want to compare the syntax for the case when statement with the equivalent {dplyr} code (see later).\nHere is the table output - with the 2 new columns at the end:\n\nNow things get more complicated.\nI have a counter field, but I want to get a cumulative count by each 15 minute segment, staging post and whether this was a movement in or out.\nOne way to do this is to wrap the original query inside another query, so that our newly created counter column can be utilised. This is a similar idea to the the method of mutating a column in {dplyr}, and having it available within the next pipe.\nWe have to make use of SQL’s windowing functionality to create virtual groupings and orders within the data ( SQL is a set based language, and there is no concept of row order within a set. Therefore to get a cumulative count, we need to make SQL think in terms of rows by partitioning the data by the desired grouping columns and providing columns to order by):\n\nSELECT        x.[MovementDateTime],\nx.[FirstName],\nx.[LastName],\nx.[Ward_Dept],\nx.[Staging_Post],\nx.[Movement_Type],\nx.[IN_OUT],\nx.[Movement15],\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x\nUnderstanding windowing techniques is a great SQL skill to have. Don't forget where you first saw this ;)!\n\nUnderstanding windowing techniques is a great SQL skill to have. Don’t forget where you first saw this ;)!\nA couple of things to note here are that when we wrap or “nest” the original query, I gave it the alias ‘x’. You do need to provide an alias for this inner query, or the outer query won’t work. Although not strictly necessary, I also prefixed the column names in the outer query so it’s clear that I am selecting the columns from the “virtual” table defined by the inner query.\nHere’s the output with our new Row number (or RNumber) field.\n\nAlmost done, but this is still not in the right format - I need to get an accurate cumulative count. Once more, I take the previous query, and nest that inside a new query - so you can see this is similar to lots of base R style manipulation where the code starts from the middle, or an end, and works back.\n\nSELECT y.MovementDateTime,\ny.FirstName,\ny.LastName,\ny.Ward_Dept,\ny.Staging_Post,\ny.Movement_Type,\ny.IN_OUT,\ny.Movement15,\ny.[counter],\ny.[counter] * y.R_Number AS Movement_15_SEQNO\nFROM (\nSELECT x.MovementDateTime,\nx.FirstName,\nx.LastName,\nx.Ward_Dept,\nx.Staging_Post,\nx.Movement_Type,\nx.IN_OUT,\nx.Movement15,\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x) y\nORDER BY MovementDateTime\nGO\n\nTo recap - our first query floored the movement time to 15 minute intervals and gave us a counter field, we then used that counter field to generate a row number field. Now, even if I’d ordered the result of the second query by MovementDateTime, it still wouldn’t suffice because the rownumbers are all positive, and I want them to be negative when the movement was a movement OUT.\nWe can’t manipulate the row number field within the same query that it is created, so we nest the whole lot once more, this time arranging in the correct time order and multiplying the counter field by our row number field.\nYou’ll notice the second query has been aliased (with a ‘y’) and the columns prefixed so that is is clear exactly where the query is obtaining the data from.\nThis gives us our final output:\n\nA reminder of the {dplyr} code I used:\n\nlibrary(tidyverse)\n\nplot_data &lt;- data %&gt;%\n  mutate(Movement15 = lubridate::floor_date(MovementDateTime,\"15 minutes\")) %&gt;%\n  group_by(IN_OUT, Movement_Type,Staging_Post,Movement15) %&gt;%\n  mutate(counter = case_when(\n    IN_OUT == 'IN' ~ 1,\n    IN_OUT == 'OUT' ~ -1)) %&gt;%\n  mutate(Movement_15_SEQNO = cumsum(counter)) %&gt;%\n  ungroup()\n\nAnd here is the output - compare with above:\n\nA lot more elegant? Definitely.\nAnother approach to writing the code in SQL would be to use a Common Table Expression, which is a more straightforward of writing and reading it. It’s a similar idea in that you create virtual tables with queries that then run top to bottom until you get your final output. However that is a post for another day :)\nWhat I hope you get from this post is that {dplyr} and other packages ({lubridate} for example) really do make life easier for data manipulation.\nLook at the SQL for flooring the date, compared to the {{lubridate}} call. Look at the elegance of mutating new columns and having them available within the next chain, compared to horrendous multi-layered nested queries (this one was pretty tame - imagine a few more levels on top of that). You can see how traditional SQL can get unwieldy.\n{dplyr} is a fantastic asset to the R community, and I hope it might prove to be a great hook to get R further established within the analytical departments of the NHS.\nThis blog was written by John MacKintosh, NHS data analyst based in Inverness, Scotland, and was originally posted on his blog site johnmackintosh.net.\nThis blog has been edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/github-action-spelling-check.html",
    "href": "blog/github-action-spelling-check.html",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "",
    "text": "Spell checking can be a really important feature of coding, particularly when creating reports, dashboards or websites. Some R solutions have spelling checks built into them, for example RStudio has a spell check function in the IDE (with underlining words that aren’t recognised in the Visual view) and Golem (“a framework for building robust Shiny apps”) has the spelling R package built into it. But when you are creating reports at a fast pace or building dashboards in flexdashboards, Shiny or Quarto, then mistakes may get missed. Spelling, being what it is, can also be incredibly hard to spot: a dropped letter or a switch can be easily overlooked and if you have several files to check you will need a quick and reliable way to do this."
  },
  {
    "objectID": "blog/github-action-spelling-check.html#github-actions",
    "href": "blog/github-action-spelling-check.html#github-actions",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "GitHub actions",
    "text": "GitHub actions\nGitHub actions work, as the name suggests, on GitHub and are stored in the folder .github at the root (the main folder) of the Project. When a Pull Request is made to the main branch, which is good practice, you can trigger actions to flag up issues to resolve before merging.\nWhilst it is possible to create GitHub actions using R, Python is a more usual language, particularly in places like the GitHub Marketplace which is where the PySpelling can be found.\nThe NHS-R Way was one of the first repositories I tried out this GitHub Action and I needed 3 files:\n\n.wordlist.txt (in the main folder)\n.spellcheck.yaml (in the main folder)\nand in the folder .github/workflows/ the file spellcheck.yml\n\n\n\n\n\n\n\n\nHidden files\n\n\n\n\n\nFiles starting with a . may be hidden from view and can be seen in File Explorer by going to View &gt; select Hidden items. However, even doing this may not mean the file shows in the RStudio Files pane.\n\n\n\nAll the files can be copied without changes but the .wordlist.txt requires specific spellings so you may wish to remove some or all of those used in NHS-R Community in case some of the words added are not suitable for your Project."
  },
  {
    "objectID": "blog/github-action-spelling-check.html#running-the-github-action-to-find-spellings-to-save",
    "href": "blog/github-action-spelling-check.html#running-the-github-action-to-find-spellings-to-save",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "Running the GitHub Action to find spellings to save",
    "text": "Running the GitHub Action to find spellings to save\nAdding the three files will undoubtedly fail the GitHub Action when first pushed because it’s likely you will have a spelling that is not recognised by a universal dictionary. Also the checks are case sensitive so if you have any references which change from upper to lower case, these will both need to be listed, like NHS and nhs.\nIt is possible to expand the GitHub Action page on GitHub to view the log of previously run actions and you will need to do this when it fails. For the spelling GitHub action, each spelling “mistake” is shown between lines of dashes:\n--------------------------------------------------------------------------------\nNHSRpopulation\n--------------------------------------------------------------------------------\nI initially searched for the ----- lines using Ctrl+F through the browser but it turns out the search doesn’t extend to the log. I had missed that the log has its own search which appears at the top of the log webpage.\n\n\n\n\nIt’s then a manual task going to the wordlist.txt and adding the words and if there are true spelling mistakes it’s possible to search all files in RStudio by using the Ctrl + Shift + F rather than the Ctrl + F to locate the spelling mistake and correct it.\n\n\n\n\n\n\nFailed GitHub Actions\n\n\n\n\n\nEven when a GitHub Action fails the Pull Request can still be accepted.\nSometimes GitHub Actions fail and just need rerunning some time later!\nTo rerun go to the Actions tab, select the last failed action and in the top right will be a button to select Re-run all jobs or Re-fun failed jobs."
  },
  {
    "objectID": "blog/github-action-spelling-check.html#forcing-commits-on-github",
    "href": "blog/github-action-spelling-check.html#forcing-commits-on-github",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "Forcing commits on GitHub",
    "text": "Forcing commits on GitHub\nIt’s possible to see on the Pull Request that there are multiple force-pushed commits (10 times!). Because there were so many spellings I did this in a few tries, adding a load, pushing the changes and re-running the action. I also was adding them in alphabetical order at first, until I realised I was never going to get through all of these before Christmas (and it’s February!).\nOn the one hand it’s ok to push all the changes to one commit as they are all related but on the other hand you will see this commit has 121 file changes to it as I had to tidy up a few files (see the later section about a rogue apostrophe). Whilst this was a decision that is ok to make: which is worse, more commits or more file changes, it is never recommended to force push to main or a branch that people are collaborating on because it changes history by changing the commit label. As I was working on a branch and within in Pull Request to main, I’ve reasonably assumed that it’s only me that will be affected by these changes.\nThe code to force a push is as follows:\n\nstage (add) all files (note the dot)\namend the previous commit\nforce the push\n\ngit add . \ngit commit --amend --no-edit\ngit push -f"
  },
  {
    "objectID": "blog/github-action-spelling-check.html#updating-a-wordlist-dictionary-with-multiple-words",
    "href": "blog/github-action-spelling-check.html#updating-a-wordlist-dictionary-with-multiple-words",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "Updating a wordlist dictionary with multiple words",
    "text": "Updating a wordlist dictionary with multiple words\nThe NHS-R Community website had over 900 specific words to add (some were generated from weblinks) and when I (finally) looked for code to speed things up I realised I could just add all the spellings into the list as I found them, import into R to order and remove duplicates. Before importing and sorting just add Header or something similar to the top of the word list as this will become the column name and removed as part of the process.\n\nlibrary(here) # a package which is useful for referring to the project's path\nlibrary(dplyr)\n\nfile_path &lt;- paste0(here::here(),\"/.wordlist.txt\")\n\n# Check to see if the file exists (I misspelt this originally as .worldlist so \n# ironically couldn't find the file!)\nfile.exists(file_path)\n\n# Import the .txt file into R as a dataframe\ndf &lt;- readr::read_delim(file_path, delim = \"\\t\")\n\n# Order and remove duplicates\ndf2 &lt;- df |&gt; \n  dplyr::arrange(Header) |&gt; \n  unique() \n  \n# Write the text file to test.txt, copy over to .wordlist.txt if everything is ok\n# or change the file name to .wordlist.txt and overwrite the original \nwrite.table(df2, \"test.txt\", \n            sep =\"/\",\n            col.names = FALSE, \n            row.names = FALSE,\n            quote = FALSE)"
  },
  {
    "objectID": "blog/github-action-spelling-check.html#replacing-multiple-and-non-ascii-characters",
    "href": "blog/github-action-spelling-check.html#replacing-multiple-and-non-ascii-characters",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "Replacing multiple and non-ascii characters",
    "text": "Replacing multiple and non-ascii characters\nCopying over the files from WordPress brought across the ’ apostrophe which doesn’t appear to get recognised correctly even when the same is used in the .wordlist so I used the following code from a Gist to find and replace it to ' in multiple files. The only change I made to the Gist was to expand on the list.files base R function to change the pattern to find.qmd files and also show the full path because I was changing files in a subfolder (doing this means that the code to setwd() isn’t necessary):\n\nlibrary(here) # use instead of setwd() to show the file path rather than change it\n\npath_to_subfolder &lt;- paste0(here::here(), \"/blog\")\n\nlist.files(path_to_subfolder, pattern = \".qmd\", full.names = TRUE)"
  },
  {
    "objectID": "blog/github-action-spelling-check.html#conclusion",
    "href": "blog/github-action-spelling-check.html#conclusion",
    "title": "NHS-R Community GitHub Actions - spelling",
    "section": "Conclusion",
    "text": "Conclusion\nYou can go a very long time without needing to use GitHub Actions or even spell checks on your scripts but they can be really convenient and powerful bits of code that can speed up a possibly manual process."
  },
  {
    "objectID": "blog/nhs-r-newscast-28th-october-2022.html",
    "href": "blog/nhs-r-newscast-28th-october-2022.html",
    "title": "NHS-R newscast 28th October 2022",
    "section": "",
    "text": "There is a new newscast podcast available, for those of you who don’t listen to podcasts here is a summary of what we talked about. Listen to the episode, find it wherever you get your podcasts."
  },
  {
    "objectID": "blog/nhs-r-newscast-28th-october-2022.html#linting-part-deux",
    "href": "blog/nhs-r-newscast-28th-october-2022.html#linting-part-deux",
    "title": "NHS-R newscast 28th October 2022",
    "section": "Linting part deux",
    "text": "Linting part deux\nStatic code analysis\n{lintr}\ndiagnostics in rstudio\nLinting in vs code\n{styler}\nGithub actions to lint code\nTidyverse style guide"
  },
  {
    "objectID": "blog/nhs-r-newscast-28th-october-2022.html#structuring-projects",
    "href": "blog/nhs-r-newscast-28th-october-2022.html#structuring-projects",
    "title": "NHS-R newscast 28th October 2022",
    "section": "Structuring projects",
    "text": "Structuring projects\nscripts, setwd when reading and writing local files\nProjects, self-contained and simpler to use (relative filepaths, folder structures, .gitignores)\nprojects as packages, simple function loading, and unit testing\ndevtools::load_all()\nusethis::use_package()\ntestthat::use_test()"
  },
  {
    "objectID": "blog/nhs-r-newscast-28th-october-2022.html#mit-vs-gpl",
    "href": "blog/nhs-r-newscast-28th-october-2022.html#mit-vs-gpl",
    "title": "NHS-R newscast 28th October 2022",
    "section": "MIT vs GPL",
    "text": "MIT vs GPL\nhttps://choosealicense.com/\nhttps://www.tidyverse.org/blog/2021/12/relicensing-packages/"
  },
  {
    "objectID": "blog/nhs-r-community-conference-ii.html",
    "href": "blog/nhs-r-community-conference-ii.html",
    "title": "NHS-R Community Conference II",
    "section": "",
    "text": "My journey to work takes me about an hour and a half, and I catch a couple of buses with Wi-Fi which means I can browse Twitter and invariably end up with hundreds of tabs open as I flit between articles and blogs. Most mornings I find it hard to concentrate on reading through entire articles, especially the really long ones, so I leave the tab open on my computer, often for days, before reading them. Given my experience of reading blogs, why would anyone want to read through mine about the NHS-R Community conference?\nIf I’d gone to the conference I’d probably skim that paragraph thinking ‘yes, I went, I know how good it was’.\nIf I’d not gone to the conference I’d probably skim that paragraph because I might prefer not to know just how great a conference was when I’d missed it!\nEven though the conference was moved to a bigger location to accommodate more people and around 250 people attended, I have still spoken to people who didn’t get a ticket or missed submitting an abstract to speak. People who never made the conference are talking about an event that is only in its 2nd year. What is going on? What is it that has made the event so successful?\nOrganising an event of any size takes a lot of work and that is often overlooked. There were the core people who did the real work – the arrangements – and quite frankly, they made it look easy, which itself is an indication of how hard they worked. But there were others who were part of a committee that chipped in with bits they could help with: setting up a specific email; reading through abstracts; suggesting things the organisers might consider, like how to ensure diversity of questioners (https://frompoverty.oxfam.org.uk/how-to-stop-men-asking-all-the-questions-in-seminars-its-really-easy/).\nThat organising committee was made up from a group who have shown a particular interest in R, and as such I found myself part of that group. Now although I have submitted a few blogs to NHS-R, I only really started using R a couple of years ago. Deep down I’m still a SQL analyst and my contributions to the conference were pretty minimal, but I feel encouraged to make those small contributions (even that last one about who gets to ask the first question in seminars) and each small involvement builds up to a bigger thing. This really is feeling like an equal and inclusive group and that’s where I think this success is coming from.\nIt may have been by design or it may be a happy accident but there is a crucial clue in the name of this group that gives away its success – Community. This conference wasn’t managed top-down. There are some key people, of course, but they are as much of this Community as the people who contribute to the blogs, those that stood up on stage and showed their work, have those that will be learning to run the R Introduction training. This is our NHS-R Community.\nIf you missed this year’s conference and want to go to the next one, get involved. The more people involved, the less work there is for everyone individually. Plus, given that tickets this year ran out in just 2 hours, you’ll be more likely to secure yourself a ticket.\nSpeaking of which, provisional dates for the next conference are the 2nd and 3rd November 2020 (Birmingham). Now aren’t you glad you read this blog!\nZoë Turner, Senior Information Analyst\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/what-is-a-proper-data-scientist-anyway.html",
    "href": "blog/what-is-a-proper-data-scientist-anyway.html",
    "title": "What is a “proper” data scientist anyway?",
    "section": "",
    "text": "I’ve talked to a lot of people over the years about whether they’re a “proper” data scientist. When I started writing code to analyse data there were no data scientists in the NHS, or none that I knew at least. In 2012 data science was called The sexiest job of the 21st Century and still I couldn’t see it anywhere in the NHS. I started calling myself a data scientist around 2017, and I started calling the people in my team data scientists too. In my book, if you’re trying to get better at data science, that means if you’re trying to know more about computing than a statistician, and more about statistics than a software engineer, you’re a data scientist, however great or terrible you are at it. I have a long running joke that I have a special sword in my office that I can use to “Knight” people and make them Proper Data Scientists.\nI had an experience that made me think about this long running issue recently. My team is working on quite a large complicated model that’s being rolled out at the moment. There was a big workshop coming up and we needed to get it looking the part. To meet the deadline I put everything else down for 1-2 weeks and wrote some code with the rest of the team. In my current role I basically never write any code which feels strange after 10 years of doing little else but that’s another blog post for another day. I was a little surprised, or I suppose not all that surprised, to find that I was a bit intimidated. I was sending pull requests for review to the person who wrote the code originally and I was a bit worried about what they’d think of me. This person is a helpful, friendly person I’ve known for a long time, never passes judgement on those who are still learning, they are line managed by me, and they know full well I haven’t written any code for a year (so might be sympathetic) but I was still worried about what they would think of my pull request.\nAnd quite honestly the other part of my job, the what the heck else I do all day as a data scientist would doesn’t write code, I don’t really feel like I know what I’m doing there either for a good chunk of the time. I came to my new role to stretch and challenge myself and it’s certainly working but I’m often finding myself doing something I’ve never done before (leading a conference) or something I’ve done lots of times but am just not particularly good at (faffing around with budgets and timesheets and all the other paperwork stuff). The point of this blog post is to say that in my experience that fear never really goes away. My advice would be to stretch and challenge yourself, sure, but don’t question whether you’re a “proper” anything. I’m as good at writing code as I’m ever going to be now I’m doing other stuff with my time, and I’ve really just started the long journey to growing and improving as a data science team manager. And that’s OK. And if you’re reading this and you’re trying to be a better data scientist, however great or terrible you are at it, you’re a data scientist. And I’ll bring my special sword to knight you as a proper data scientist to the next NHS-R conference to prove it.\nChris Beeley, Head of Data Science, Strategy Unit, MLCSU\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/spc-charting-in-r.html",
    "href": "blog/spc-charting-in-r.html",
    "title": "SPC Charting in R",
    "section": "",
    "text": "For some time the Information Department at Worcestershire Acute NHS Trust has been making use of statistical process control (SPC) charts for internal reporting purposes. This has formed part of our drive toward better decision-making (as recommended in NHSI’s Making Data Count https://improvement.nhs.uk/resources/making-data-count/).\nIn doing so we have made extensive use of NHSI’s Excel-based SPC tool and have also sought to implement this methodology outside of the confines of MS Excel (for example within our SQL/SSRS based reporting suite).\nAs the Department’s unofficial ‘R Champion’, I have driven efforts to increase my team’s knowledge and usage of R over the last six months. My experience with NHSI’s resources suggested that R offered a route to more efficient, consistent and quickly reproducible SPC charting. I set about developing a charting function within R which would replicate NHSI’s logic and methodology[1].\nI developed and tested a custom function in R which requires two primary arguments: a series of data points, and a set of accompanying date values. The function then creates a data frame containing the data series, its mean and moving average values, and upper and lower control limits. The series is then tested against NHSI’s methodology, and special cause variations are highlighted and colour coded. This formatting is done according to a secondary function argument which identifies whether an increase or decrease in the series indicates system improvement. This data frame is then visualised using ggplot, which displays the SPC and any additional information such as a performance trajectory or national target.\nI then tested the function and compared against our existing SPC reporting. A few logical ‘gremlins’ in the function were identified and subsequently removed, and once I was happy with the function it was integrated into a growing departmental R package (currently only internally available) for use in R Markdown reporting and our expanding R Shiny dashboard repertoire.\nMy next step was to use Shiny to create an SPC Wizard app, to enable colleagues without R knowledge to test and utilise the SPC function. The app allows users to supply CSV files containing multiple data series, and generate SPC charts with little or no effort. These can then be exported as image files for Trust reporting. The app allows users to make formatting changes to the chart such as customising main and axis titles, customising the frequency of axis labels and size of point and line geoms (chart objects) for lengthy data series. It also allows users to specify multiple data series at a time to create ‘small multiple’ SPC charts for simultaneous analysis.\nThe project provided an excellent challenge in developing my Shiny skills, and provided an opportunity to utilise the visually impressive and professional appearance of the ShinyDashboard package. Development of this Shiny app also led to a challenging project of setting up a Linux based Shiny server, to allow hosting of the app for colleagues to use.\nA key advantage of this function-based approach is that the SPC methodology is now available for use by all analysts within the Department, and can be implemented with a minimum of coding. One of the primary difficulties with SQL based SPC logic encountered by our team was the length of code required to produce the chart data, and therefore the increased risk of error when recycling this code for different reports. The simplicity and self-contained nature of the SPC function avoids this.\nHaving successfully tested and embedded the SPC function within an ad-hoc SPC wizard, I have continued to develop a Shiny Performance Dashboard for Cancer KPIs. This rapidly produces SPC charting for 2-Week-Wait Referral and 62-Day Cancer Treatment metrics from live data pulled from our SQL warehouse. I hope this will be the first of many dashboards to take advantage of an easily available and consistent SPC methodology, allowing our Department to create reports and dashboards which are better able to communicate the nature of changing time series to Trust decision-makers, and to track and evaluate the impact of operational management decisions.\nDespite the (at times steep!) learning curve involved, from creating the initial function and replicating NHSI’s SPC logic, to setting up the Shiny server and deploying apps for use, this project has been an excellent way to develop my R skills and to demonstrate the value in embedding use of R within our organisation, and making it part of our toolkit for ‘business as usual’ analysis.\nI hope that next steps for this project will be sharing our methodology with other NHS organisations, to allow further input and development of the methodology and reporting applications. Recently there have been discussions around a collaboration with other NHS Trusts and the Strategy Unit, regarding the possibility of developing an SPC package and shiny app to be available to all NHS organisations. If you would like to learn more or take part in the discussions, please join us on the NHS-R community slack channel (nhsrcommunity.slack.com) and let us know your thoughts on an SPC package, and what you might want to see as part of it!\n[1] For those not familiar with the Making Data Count resources, the SPC tool is based around a moving average measurement of sigma and significant variations in data based on the this value. These include the identification of any data points above or below three sigma; sequences of consecutive data points above/below the mean; runs of consecutively increasing/decreasing data points; and two out of three data points at greater (or less than) than 2 sigma.\nThis blog has been formatted to remove Latin Abbreviations.\nThe original comments form the WordPress site:\n\nedwatto\n13 March 2020\nGreat stuff - well done. We have often found that consistency with SPC charts can be as important than statistical accuracy… Please share the code!! It doesn’t have to look pretty, be well commented or have examples (to start with) - just drag and drop onto github at https://github.com/new. The community will thank you :)\nSimon Wellesley-Miller\n13 March 2020\nPlease share. We have been doing something locally and trying to adapt QCC package but would be really interested to see what you have done.\nChristopher Reading\n13 March 2020\nThe code is currently available on my github page, apologies if it’s a bit messy! https://github.com/chrisreading01/SPCwizard You can also see the shiny app live, kindly hosted by Chris Beeley from Nottinghamshire NHS Trust: http://suce.co.uk:8080/apps/spc_wizard/ As mentioned in the blog there are some great conversations going on in the slack thread on SPCs.\nVicky Cross\n18 March 2020\nHi Chris, This looks great! I’m trying to have a play around with the SPC Wizard, but I don’t think I’m getting the CSV structured properly, please could you upload a sample CSV file so I can see what I’m doing wrong? Thanks!\nChristopher Reading\n18 March 2020\nHi Vicky I’ve put a sample file on my github page which works with the tool. https://github.com/chrisreading01/SPCwizard/blob/master/sampledata2.csv Let me know if you have any questions! Best Chris\nAndrew Ward\n25 March 2020\nHi Chris. This looks great, I’ve used the shinyapp and the charts produced look exactly what we are trying to recreate in our Trusts (as I’m v=certain other Trusts are. (I’m currently producing these charts in excel each month) I’ve copied R script and sample data file (imported to r to create two fields ‘data’ (GP referrals) and ‘dates’ (date) but I seem to be hitting some errors at the create df section. Error in UseMethod(“mutate_”) : no applicable method for ‘mutate_’ applied to an object of class “function” &gt; xaxis xaxisdisplay &lt;- xaxis[seq(1,length(xaxis),x.axis.breaks)] Error in seq.default(1, length(xaxis), x.axis.breaks) : object ‘x.axis.breaks’ not found Any help would be gratefully received Many Thanks Andy\nChristopher Reading\n26 March 2020\nHi Andrew If you can email me your script, I’d be happy to take a look and try to find what’s causing the error. My email is christopher.reading1@nhs.net Bw Chris\nAndrew Ward\n26 March 2020\nMany thanks Chris I’ve emailed the file to you. Kind Regards Andrew\nAaron\n1 June 2021\nHi Chris, There is a package called “bslib” that is required to load the shinyWidget. I install the package, but then when I load the library shinyWidget, I get the following error message: Error: package or namespace load failed for ‘shinyWidgets’ in loadNamespace(j &lt;- i[[1L]], c(lib.loc, .libPaths()), versionCheck = vI[[j]]): there is no package called ‘bslib’ I tried installing “bslib” and I get the message: Error in library(bslib) : there is no package called ‘bslib’ Did you run into this problem? Thanks,\nASM\n11 June 2021\nI’m having the same issue. Just getting started…\nChristopher Reading\n14 June 2021\nHi both, have you tried installing the bslib package using the install.packages() function before trying to load it with the library function()? Let me know if you are still unable to load the package, I can upload a copy of the app to GitHub without the shinyWidgets library as this is only loaded for cosmetic purposes in the app.\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-open-source-public-datasets.html",
    "href": "blog/nhs-open-source-public-datasets.html",
    "title": "NHS open source public datasets – creating realistic synthetic datasets",
    "section": "",
    "text": "This blog is an initial attempt to garner interest in a project to create NHS synthetic datasets in a range of fields and also to understand the underlying principles around creating synthetic healthcare data and its intricacies.\nHealthcare data is increasingly electronic. With the huge datasets that the NHS is in the process of collecting for patient care comes an impetus to standardise data collection between hospitals and trusts. This is good for patient care not least because it allows for standardised analysis of large datasets. This standardisation is in progress across many of the medical specialities but several have already reached a mature stage. A good example is the National Endoscopy Database which aims to automatically collect data from all endoscopic procedures in the UK according to a standardised template. This will allow for the analysis of variation in endoscopic performance, quality and outcomes amongst many other outputs.\nThe analysis of these datasets will need to be validated, reproducible and of course progressive as the desired metrics change. The analyses will therefore require two things\nOngoing input from analysts to maintain the methodology and code base to perform the analysis.\nCreative ideas for the representation of the datasets.\nHS datasets and the methods for their manipulation are likely to attract a lot of interest from diverse sources such as pharmaceutical companies to healthcare software developers and academic researchers. By restricting access to the datasets because of privacy issues we also, by necessity restrict the speed at which solutions can be found using these datasets as analysis will only be carried out by a small sample of authorised analysts\nThe obvious solution here is to create NHS datasets which are constructed according to accepted and used data templates, but to populate them with synthetic data and then to allow open source access to these datasets.\nSynthetic data is not always easy to create. The vast majority of NHS electronic data is still semi-structured free text. Reports also have to make sense internally so that, for example, an endoscopy report describing a stomach ulcer has to contain text that is relevant to the ulcer finding. It gets even more complex when further reports are written that reference a report in another dataset. An example is the histopathology report from a biopsy taken from the stomach ulcer. This biopsy report will obviously have to be reporting on the stomach ulcer and the text will be about pathology findings relevant to the description of an ulcer.\nAn example of an attempt at creating a synthetic medical dataset can be found here using the above example (https://github.com/sebastiz/FakeEndoReports). This contains some description of how the reports are created and I have tried to derive some principles regarding how to make fake healthcare datasets in general based on this example.\nThere are of course many other datasets that would be incredibly powerful if they were created an open sourced. One such datasets is the NHS patient administration system on which most statistics about waiting times and patient pathways are based. Another is the Hospital Episode Statistics (HES) which collect information regarding all NHS appointments (in-patient and outpatient) and which is being used to create data linkage between a wide range of data repositories (My attempt at creating synthetic HES data can be found here but is still incomplete at the moment: https://github.com/sebastiz/HesMineR)\nR is the perfect language to create such synthetic datasets and it would be a valuable addition to the NHS-R armamentarium to have a package that contained synthetic NHS datasets so that open source solutions can be more quickly and creatively derived.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/thoughts-on-the-nhs-r-conference.html",
    "href": "blog/thoughts-on-the-nhs-r-conference.html",
    "title": "Thoughts on the NHS-R conference",
    "section": "",
    "text": "It’s been a few weeks since the first NHS-R conference was held in Birmingham.\nI co-presented a couple of workshops with Neil Pettinger on visualising patient flow, covering the following\n\nimporting from Excel (and connecting to SQL Server)\ndplyr\nggplot2 & plotly\ngifski for basic animation\nautomating reports with officer\ngganimate demo\ntidy evaluation via a custom plot function\nalternative flow plots with ggalluvial\na simple Shiny app\nCode, data and templates for the workshop are available here.\n\nI hope those who attended found it useful.\nThe event itself was a huge success. I’d have loved to have been able to see all the sessions that were taking place.\nThere were a series of lightning talks – there are Trusts using R for machine learning and classification, and others using it for predicting admissions Chris Beeley presented on the use of Shiny, while Jacob Anhøj led a session on SPC using his qicharts2 package. It was a huge honour to meet Jacob, I’ve used his package almost since inception, and I know how much effort has gone into the rigorous analysis the package provides.\nI was also very pleased to meet Chris (Beeley), Gary Hutson, Ed Watkinson, Zoë (who masquerades on Twitter as Applied Information Nottingham) and Garry Fothergill face to face, and to see Val Perigo, Paul Stroner and Professor Mohammed Mohammed again. I also want to say thank you to Shahima who helped me revise my incredibly shoddy attempt at a bio, and for organising the event.\nFrom my point of view, the workshop was enjoyable to deliver. There were a couple of technical glitches ( I couldn’t share my laptop screen, so had to project it and then fly blind , which led to me spending more time facing the screen than the audience, which was not ideal), but even with that, we got through the material and it all worked within the allotted time.\nIt was great to be in a room full of analysts passionate about R and the NHS. Neil mentioned ‘community’ in his blog post, and it is true. There was a great vibe, as I’d expect to be honest. I’ve never been one for being part of a gang but anyone into R in the NHS is instinctively all right by me 🙂 I’m pretty sure there was enough brain power in that room to tackle any analytical challenge that could get thrown at the NHS. The challenge is in harnessing that power , promoting R as the incredible tool that it is, and enabling us to work collaboratively rather than in silos. If only there was an R package for that..\nThis blog was written by John MacKintosh, Data Manager at NHS Highland and was originally posted on John Mackintosh’s own blog Data by John\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/introduction-to-funnel-plots.html",
    "href": "blog/introduction-to-funnel-plots.html",
    "title": "Introduction to Funnel Plots",
    "section": "",
    "text": "Funnel plots are a common tool for comparing organisations or units using proportions or standardised rates. A common use of them is for monitoring mortality at hospitals. This is an introductory post on the subject, that gives a little information about them and how they are constructed. It is deliberately light on theory, focussing on use, some of the theory is referenced for interested readers.\nThis post also uses a funnel plot function, for indirectly standardised ratios, that I built as part of my PhD work. The function is based on {ggplot2} (Wickham 2009), and is available at https://github.com/chrismainey/CMFunnels, although it’s a work in progress.\nThis post was original written for my own blog (https://mainard.co.uk/post/introduction-to-funnel-plots/), built using RMarkdown, with {blogdown}, using a Hugo site. Code is available at https://github.com/chrismainey/Mainard/tree/master/content/post\nThere are different kinds of funnel plot, but this post focusses on the type used to compare standardised mortality and other similarly constructed indicators."
  },
  {
    "objectID": "blog/introduction-to-funnel-plots.html#rationale",
    "href": "blog/introduction-to-funnel-plots.html#rationale",
    "title": "Introduction to Funnel Plots",
    "section": "Rationale",
    "text": "Rationale\nHow do you go about comparing organisations? We could simply look at indicator data and rank them, but that could be unfair if the conditions are different at each organisation. For example every hospital differs in size, the services it offers, and the patients it sees. We might expect a hospital seeing a higher proportion of elderly patients to have a higher mortality rate. Is it fair to compare it to an organisation serving a younger population who may be ‘healthier’ in general? Naively comparing organisations by ranking in league tables has been shown to be a bad idea (Goldstein and Spiegelhalter 1996; Lilford et al. 2004).\nThis scenario is not a million miles away from the techniques used in meta-analysis of clinical trial, where we may have trials of different sizes, with different estimates of effect, and differing variances. Some of the techniques applied to meta-analysis have been adapted for healthcare monitoring, including funnel plots and methods to adjust for overdispersion (Spiegelhalter 2005a, 2005b; Spiegelhalter et al. 2012)."
  },
  {
    "objectID": "blog/introduction-to-funnel-plots.html#example",
    "href": "blog/introduction-to-funnel-plots.html#example",
    "title": "Introduction to Funnel Plots",
    "section": "Example:",
    "text": "Example:\n\nlibrary(ggplot2)\nlibrary(tidyr)\n# Make up some data, as if it was from a regression model with observed and predicted (expected) events.\ndt &lt;- data.frame(\n  observed = c(15, 40, 72, 28, 50, 66, 75),\n  expected = c(13, 32, 75, 33, 54, 60, 72),\n  unit = factor(c(\"A\", \"B\", \"c\", \"D\", \"E\", \"F\", \"G\"))\n)\n\n# Add a ratio (SR) of observed to expected, our indicator\ndt$SR &lt;- dt$observed / dt$expected\n\n# Scatter plot in ggplot\na &lt;- ggplot(dt, aes(x = expected, y = SR)) +\n  geom_point()\n\na\n\n\n\n\n\n\n\n\n# Now add a central line, in a ration like this, 1 is the average/expected value.\na &lt;- a + geom_hline(aes(yintercept = 1))\na\n\n\n\n\n\n\n\n\n# Add a 95% Poisson limit, by using the density function to get the quantile value for each 'expected'.\nlkup &lt;- data.frame(id = seq(1, max(dt$expected), 1))\nlkup$Upper &lt;- (qpois(0.975, lambda = lkup$id) - 0.025) / lkup$id\nlkup$lower &lt;- (qpois(0.025, lambda = lkup$id) - 0.975) / lkup$id\n\nlkup &lt;- gather(lkup, key, value, -id)\n\na + geom_line(aes(x = id, y = value, col = key), data = lkup)\n\n\n\n\n\n\n\nYou’ll probably notice the ‘jagged’ lines in the plot above. This is because the Poisson distribution is only defined on integers, and most common implementations of Poisson functions make some sort of rounding/guess between points. They are generally poorly defined on low values, but there are other options that I’ll discuss in another future post."
  },
  {
    "objectID": "blog/nhs-r-community-datasets-package-released.html",
    "href": "blog/nhs-r-community-datasets-package-released.html",
    "title": "NHS-R Community datasets package released",
    "section": "",
    "text": "Firstly, it is now available on CRAN, the major package repository for R, and can be installed like any other package, or directly from GitHub as follows:\n\ninstall.packages(\"NHSRdatasets\")\n\n#or\n\nremotes::install_github(\"https://github.com/nhs-r-community/NHSRdatasets\")\n\nWhy?\nSeveral community members have mentioned the difficulties learning and teaching R using standard teaching datasets. Stock datasets like iris, mtcars, nycflights13 and so on. are all useful, but they are out-of-context for most NHS, Public Health and related staff. The purpose of this package to provide examples datasets related to health, or reusing real-world health data. They can be accessed easily and used to communicate examples, learn R in context, or to teach R skills in a familiar setting.\nFor those of us wanting to contribute to Open Source software, or practise using Git and GitHub, it also provides an opportunity to learn/practise these skills by contributing data.\nWhat’s in it? At present we have two data sets:\n\n\n\n\n\n\n\nName\nContributor\nSummary\n\n\n\nLOS_model\nChris Mainey\nSimulated hospital data, originally for learning regression modelling\n\n\nae_attendances\nTom Jemmett\nNHS England’s published A&E attendances, breaches and admission data\n\n\n\nBoth datasets have help files that explain the source, data columns, and give examples of use. The package also contains vignettes that show the datasets being put to use. You can find them in the help files, or using:\n\n# list available vignettes in the package:\nvignette(package=\"NHSRdatasets\")\n\n# Load the `ae_attendances` vignette:\nvignette(\"ae_attendances\")\n\nHow can you contribute?\nWe are aiming to build a repository of different health-related datasets. We are particularly keen to include primary care, community, mental health, learning disabilities, or public health data.\nPlease head to the GitHub page, and follow the submission guidelines on the README page. If you would like to add a vignette or alter an existing one, follow the same instructions: fork the repository, commit and push your changes, then make a pull request. We will then review your submission before merging it into the package.\nPlease ensure that any data you submit is in the public domain, and that it meets all obligations under GDPR and NHS/Public Health information governance rules.\nSummary\nThe {NHSRdatasets} package is a free, collaborative datasets package to give context for NHS-R community when learning or teaching R. The package is available on CRAN. We actively invite you to submit more datasets and help us build this package.\nTo read the reference material, and vignettes, you can find them in the package help files or go to the pkgdown website: https://nhs-r-community.github.io/NHSRdatasets/.\nChris Mainey, Intelligence Analyst, Health Informatics – University Hospitals Birmingham NHS Foundation Trust\nTo see the package code, or contribute, visit https://github.com/nhs-r-community/NHSRdatasets.\nThis blog has been formatted to remove Latin Abbreviations and edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/the-health-service-modelling-associates-hsma-programme-the-future-of-analytics-for-health-social-care-and-policing.html",
    "href": "blog/the-health-service-modelling-associates-hsma-programme-the-future-of-analytics-for-health-social-care-and-policing.html",
    "title": "The Health Service Modelling Associates (HSMA) Programme – the future of analytics for health, social care and policing?",
    "section": "",
    "text": "5 years ago our team brought into reality a vision that we’d had for some time.  We recognised the vital importance of modelling, simulation, analytical and data science techniques to inform decision making in health and social care organisations.  Indeed, our very existence was born out of a need to bring more of this stuff into practice – to work with health and social care partners to use these fancy methods to give them evidence to help them make decisions about problems they were facing.  And we had (and continue to have) huge success from this – impactful work that makes a real difference to services and their patients.  We’ve helped to transform the speed at which stroke patients can receive clot-busting treatment ().  We’ve reduced the time taken to send bladder cancer patients for treatment in Cornwall by over 5 weeks when every day lost reduces their chances of survival (.  We transformed the way in which dialysis services could handle the COVID-19 pandemic ().\nThis is all great, and it’s the reason we do what we do.  It’s what led us to becoming applied researchers in the first place.  But we’re a small team, with limited resource, and there’s only so much we can do.  We saw that what was really needed was a push to build skills within health and social care organisations to be able to do this kind of stuff themselves – as a routine part of what they do.  We needed to not only talk to them about what these methods could do, but show them how to do it.  We also saw that analytical staff working in health and social care organisations are highly capable people who could do so much more with the data they’ve got, if only they had the knowledge (and time) to apply some of these methods.\nSo we introduced the Health Service Modelling Associates programme – or HSMA for short.  The aims of HSMA were simple – help analysts develop skills in modelling, simulation and data science and help their organisation see the value by getting the newly skilled analyst to immediately put these skills into practice to solve a problem that’s important for the organisation.  HSMAs get released for a day a week for a year, during which they get lots of training from us at the start, and then run with a project with our support and guidance.\nHSMA has had a huge impact for the HSMAs, for their organisations and for patients.  HSMA projects have led to brand new £multi-million mental health and urgent care facilities, improved discharge processes, improvements to ambulance service delivery.  One of our HSMA projects helped establish “Crisis Cafés” in Devon, giving a space for those in mental health crisis to sit and talk.  We’ve got a GP who, six months ago, had never done any programming before in his life.  He’s just built a generic vaccination delivery model that not only predicts queue lengths and how long people are in the system, but the risk of social distancing breaches in waiting areas, and overcrowding in the car park.  He’s made it freely available online so that anyone can use it for their own centres, or even develop it further.  Most HSMAs have seen rapid career progression because of their new skills, and a few of them are now even running their own teams of modellers in their organisations.That’s just not happened before.\nHSMA has grown significantly.  The first year – we had 7 applications, and we took on 6 of them.  For the next round, we had 52 applications and took forward 26.  In the current round, we took on over 50 HSMAs.\n\n\n\nThe PenCHORD team with just some of the HSMAs from our 2018 round. I’m the one in the middle beaming with pride at what they managed to achieve.\n\n\nBut the programme hasn’t just grown in numbers, but in scope and ambition.  The 2 day training course that we had in the first year?  That’s now a 17 days of content teaching everything from how to program in Python and R, to how to build a simulation model of a system and use it to ask “what if?” questions, to building an AI that can make decisions or extract information automatically from free text.  All the approaches we teach are Free and Open Source, so models can be freely shared and knowledge and skills passed on.  We’ve got policing associates now too – so health, social care and policing staff can work together to tackle system-wide issues.  And we’ve gone completely online – a necessary step when launching in a pandemic, but with the significant benefit that we can extend our reach.\n\n\n\nSimon Wellesley-Miller – Information Manager at Devon Partnership Trust – talking about his HSMA project which modelled the resourcing requirements to establish new crisis cafés in Devon for those in need of a calm space, a cuppa and a chat.\n\n\nAnd that brings us to HSMA 4.  Since its inception, HSMA has exclusively focused on the geographic patch our organisation is funded for – the South West Peninsula.  But we’ve had a lot of people across England who want to be involved.  And now, when we’re in the midst of trying to deal with the medium to long-term impacts of the global pandemic, and there are common challenges being faced nationally, seems like a good opportunity to see if HSMA can help.\nSo in October, HSMA 4 arrives, and anyone in England who’s working in health, social care or policing organisations can apply.  That includes people working in organisations “close to” this too – so those of you working in public health, or for third sector organisations, are very welcome.  HSMA 4 will be all online – just like HSMA 3.  You’ll get a mix of live lectures with group and individual exercises delivered via Zoom, group work and access to support from the HSMA community on our Slack channel, and pre-recorded bonus content on our YouTube channel.  Oh – and that big 17 day training course from HSMA 3?  It’s got bigger – 23 days of training this time, with lots of new, exciting stuff to teach you.  The course is free too – it’s just the release of your time we need.\n\n\n\nA snapshot from one of our training sessions – here, we’re looking at how Python generator functions can be used to simulate individual arrivals into a system in a Discrete Event Simulation\n\n\nHere’s how it works – you’ll apply for the programme, and if you’re successful, you’ll spend the first three month phase (October to December 2021) being trained up on a smorgasbord of modelling, simulation and data science techniques, whilst thinking about how you could apply these methods to a project that addresses something important for your organisation.  Then, towards the end of Phase 1, you’ll pitch your project ideas.  We’ll take through a selection of projects, and then you’ll spend the 9 months of Phase 2 (January to September 2022) working on your project, with us as your mentors, and meeting with all the other HSMAs and mentors once a month to talk about your project progress, share ideas and collaborate.  If your project doesn’t get selected, you can still progress – each project will have a project team, led by the HSMA that pitched the project, but made up of other HSMAs who’ve also got these new skills.  We’re going to be encouraging you to pitch national collaborative projects that address issues of common importance across the country and across the health, social care and policing system.\nThis is a pilot for us, and we’re still that small team, so at the moment places will be limited – we’ll have a total of 50 places available – 20 for those working in health and social care in the South West Peninsula, 10 for those in the rest of England, 10 for those in policing organisations anywhere in England, and 10 for our new “Trainee Mentor Scheme” – for those of you who are already Operational Research or Data Science academics or practitioners and could mentor a HSMA project in Phase 2 (in return, we’ll give you lots of training in Free and Open Source methods and the opportunity to work and publish with some brilliant people on some impactful projects).  Next year, we’re hoping we can launch a massive HSMA 5 that vastly increases the number of places we can offer – we just need to find some funding to help build up our resource to allow us to do that.\nBut for now, if you’re reading this and thinking “I want to be part of this!” then register your interest here : .  It takes 60 seconds – we just need a few of your details so we can write to you in June and invite you to apply.  It helps us knowing what demand there is out there for this (and how many potential mentors could help us run the course!).  There’s a bit of information about the programme here too : .  If you’ve got any questions, email us at penchord@exeter.ac.uk.\nI hope at least some of you reading this will join us in October – it’s a great opportunity for you and for your organisations, but it’s also fantastic fun!  You’ll develop new skills that you can use throughout your careers.  And you just might work on a project that changes people’s lives.\nAnalytics will never be the same again.\nDaniel Chalk, NIHR Applied Research Collaboration for the South West Peninsula (PenARC) - Senior Research Fellow in Applied Healthcare Modelling and Analysis, and HSMA Programme Lead\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/rstudio-and-git-selecting-many-files.html",
    "href": "blog/rstudio-and-git-selecting-many-files.html",
    "title": "RStudio and Git – selecting many files",
    "section": "",
    "text": "I’m currently preparing a workshop for an introduction to Git and GitHub, in the same way we have an Introduction to R and RStudio course for NHS-R Community. But the thing I’ve realised as I’ve been putting my thoughts in order about where to start with Git and GitHub (using RStudio) is that I flit about from using the RStudio Git panel to using the Terminal command line in RStudio. I tend to go for the quickest action to achieve what I want to do which can mean things like:\n1 Use the Terminal git add . to “stage” all the files that I want to commit as that’s quicker than clicking on all the files often that I want to commit.\n2 Go to RStudio Commit Pending changes icon (the white docs icon with a tick in a Git pane) to write the commit as I find git commit -m \"Write your message here\" a bit too long!\n3 Use the Push and Pull buttons in RStudio as that’s easier than typing git push or git pull in the terminal.\nI’ve started trying out the pr_*() functions from the {usethis} package and following the Choose your own adventure by Garrick Aden-Buie so I’ll use those functions when working with branches and pull requests but more about that in the course/future blogs!\nThen, when looking for anything on working with Git on the Posit Cloud I found this YouTube video from Mine Çetinkaya-Rundel where I saw that it is possible to stage a number of files in RStudio more easily than clicking on each file! Mine shows selecting a file in the Git pane by clicking on the first click box by the file you want and, I’m guessing, hold down Shift and click the bottom file to select it and all between. The ticks just appear next to all the files!\nIt’s also possible to select non-concurrent files by clicking on the names and then, when the files you want have a grey highlight over them, click for a tick in “Staged”. To select files that are not concurrent use Ctrl and, if this isn’t familiar already, this is the same way to select files in things like Windows Explorer.\nNote too that this can be used to unstage files which is useful when you stage a folder which has lots of files in it but you change your mind about committing them all.\nSo now my work will probably go:\n1 Select the files I want to stage in the Git pane in RStudio\n2 Commit using the Commit pending changes button in RStudio\n3 Select the Push and Pull buttons in RStudio\nbecause, whilst I much prefer typing on the command line, sometimes clicking on a button makes life just a touch easier.\nThis blog is also available on GitHub as a quarto document.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/forecasting-r.html",
    "href": "blog/forecasting-r.html",
    "title": "Forecasting R",
    "section": "",
    "text": "Note on recording\n\n\n\n\n\nThis workshop was only available in person but recordings from on NHS-R Community’s YouTube include: Forecasting with Multiple Seasonality and a talk at the conference on Probabilistic Forecast Reconciliation For Health Services\nThe initiative sponsored by the International Institute of Forecasters provides cutting-edge training in the use of forecasting with R software in developing countries. There is no doubt that many people in developing countries cannot afford high fees to attend forecasting workshops. The project aims to make such a training accessible to those people and provide up-to-date training on the principles of forecasting and create a network of forecasters to conduct research on forecasting with social impact for less developed countries. The training has been delivered in Tunisia, Iraq, Senegal, Uganda, Nigeria, Turkey, Indonesia so far.\nWe expand the “Democratising Forecasting” initiative to deliver four forecasting workshops per year for the National Health Service (NHS), UK, in collaboration with NHS-R community. We give our time for free despite the debate that such organisations may afford to pay such training."
  },
  {
    "objectID": "blog/forecasting-r.html#program",
    "href": "blog/forecasting-r.html#program",
    "title": "Forecasting R",
    "section": "Program",
    "text": "Program\nStart: 09:30 a.m.\nEnd: 04:30 p.m.\nRefreshment breaks:\nMorning: 11:00 – 11:20\nAfternoon: 03:00 – 03:20 p.m.\nLunch: 12:30 p.m. – 01:30 p.m."
  },
  {
    "objectID": "blog/forecasting-r.html#webinar",
    "href": "blog/forecasting-r.html#webinar",
    "title": "Forecasting R",
    "section": "Webinar:",
    "text": "Webinar:\nTitle: Essentials to do forecasting using R\nDate: Two weeks before the workshop (to be confirmed)"
  },
  {
    "objectID": "blog/forecasting-r.html#day-1",
    "href": "blog/forecasting-r.html#day-1",
    "title": "Forecasting R",
    "section": "Day 1",
    "text": "Day 1\n1.1.Forecasting and decision making: What is forecasting? How forecasting task is different from other modelling tasks? What is the link between forecasting and decision making, how to identify what to forecast?\n1.2. Data preparation, and manipulation: how to prepare data for the forecasting task, how to clean data? How to manipulate data to extract time series?\n1.3. Time series patterns and decomposition: what could be used in data for forecasting task? how to detect systematic pattern in the data? how to separate non-systematic pattern?\n1.4. Forecaster’s toolbox: How to use time series graphics to identify patterns? what is a forecasting benchmark? What are the simple forecasting methods that could be used as benchmark? How to generate point forecasts and prediction interval using simple forecasting methods?\n1.5. Forecast accuracy evaluation: How do we know if the forecasting method captures systematic patterns available in data? How to judge whether a forecast is accurate or not? How to evaluate the accuracy of point forecasts and prediction interval? Why do we need to distinguish between fitting and forecast?\n1.6. Exponential smoothing models: What is the exponential smoothing family? what are available models in this family? What is captured by this family? how to generate point forecast and prediction intervals using exponential smoothing models?"
  },
  {
    "objectID": "blog/forecasting-r.html#day-2",
    "href": "blog/forecasting-r.html#day-2",
    "title": "Forecasting R",
    "section": "Day 2",
    "text": "Day 2\n2.1. ARIMA models: This is another important family of forecasting models. What is the ARIMA framework? what are available models in this family? What is captured by this family? how to generate point forecast and prediction intervals using ARIMA models?\n2.2. Regression: We also look at causal techniques that consider external variables. What is the difference between regression and exponential smoothing and ARIMA? How to build a simple regression model?\n2.3. Special events: In addition to using systematic patterns in time series, we discuss how to include deterministic future special events, such as holidays, festive days, and so on in models.\n2.4. Forecasting by aggregation: How to forecast in a situation where we have a high frequency time series, for example daily but we need a low frequency forecast, for example monthly? How do we generate forecast for items with a hierarchical or grouped time series structure?\n2.5. Forecasting by combination: how to use an ensemble of forecasting approaches? In which conditions ensemble forecasts are better than individual methods?\n2.6. Forecasting for many time series: what is the best approach to forecast many time series? How to classify items for forecasting? How to forecast them? How to report the accuracy?\nReference:\nWorkshop Booklet:\nMaterials will be provided for the workshop in RMarkdown."
  },
  {
    "objectID": "blog/forecasting-r.html#books",
    "href": "blog/forecasting-r.html#books",
    "title": "Forecasting R",
    "section": "Books:",
    "text": "Books:\n\nForecasting: Principles and Practice (2018), Rob J Hyndman and George Athanasopoulos.\nR for Data Science (2018), Garrett Grolemund and Hadley Wickham.\n\nBahman Rostami-Tabar, Associate Professor in Management Science, Cardiff University, UK\nThe blog has been edited for NHS-R Style and formatted to remove Latin Abbreviations"
  },
  {
    "objectID": "blog/r-studio-shortcuts.html",
    "href": "blog/r-studio-shortcuts.html",
    "title": "R studio shortcuts",
    "section": "",
    "text": "I love keyboard shortcuts. I work in R studio and using keyboard shortcuts has saved me a lot of time. There is a full list of short cuts and I have pulled together my three most used shortcuts.\nCtrl+enter or cmd+enter (Mac) will run the command where the cursor is and then move the cursor down. This is perfect for when you want to run your code line by line.\nCtrl+shift+m or cmd+shift+m (Mac) will insert a pipe (if you don’t already use pipes then you can learn more in R for Data Science.\nCtrl+shift+F10 or cmd+shift+F10 (Mac) will restart your R session. It unloads your packages but leaves the elements in your environment untouched. I use this a lot when I am jumping between scripts to minimise conflicts between packages.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/hexitime-is-shortlisted-as-a-finalist-for-the-hsj-partnership-awards-2021.html",
    "href": "blog/hexitime-is-shortlisted-as-a-finalist-for-the-hsj-partnership-awards-2021.html",
    "title": "Hexitime is shortlisted as a finalist for the HSJ Partnership Awards 2021",
    "section": "",
    "text": "Hexitime has been shortlisted for Best Not for Profit Working in Partnership with the NHS at the HSJ Partnership Awards 2021. This recognises outstanding dedication to improving healthcare and enabling effective collaboration across the NHS.\nFollowing the most testing period for the healthcare sector in recent history, this year’s awards reinstate the essential role of private and third sector organisations in strengthening the healthcare system. Being shortlisted for a Partnership Award has given our team the confidence in our worthwhile project and has ensured that our time saving project is nationally recognised, learned from and up scaled.\nThe judging panel comprised a diverse range of highly regarded figures across the NHS and wider healthcare sector. To be shortlisted as a finalist for these awards, despite tough competition from a pool of brilliant applications, is a mark of real achievement for the NHS-R Community, Hexitime and their partners KSSAHSN and Walsall Together. That Hexitime has been selected based on their diligence, ambition, and the positive impact that the project has had on both practitioners and patients within the health care industry.\nIn the last year, hundreds of hours have been exchanged across the country, bringing skills to where they are needed when they are needed and hence adding value to staff, organisation and networks. The partnership with the NHS-R Community, KSS AHSN and Walsall Together has allowed the platform to be provided free to use for all staff in health and social care, enabling them to collaborate across organisational and professional boundaries.\nHexitime Co-founders Dr. Hesham Abdalla and John Lodge commented:\n\nWe are delighted to have been shortlisted for Best Not for Profit working in partnership with the NHS, recognising the collaborative efforts and dedication of our team/s to successfully implement Hexitime. We are committed to delivering improved outcomes for patients, and to be chosen among the other incredible nominees is a wonderful achievement. This nomination has been a tremendous boost to both to our team and our NHS partners and I am sure it will bolster our continued efforts to improve health and care.\n\nNHS-R founder Prof. Mohammed Mohammed commented:\n\nHexitime enables us to mobilise, visualise and marvel at our amazing NHS-R community that is bringing 21st century open-source data science solutions to the NHS.\n\nHSJ editor Alastair McLellan comments:\n\nWe would like to congratulate Hexitime on being nominated in the category of Best Not for Profit working in partnership with the NHS ahead of HSJ Partnership Awards 2021. We are looking forward to welcoming them to the ceremony in June, to join us in recognising the very best collaborations and innovations in the healthcare sector. This year’s finalists are of an outstanding calibre and all of them are exceptionally dedicated to enhancing healthcare across the UK.\n\nThe winners will be selected following a rigorous, judging stage ahead of the HSJ Partnership Awards 2021 awards ceremony. The awards evening is expected to be attended by leaders and professionals from both the NHS and private sector as well as figures from non-clinical backgrounds to celebrate innovation and collaboration in healthcare.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/annotating-spc-plots-using-annotate-with-ggplot.html",
    "href": "blog/annotating-spc-plots-using-annotate-with-ggplot.html",
    "title": "Annotating SPC plots using annotate with ggplot",
    "section": "",
    "text": "Statistical Process Control (SPC) charts are widely used in healthcare analytics to examine how the metric varies over time and whether this variation is abnormal. Christopher Reading has already published a blog on SPC Charts.\nHere is a simple example of annotating points and text on SPC plots using {ggplot2} package. We won’t explain all the parameters in the annotate function. Instead we see this as a short show and tell piece with signposts at end of the blog.\nSo let’s get started and generate some dummy data from a normal distribution with a mean of 0 and and a standard deviation of 1.\n\nlibrary(tidyverse)\nset.seed(2020) # set the random number seed to ensure you can replicate this example\ny &lt;- rnorm(30, 0, 1) # generate 30 random numbers for the y-axis\ny &lt;- c(y, rep(NA, 10)) # add 10 NAs to extend the plot (see later)\nx &lt;- 1:length(y) # generate the x-axis\ndf &lt;- tibble(x = x, y = y) # store as a tibble data frame for convenience\n\nNow we can plot the data using {ggplot} function.\n\nfig1 &lt;- ggplot(df, aes(x, y)) +\n  geom_point(size = 2) +\n  geom_line() +\n  ylim(-4, 4) # increase the y-axis range to aid visualisation\n\nfig1 # plot the data\n\n\n\n\n\n\n\nOne of the main features of SPC charts are upper and lower control limits. We can now plot this as an SPC chart with lower and upper control limits set at 3 standard deviations from the mean. Although in practice the calculation of control limits differs from this demo, for simplicity we imply control limits and a mean as set numbers. Alternatively, you could use {qicharts2} package to do SPC calculations and then use the generated {ggplot2} object and keep following our steps.\n\nfig1 &lt;- fig1 +\n  geom_hline(yintercept = c(3, 0, -3), linetype = \"dashed\") + # adds the upper, mean and lower lines\n  annotate(\"label\",\n    x = c(35, 35, 35),\n    y = c(3, 0, -3),\n    color = \"darkgrey\",\n    label = c(\"Upper control limit\", \"Average line\", \"Lower control limit\"),\n    size = 3\n  ) # adds the annotations\n\nfig1 # plot the SPC\n\n\n\n\n\n\n\nRemarkably we see a point below the lower control limit even though the data are purely pseudo-random. A nice reminder that control limits are guidelines not hard and fast tests of non-randomness. We can now highlight this remarkable special cause data point which is clearly a false signal also known as special cause variation.\n\nfig1 &lt;- fig1 +\n  annotate(\"point\", x = 18, y = df$y[18], color = \"orange\", size = 4) +\n  annotate(\"point\", x = 18, y = df$y[18])\n\nfig1 # plot the SPC with annotations\n\n\n\n\n\n\n\nWe can now add a label for the special cause data point. You can play around with the vjust value (for example try -1, 0, 1) to get a feel for what it is doing to the vertical position of the label. There is also a hjust which operates on the horizontal plane.\n\nfig1 &lt;- fig1 +\n  annotate(\"label\", x = 18, y = df$y[18], vjust = 1.5, label = \"Special cause variation\", size = 3)\n\nfig1 # plot the SPC with more annotations\n\n\n\n\n\n\n\nTo learn more about the annotate function see https://ggplot2.tidyverse.org/reference/annotate.html\nThis blog has been formatted to remove Latin Abbreviations and edited for NHS-R Style\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-newscast-25th-may-2023.html",
    "href": "blog/nhs-r-newscast-25th-may-2023.html",
    "title": "NHS-R newscast 25th May 2023",
    "section": "",
    "text": "We have another newscast from the podcast, if you don’t do the whole podcast thing here are the notes and links from what we talked about.\nThe NHS-R Conference call for abstracts has been extended to 23rd June 2023 https://www.smartsurvey.co.uk/s/7FLFRY/\nwe’ve started an NHSR-Way book and also published through Netlify because some organisations block GitHub published pages. This book is inspired by The Turing Way and The GDS Way with more technical elements coming from the Statement on Tools book. For example, on licences we’ve detailed a lot of the options in the Statement on Tools but then said what NHS-R Community use in the NHS-R Way.\nThe NHSR book club have started reading the R4DS second edition and created an unofficial solutions book.\nZoë Turner will be talking next month at the Government Data Science Community Meet-Up: The Data Science Toolshed Tue 13 Jun 2023 1.30 to 3pm. Title to be confirmed but about how people can contribute to NHS-R open repositories or even pass them to NHS-R as has happened with the quarto NHS theme from Craig Shenton and {NHSRepisodes} package from Tim Taylor. Sign up to the event is free for Public Sector and Civil Service\nChris Beeley spoke at the DataConnect23 Government launch event and gave a taster session of text mining of patient experience data. Sign up to DataConnect events can be found at https://www.gov.uk/government/news/dataconnect23-is-on-its-way"
  },
  {
    "objectID": "blog/nhs-r-newscast-25th-may-2023.html#finds-from-the-slack-group",
    "href": "blog/nhs-r-newscast-25th-may-2023.html#finds-from-the-slack-group",
    "title": "NHS-R newscast 25th May 2023",
    "section": "Finds from the Slack group",
    "text": "Finds from the Slack group\nPython linter: https://github.com/charliermarsh/ruff\nLook at this great guidance on pie charts, unfairly maligned Pie charts from the Government Analysis Function.\nA retro graphic to brighten up the Python exception messages – https://github.com/James-Ansley/cowexcept (can feature a dragon as well as a cow!).\nFor an extension of retro visuals then check out Star Wars in the Linux Terminal – https://www.linuxfordevices.com/tutorials/linux/ascii-star-wars-linux-terminal"
  },
  {
    "objectID": "blog/a-simple-function-to-create-nice-correlation-plots.html",
    "href": "blog/a-simple-function-to-create-nice-correlation-plots.html",
    "title": "A simple function to create nice correlation plots",
    "section": "",
    "text": "The problem\nI was working with a dataset where I wanted to assess the correlation of different variables in R. As much as I like R – the outputs from the console window leave something to be desired (in terms of data visualisation). Therefore, I wanted a way to visualise these correlations in a nicer / cleaner / crisper way. The solution to this is to use a correlation plot.\nThe package I used for creating my correlation plots was the {corrplot} package, this can be installed and loaded into the R workspace by using the syntax below:\n\n# install.packages(\"corrplot\")\nlibrary(corrplot)\n\nAt this point I would encourage you to check out help for the corrplot function, as it allows you to pass a multitude of parameters to the function.\nDeconstructing the function\nAs mentioned previously, this plotting function has a multitude of uses, but all the parameters can be off putting to a newbie! This was me 6 years ago vigorously typing ‘how to do this with R relating to x’ and bombarding stackoverflow and other useful websites with questions. Shout out to RBloggers as well!\nThe function I have created uses the functionality of the {corrplot} packages, but it simplifies the inputs. I will include the function in stages to explain each step, however, if you just want to use the function and are not bothered with the underpinnings then skip the following section:\nStep 1 – Function parameters\nParameters of the function are as below:\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {}\n\nThe parameters to pass to the function are:\ndf_numeric_vals this means a data frame of numeric values only, so any categorical (factor) data needs to be stripped out before passing the data frame to the function; method_corrplot this is a numeric range from 1 – 5. So, for a shaded correlation plot you would use 1. Further examples of the various options will be discussed when I describe how the if statement works. colour_min this uses a gradient colour setting for the negative positive correlations. An example of an input here would be “green”. colour_middle this is the middle range colour, normally I set this equal to (=) “white”. colour_max this is the colour of the strong positive correlations For information on the strength of correlations, refer to this simple guide - what is R value correlation?.\nStep 2 – Creating the conditional (IF statement) to select correlation plot type\nThe below conditional statement uses the input of the function for example 1-5 to select the type of chart to display. This is included in the code block below:\n\nlibrary(corrplot)\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {\n  if (method_corrplot == 1) {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 2) {\n    type_var &lt;- \"number\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 3) {\n    type_var &lt;- \"pie\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 4) {\n    type_var &lt;- \"ellipse\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 5) {\n    type_var &lt;- \"circle\"\n    method_corrplot &lt;- type_var\n  } else {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  }\n}\n\nWhat does this do then? Well firstly nested in the function I make sure that the {corrplot} library is referenced to allow for the correlation plot functionality to be used. The next series of steps repeat this method:\n\nBasically, this says that if the method_corrplot parameter of the function equals input 1, 2, 3, and so on – then select the relevant type of correlation plot.\nThe type_var is a variable that sets the value of the variable equal to the string stated. These strings link directly back to the parameters of the corrplot function, as I know a type of correlation plot is equal to shade or number, and so on.\nFinally, the last step is to convert method_corrplot equal to the textual type specified in the preceding bullet.\n\nIn essence, what has been inputted as numeric value into the parameter that is. 1; set the type_var equal to a text string that matches something that corrplot is expecting and then set the method_corrplot variable equal to that of the type variable. Essentially, turning the integer value passed into the parameter into a string / character output.\nStep 3 – Hacking the corrplot function\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {\n  if (method_corrplot == 1) {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 2) {\n    type_var &lt;- \"number\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 3) {\n    type_var &lt;- \"pie\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 4) {\n    type_var &lt;- \"ellipse\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 5) {\n    type_var &lt;- \"circle\"\n    method_corrplot &lt;- type_var\n  } else {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  }\n\n  corrplot(cor(df_numeric_vals, use = \"all.obs\"),\n    method = method_corrplot,\n    order = \"AOE\",\n    addCoef.col = \"black\",\n    number.cex = 0.5,\n    tl.cex = 0.6,\n    tl.col = \"black\",\n    col = colorRampPalette(c(colour_min, colour_middle, colour_max))(200),\n    cl.cex = 0.3\n  )\n}\n\nLet’s explain this function.\nSo, the corrplot function is the main driver for this and the second nested cor is just as important, as this is the command to create a correlation matrix. The settings are to use the df_numeric_vals data frame as the data to use with the function, the use = 'all.obs' just tells the function to use all observations in the data frame and the method = method_corrplot uses the if statement I created in step 2 to select the relevant chart from the input. The order uses the angular ordering method and the addCoef.col = 'black' sets the coefficient values to always show black, as well as the colour of the labels. The background colour of the correlation plot uses the colorRampPalette function to create a gradient scale for the function and the parameters of each of the colour settings like to those inputs I explained in step 1.\nThe full function is detailed here:\n\ncreate_gh_style_corrplot &lt;- function(df_numeric_vals,\n                                     method_corrplot,\n                                     colour_min,\n                                     colour_middle,\n                                     colour_max = \"green\") {\n  library(corrplot)\n\n  if (method_corrplot == 1) {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 2) {\n    type_var &lt;- \"number\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 3) {\n    type_var &lt;- \"pie\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 4) {\n    type_var &lt;- \"ellipse\"\n    method_corrplot &lt;- type_var\n  } else if (method_corrplot == 5) {\n    type_var &lt;- \"circle\"\n    method_corrplot &lt;- type_var\n  } else {\n    type_var &lt;- \"shade\"\n    method_corrplot &lt;- type_var\n  }\n\n\n  corrplot(cor(df_numeric_vals, use = \"all.obs\"),\n    method = method_corrplot,\n    order = \"AOE\",\n    addCoef.col = \"black\",\n    number.cex = 0.5,\n    tl.cex = 0.6,\n    tl.col = \"black\",\n    col = colorRampPalette(c(colour_min, colour_middle, colour_max))(200),\n    cl.cex = 0.3\n  )\n}\n\nIf you want to use the function, just copy and paste this code into a R script file and this will create the function for you. Please remember to install the {corrplot} package by using install.packages(corrplot).\nUtilising the function\nThe example dataset I will use here is the mpg sample file provided by {ggplot2}. Load the R script provided towards the end of the last section first, as this will create the function in R’s environment. Next, add this code to the end to look at the various different iterations and charts that can be created from the data:\n\nlibrary(ggplot2)\n##------------------CREATE DATASET---------------------------------------\n\nnumeric_df &lt;- data.frame(mpg[c(3,5,8,9)])\n\n#This relates to the numeric variables in the data frame to use with my function\n##------------------USE FUNCTION-----------------------------------------\n\ncreate_gh_style_corrplot(numeric_df,1, \"steelblue2\",\"white\", \"whitesmoke\")\ncreate_gh_style_corrplot(numeric_df,2, \"steelblue2\",\"black\", \"black\")\ncreate_gh_style_corrplot(numeric_df,3, \"steelblue2\",\"white\", \"whitesmoke\")\ncreate_gh_style_corrplot(numeric_df,4, \"steelblue2\",\"white\", \"whitesmoke\")\ncreate_gh_style_corrplot(numeric_df,5, \"steelblue2\",\"white\", \"whitesmoke\")\n\nThe outputs of the charts are reliant on the correlation plot type select 1-5, and the colour ranges selected. You can choose any colour and I would recommend using the command colours() in R console or script to pull up the list of colours native to R.\nHow about these visualisations:\n\nlibrary(ggplot2)\n##------------------CREATE DATASET---------------------------------------\n\nnumeric_df &lt;- data.frame(mpg[c(3,5,8,9)])\n\n#This relates to the numeric variables in the data frame to use with my function\n##------------------USE FUNCTION-----------------------------------------\n\ncreate_gh_style_corrplot(numeric_df,1, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,2, \"steelblue2\",\"black\", \"black\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,3, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,4, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\ncreate_gh_style_corrplot(numeric_df,5, \"steelblue2\",\"white\", \"whitesmoke\")\n\n\n\n\n\n\n\nEach plot can be tailored to suite your needs. I tend to like blue shades, but go all out and choose whatever colours you like. The R source code is accessible on GitHub.\nI do hope you will use this function to maximise your correlation plots – its all about relationships!\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted on Hutson’s Hacks.\nThis blog has been edited for NHS-R Style, formatted to remove Latin Abbreviations and to ensure running of code in Quarto.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/a-new-kid-on-the-nhs-r-block.html",
    "href": "blog/a-new-kid-on-the-nhs-r-block.html",
    "title": "A new kid on the NHS-R block",
    "section": "",
    "text": "My journey into data science took a baby step a few years ago when I learned SQL during my global health work overseas. A dearth of good data and data sets is every global health worker’s nightmare.\nRealising R in data science is the next best thing since sliced bread, I decided to give it a go. My initial introduction was aided by my daughter who volunteered to teach me during her summer break from UCLA. That was a terrible idea. We didn’t speak to each other for days. (It was worse than learning to drive from your dad!) My self-esteem took a nosedive.\nI searched through various resources such as YouTube, MOOC, and edX, which only confused me further. Then I stumbled across the NHS-R Community website and voila!\nNo more learning about irises, motorcars in the USA, or global flight statuses. The NHS-R Community provided relevant teaching materials presented in the simplest possible way. The gap minder data set is a Godsend for any global health worker. I enthusiastically attended all the R workshops I could. R was becoming an addiction.\nLast week, I attended NHS-R Community conference in Birmingham. I am pleased to say this conference was worthy of the investment of my time, money and energy. All the ‘biggies’ in R were there to facilitate you through your anxieties (although, sadly, no Hadley Wickham!).\nMeeting absolute novices was a great boost for previously dented self-esteem. Helping these novices made me realise how it is actually possible to learn more by teaching and finding ways to explain concepts for others to understand.\nMy biggest achievement is converting my husband (a senior consultant in the NHS) to R. He went from rolling his eyes at my NHS-R-venture in early summer to signing up to NHS-R community this week: a long stride in a short time. I have managed to corrupt his hard drive with R. Let’s see how far he is willing to go!\nI have come across quite a few reluctant R-ists like my husband in the NHS. So now the question is: how do we make a success of converting people at various stages of their careers into using a wonderfully useful open-source resource like R?\nMelanie Franklin, author of ‘Roadmap to Agile Change Management’, puts across the core principle of creating small, incremental changes to be implemented as soon as possible so that organisations can realise benefits sooner rather than later and achieve rapid return on investment. This is precisely what the NHS-R Community can do.\nAn understanding of the behaviours and attitudes is key for change implementation. When a change is sought in an organisation, there is a period of ‘unknown’ amongst the staff. Questions are asked. Do we learn completely new things? Do we unlearn what we had learned before? Will this change make us redundant? This anxiety can be age-related. While younger members are ready to embrace change, senior staff may resist change and become an obstacle. Encouraging people to participate in taking that initial step at every level is imperative, as is encouraging them to express their anxieties. This approach should not just be top-down or bottom-up. Making a mistake is a part of a process and creating a no-blame culture within an organisation is essential. Building an environment that supports exploration and rewarding and celebrating participation in new experiences.\nHealth data science is a melange de competences: not just a group of statisticians and data analysts. A data analyst will no doubt do a great job in analysing your data, but will they ask the right questions in the right context in an environment like the NHS? This space belongs to the health care workforce. As Mohammed rightly pointed out in Birmingham: ‘Facilitate an arranged marriage between two and hope for a happy union’. Lack of time to practice is one of the biggest challenges many of my fellow learners had cited. Unfortunately, there is no simple solution. Just keep trudging along and you will get through this steep learning process.\nEn finalement, I would like to propose a few suggestions to the NHS-R Community (If they are not already being thought about).\n\nStreamlining the training. Many groups up and down the country are providing training in R, but these fantastic training opportunities are few and far. The NHS-R Community is a great body to ensure that the training programs are more frequent and streamlined.\nStandardising the training. Diverse levels of these training programs mean a varied level of R-competence. The NHS-R Community could set up levels of competence (Levels I to III, for instance) and people can advance their level of competence in subsequent workshops, which may motivate people to find time to practice. An online competency exam for each level may be another way to build the capacity of the NHS workforce.\nAccreditation and validation of R skills. This is an eventuality just like any skill in health care.\n\nHappy R-ing!!\nI am hoping to up my game to assist overseas researchers who don’t have means and mentors to learn R. There are many synthetic datasets available for them to learn and practice. I will continue learning as there is never an ending to learn new things.\nNighat Khan is a global ehealth worker based in Edinburgh\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/Community_in_data_science.html",
    "href": "blog/Community_in_data_science.html",
    "title": "The importance of community in NHS data science",
    "section": "",
    "text": "I was alerted to the news of NHS England’s fate by my phone pinging a news app alert to me mid-meeting.\nI try to avoid such distractions, but for obvious reasons this one caught my eye.\nFor a moment I was incredulous. “That’s not what they mean, surely. I mean, they can’t, can they?!”.\nI joined the NHS much more recently than most of my colleagues, but I started by joining an organisation that had barely begun to settle into its new home in NHS Digital before the announcement came that we’d be moving into NHS England.\nNDRS, my previous team, had been part of Public Health England, which was disbanded during the pandemic.\nI loved that job but left late last year to pursue my current role in The Strategy Unit.\nTherefore, the recent news left me feeling a mix of vertigo and deep exhaustion on behalf of my ex-colleagues for having to undergo such big change again, along with relief that I’d left when I did (plus a good dose of guilt for feeling relieved).\nI joined the NHS, as so many of us did, because I need to feel a sense of genuine impact from my work. We’re surrounded by evidence of need: long waiting lists, chronic diseases, burned-out clinical staff, growing populations, desperate inequalities and terrifying headlines. We have what we need to fix these issues. We really do. Thousands of motivated, caring, skilled medical staff, mind-bending amounts of world-class health data, and the modern tools that can squeeze every drop of truth and value from those data. But NHS data and computing tools are useless without energetic and skilful analysts and data scientists.\nMedicine is an evidence-based profession. With something as important as our health and lives, we cannot rely on guesswork, assumptions, or our biases. We look to data to answer the big questions. But we ask these questions because we feel empathy for patients and their families. I’m worried that the analytical workforce we rely upon to do a lot of the hard work of turning data into better health outcomes, are themselves in need of empathy right now.\nOrganisational change is very disruptive, time consuming, and exhausting. Those who are now fearing for their jobs may be feeling undervalued and vulnerable. I can’t comment on the necessity of the government’s choice, as its very far above my pay grade.\nBut I am confident that many colleagues in analytics and data science would benefit from a sense of community and mutual support.\nSince 2018, the NHS-R community has been a friendly, values-led group of NHS (plus public sector and more) employees intent on doing the best thing for patients with the best data, in the best way.\nThe community has supported analytics and data science by providing training, resources and crucially, positive spaces to talk, share problems, swap tips and revel in the magic of coding. It’s all delightfully nerdy. The community has also received a good deal of recognition for its efforts, including this from the 2022 Goldacre review:\n“The NHS-R community – with its welcoming and supportive ethos – provides an excellent entry point for those looking to begin learning the benefits of working with modern, open and collaborative data science tools, and those looking to further develop their skills by accessing the expertise in the community.”\nThrough the community’s Slack group, and the direct links to other community members I have forged through it, I’ve been able to chat, commiserate, vent, argue and de-stress with like-minded colleagues from across the NHS. I hugely value this, and I know many others do too. Feeling supported and included is a human need, and when work can be tough, leaning into these types of communities can offer genuine positivity and relief.\nSo, my advice to anyone in the NHS or wider public sector, who codes or aspires to, for health and care analysis, is to join the community. Make use of it. Post questions in the Slack. Come along to Coffee and Code. Read our blogs. Give yourself the space to use our resources to fall down an interesting rabbit hole or two. Reach out to community members, offer support to others. Be part of NHS-R. The wonderful nerdy content is fantastic, but community is the real point.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/our-first-ever-nhs-r-webinar.html",
    "href": "blog/our-first-ever-nhs-r-webinar.html",
    "title": "Our first ever NHS-R webinar!",
    "section": "",
    "text": "We ran our first ever NHS-R webinar on Wednesday 19th February and it went down a storm! Chris Mainey facilitated a session on Database Connections in R which was attended by a peak audience of 72 individuals.\nThe webinar began with some mentimeter questions to get to know more about who was on the line. After an icebreaker question revealed that the majority of people’s favourite TV series is Breaking Bad, we found out that (of those who answered) approximately 25% were Analysts, 40% were Senior Analysts, 15% were Intelligence/Analytical Leads and 20% were some other role. There was also a broad selection of organisations represented on the line, including approximately 30% from provider trusts, 20% from CSUs, 15% from local authorities and 5% from CCGs. Just over 30% were from other organisations and it would be interesting to delve more deeply into where these individuals are from.\n\nWe also asked about people’s experience of the topic and what they were hoping to get out of the session. When asked what the current level of understanding around database connections in R on the line was, the average score of those who answered was 1.4/5, suggesting that this was a relatively new topic for most individuals. Moreover, regarding what individuals wanted to get out the session, people wanted to: gain a basic understanding of how to make database connections in R, how to make SQL connections, write temp tables and learn tips, tricks and best practice on how database connections can be made.\n\nChris then began, explaining the fundamental elements of SQL (Structured Query Language) before highlighting the two common methods for creating database connections – the RODBC package and DBI system. Both can be used to create a connection object which can be used to manipulate or transfer data into R.\nChris firstly went into more detail about the RODBC package, showing code for creating connections. He then explored DBI in more detail, including: making connections, how SQL can be used with DBI, writing to databases, using tables in the database, constructing dplyr queries and using SQL and returning data into R. He ended his webinar by taking us through an example script in R, which was a great way of putting the learning in context, before giving participants the opportunity to ask more specific questions about database connections in R.\nWe obtained some fantastic feedback about the webinar. The top 3 words that participants used to describe the webinar were “useful”, “interesting” and “clear”. Moreover, the average rank that participants gave for: satisfaction with the webinar, whether they would recommend to others, relevance in helping them to achieve work goals and increasing their understanding of database connections in R was between 4-5 out of 5.\n\nWe also wanted to understand what other webinar topics participants may be interested in, in future. {shiny}, RMarkdown, {ggplot2} and time series analysis were some of the most popular suggestions.\nFinally, we would like to thank Chris Mainey for doing a fantastic webinar and to our participants for tuning in, being engaged and asking some great questions! The material from the webinar is available on GitHuband a recording of the webinar can be found on our NHS-R Community YouTube page.\nMoreover, we are planning to run topical webinars on the third Wednesday of each month between 1-2pm. Our next webinar is taking place on Wednesday 18th March from 1-2pm on “Functional programming with Purrr” led by Tom Jemmett.\nIf you are interested in being part of the NHS-R community, please join our slack channel to keep up to date with the goings on tips and tricks that people have for using R in health and care. Use the #webinars channel to suggest topics for future webinar sessions, or to put your name forward to run your own!\nThis blog has been edited for NHS-R Style.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/phsmethods-an-r-package-for-public-health-scotland.html",
    "href": "blog/phsmethods-an-r-package-for-public-health-scotland.html",
    "title": "{phsmethods}: an R package for Public Health Scotland",
    "section": "",
    "text": "On Wednesday 1st April 2020, Public Health Scotland (PHS) came into being. It was formed as a result of a three-way merger between NHS Health Scotland and the Information Services Division (ISD) and Health Protection Scotland (HPS) sections of Public Health and Intelligence (PHI) (which was in itself a strategic business unit of NHS National Services Scotland (NSS)). It’s fewer acronyms to remember at least.\nThe Transforming Publishing Programme (TPP) was devised in 2017 in an attempt to modernise the way in which ISD produced and presented its statistics. Traditionally, work within ISD had been undertaken using proprietary software such as Excel and SPSS, with output presented predominantly in the form of PDF reports. It required a great deal of manual formatting, caused an even greater deal of frustration for analysts with programming skills, and resulted in more copy and paste errors than anyone would care to admit.\nOver time, ISD gradually (and at times begrudgingly) came to embrace open source software – predominantly R and, to a lesser extent, Python. TPP were at the forefront of much of this: creating style guides; working with producers of official and national statistics to convert PDF reports into Shiny applications; producing R packages and Reproducible Analytical Pipelines; and introducing version control, among other things. Now, with the move to PHS complete, TPP’s purview has broadened: not only to further the adoption of R, but to ensure it’s adopted in a consistent manner by the hundreds of analysts PHS employs."
  },
  {
    "objectID": "blog/phsmethods-an-r-package-for-public-health-scotland.html#background",
    "href": "blog/phsmethods-an-r-package-for-public-health-scotland.html#background",
    "title": "{phsmethods}: an R package for Public Health Scotland",
    "section": "",
    "text": "On Wednesday 1st April 2020, Public Health Scotland (PHS) came into being. It was formed as a result of a three-way merger between NHS Health Scotland and the Information Services Division (ISD) and Health Protection Scotland (HPS) sections of Public Health and Intelligence (PHI) (which was in itself a strategic business unit of NHS National Services Scotland (NSS)). It’s fewer acronyms to remember at least.\nThe Transforming Publishing Programme (TPP) was devised in 2017 in an attempt to modernise the way in which ISD produced and presented its statistics. Traditionally, work within ISD had been undertaken using proprietary software such as Excel and SPSS, with output presented predominantly in the form of PDF reports. It required a great deal of manual formatting, caused an even greater deal of frustration for analysts with programming skills, and resulted in more copy and paste errors than anyone would care to admit.\nOver time, ISD gradually (and at times begrudgingly) came to embrace open source software – predominantly R and, to a lesser extent, Python. TPP were at the forefront of much of this: creating style guides; working with producers of official and national statistics to convert PDF reports into Shiny applications; producing R packages and Reproducible Analytical Pipelines; and introducing version control, among other things. Now, with the move to PHS complete, TPP’s purview has broadened: not only to further the adoption of R, but to ensure it’s adopted in a consistent manner by the hundreds of analysts PHS employs."
  },
  {
    "objectID": "blog/phsmethods-an-r-package-for-public-health-scotland.html#introducing-phsmethods",
    "href": "blog/phsmethods-an-r-package-for-public-health-scotland.html#introducing-phsmethods",
    "title": "{phsmethods}: an R package for Public Health Scotland",
    "section": "Introducing {phsmethods}",
    "text": "Introducing {phsmethods}\nAnalysts working across a multitude of teams within PHS have to perform many of the same, repetitive tasks, not all of which are catered for by existing R packages: assigning dates to financial years in YYYY/YY format (for example 2016/17); formatting improperly recorded postcodes; returning quarters in plain English (e.g January to March 2020 or Jan-Mar 2020) rather than the slightly restrictive formats offered by lubridate::quarter() and zoo::yearqtr(). The list is endless, and every analyst has their own workaround, which quickly becomes a problem. Time is wasted by multiple people devising an alternative way of doing the same thing – and how can anyone be sure that everyone’s method actually does the same thing?\nThe {phsmethods} package was conceived to address (some of) these concerns. At the time of writing, it contains eleven functions to facilitate some of the more laborious tasks analysts regularly face, with more in the works. None deal with any statistical methodology, nor are they likely to provoke any controversy or consternation over their methods of implementation; they are simple functions designed to make routine data manipulation easier.\n{phsmethods} isn’t on CRAN. Perhaps it will be at some point, but there hasn’t been any need to make it available more widely thus far. {phsmethods} does, however, come with many of the features one would expect from a CRAN package: function documentation; unit tests; continuous integration; code coverage; and releases. No hex sticker yet, but maybe one day."
  },
  {
    "objectID": "blog/phsmethods-an-r-package-for-public-health-scotland.html#using-phsmethods",
    "href": "blog/phsmethods-an-r-package-for-public-health-scotland.html#using-phsmethods",
    "title": "{phsmethods}: an R package for Public Health Scotland",
    "section": "Using {phsmethods}",
    "text": "Using {phsmethods}\nPractical examples featuring all of {phsmethods}’ functions are available in the package’s README and no one will make it to the end of this blog post if they’re all regurgitated here. But hopefully one demonstration is okay. Consider the following fictitious dataset containing postcodes:\n\ndf &lt;- tibble::tribble(\n  ~patient_id, ~postcode,\n  1,           \"G26QE\",\n  2,           \"KA8 9NB\",\n  3,           \"PA152TY \",\n  4,           \"G 4 2 9 B A\",\n  5,           \"g207al\",\n  6,           \"Dg98bS\",\n  7,           \"DD37J    y\",\n  8,           \"make\",\n  9,           \"tiny\",\n  10,          \"changes\"\n)\ndf\n\n# A tibble: 10 × 2\n   patient_id postcode     \n        &lt;dbl&gt; &lt;chr&gt;        \n 1          1 \"G26QE\"      \n 2          2 \"KA8 9NB\"    \n 3          3 \"PA152TY \"   \n 4          4 \"G 4 2 9 B A\"\n 5          5 \"g207al\"     \n 6          6 \"Dg98bS\"     \n 7          7 \"DD37J    y\" \n 8          8 \"make\"       \n 9          9 \"tiny\"       \n10         10 \"changes\"    \n\n\nThis is a problem that analysts across the NHS and beyond face regularly: a dataset containing postcodes arrives, but the postcodes are recorded inconsistently. Some have spaces; some don’t. Some are all upper case; some are all lower case; some are a combination of both. And some aren’t real postcodes. Often this dataset is to be joined with a larger postcode directory file to obtain some other piece of information (such as a measure of deprivation). Joining these datasets tends not to be a trivial task; sometimes a {fuzzyjoin} may suffice, but often it requires an arduous process of formatting the postcode variable in one dataset (or both datasets) until they look the same, before combining using a {dplyr} join.\nIn PHS, postcodes typically follow one of two formats: pc7 format (all values have a string length of seven, with zero, one or two spaces before the last three digits as necessary); or pc8 format (all values have one space before the last three digits, regardless of the total number of digits). phsmethods::postcode() is designed to format any valid postcode, regardless of how it was originally recorded, into either of these formats. Consider the earlier dataset:\n\nlibrary(tidyverse)\nlibrary(phsmethods)\n\ndf %&gt;%\n  mutate(postcode = phsmethods::format_postcode(postcode, format = \"pc8\"))\n\n# A tibble: 10 × 2\n   patient_id postcode\n        &lt;dbl&gt; &lt;chr&gt;   \n 1          1 G2 6QE  \n 2          2 KA8 9NB \n 3          3 PA15 2TY\n 4          4 G42 9BA \n 5          5 G20 7AL \n 6          6 DG9 8BS \n 7          7 DD3 7JY \n 8          8 &lt;NA&gt;    \n 9          9 &lt;NA&gt;    \n10         10 &lt;NA&gt;    \n\n\n\ndf %&gt;%\n  # The format is pc7 by default\n  dplyr::mutate(postcode = phsmethods::format_postcode(postcode)) %&gt;%\n  dplyr::pull(postcode) %&gt;%\n  nchar()\n\n [1]  7  7  7  7  7  7  7 NA NA NA\n\n\n\n\n\n\n\n\nUpdated package\n\n\n\n\n\nThe following paragraphs refer to the original function postcode() which has since been deprecated and warning messages suggest using format_postcode().\nIn order to ensure this blog renders the newer function has been used but the corresponding blog text has not been changed.\n\n\n\nIn hindsight, maybe it should have been called something other than postcode() to avoid the whole postcode = postcode(postcode) bit, but everyone makes mistakes when it comes to naming functions.\n\n\nThe NHS-R package {NHSRpostcodetools} does check for whether a postcode exists but is currently only for England.\nphsmethods::postcode() isn’t designed to check whether a postcode actually exists; it just checks whether the input provided is a sequence of letters and numbers in the right order and of the right length to theoretically be one and, if so, formats it appropriately. If not, it returns an NA. Handily, phsmethods::postcode() comes with some warning messages to explain how many values were recorded as NA, why they were recorded as NA, and what happens to lowercase letters:\n\nwarnings &lt;- testthat::capture_warnings(df %&gt;% dplyr::mutate(postcode = phsmethods::postcode(postcode)))\n\nwriteLines(warnings[1])\n\n## 3 non-NA input values do not adhere to the standard UK postcode format (with or without spaces) and will be coded as NA. The standard format is:\n## • 1 or 2 letters, followed by\n## • 1 number, followed by\n## • 1 optional letter or number, followed by\n## • 1 number, followed by\n## • 2 letters\n\n\nwriteLines(warnings[2])\n\n## Lower case letters in any input value(s) adhering to the standard UK postcode format will be converted to upper case"
  },
  {
    "objectID": "blog/phsmethods-an-r-package-for-public-health-scotland.html#next-steps",
    "href": "blog/phsmethods-an-r-package-for-public-health-scotland.html#next-steps",
    "title": "{phsmethods}: an R package for Public Health Scotland",
    "section": "Next steps",
    "text": "Next steps\nThe seven functions which comprised the 0.1.0 release of {phsmethods} were all written by members of TPP. However, the package is not intended to be a vanity exercise for the team. Those initial functions were designed to get the package off the ground, but now contributions are open to be made by anyone in PHS. With that in mind, the 0.2.0 release contains four functions written by PHS analysts not part of TPP.\nUnlike many R packages, {phsmethods} will, at any given time, have two or three maintainers, each with an equitable say in the direction the package takes. When one maintainer moves on or gets fed up, someone else will be sourced from the pool of analysts, and the show will go on. In theory at least.\nThere are moderately extensive guidelines for anyone who wishes to contribute to {phsmethods} to follow. Proposed contributions should be submitted as GitHub issues for the maintainers to consider and discuss. If approved, the contributor makes a branch and submits a pull request for one or more of the maintainers to review. The hope is that, by creating an issue prior to making a contribution, no one will devote time and energy to something which can’t be accepted, and that no duplication of effort will occur in the case of multiple people having the same idea.\nIt’s not expected that everyone who wishes to contribute to {phsmethods} will understand, or have experience of, all of the nuances of package development. Many won’t have used version control; most won’t have written unit tests or used continuous integration before. That’s okay though. The only requirement for someone to contribute code to {phsmethods} is that they should know how to write an R function. The package maintainers are there to help them learn the rest.\nHopefully {phsmethods} will provide a safe incubator for analysts who wish to develop their skills in writing, documenting and testing R functions prior to making their own open source contributions elsewhere. And hopefully the knock-on effect of more analysts developing these skills will be improved coding standards permeating throughout PHS. If not, it’ll at least look good having a package pinned on my GitHub profile.\nThis blog has been formatted to remove Latin Abbreviations and has been edited for NHS-R Style and to ensure running of code in Quarto.\nThe original comments form the WordPress site:\nDavid Henderson\n1 May 2020\nPlease, please, please get this on CRAN. That way it’ll be available in the National safe haven for linked data projects. Some of these functions would be super handy.\nDavid McAllister\n2 May 2020\nThe transforming publishing team have done amazing work at both the “back-end” and front-end of modernising work at ISD. This has great potential to lead to even more in the future for Public Health Scotland in terms of reliability, efficiency, openness and quality.\nPeter Hall\n29 December 2020\nI too would love to see this on the CRAN - can’t underestimate the value of doing this."
  },
  {
    "objectID": "blog/code-snippets-first-last-and-nth-dplyr-functions.html",
    "href": "blog/code-snippets-first-last-and-nth-dplyr-functions.html",
    "title": "Code snippets – first(), last() and nth() {dplyr} functions",
    "section": "",
    "text": "Inspired by conversations on the NHS-R Slack where code answers are lost over time (it’s not a paid account), and also for those times when a detailed comment in code isn’t appropriate but would be really useful, this blog is part of a series of code snippet explanations.\nWhere this code snippet comes from\nThis blog comes from small part of code shared as part of a larger piece of analysis from the Strategy Unit and the British Heart Foundation to visualise socio-economic inequalities in the Coronary Heart Disease (CHD) pathway. The report and analysis was presented at a Midlands Analyst Huddle in January. If you would like to know more about the report and the code I’ll be referring to, it is published on GitHub.\nThe code is in two parts with the first being data formatting and the second part being the statistics for relative index of inequality (RII).\nThanks to Jacqueline Grout, Senior Healthcare Analyst and Tom Jemmett, Senior Data Scientist of the Strategy Unit.\nCreating a column with the first() or the last() data in a group\nIn the analysis example these functions are used to repeat the highest and lowest population count per Indices of Multiple Deprivation (IMD) decile for each GP practice area1.\nThe best way to show what is happening with the two {dplyr} functions first() and last() is to show it with an even simpler dummy data set:\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\n# There are two patients A and B with A having a numbers 1:10 and patient B has numbers 11:20\n\ndata &lt;- tibble(id = rep(c(\"PatientA\", \"PatientB\"), 10)) |&gt;\n  group_by(id) |&gt;\n  mutate(number = ifelse(id == \"PatientA\", 1:10, 11:20)) |&gt;\n  arrange(number) |&gt;\n  mutate(\n    max_number = last(number),\n    min_number = first(number)\n  ) |&gt;\n  ungroup() # persistent grouping is needed in the code but removed at the end for good practice\n\ndata\n\n# A tibble: 20 × 4\n   id       number max_number min_number\n   &lt;chr&gt;     &lt;int&gt;      &lt;int&gt;      &lt;int&gt;\n 1 PatientA      1         10          1\n 2 PatientA      2         10          1\n 3 PatientA      3         10          1\n 4 PatientA      4         10          1\n 5 PatientA      5         10          1\n 6 PatientA      6         10          1\n 7 PatientA      7         10          1\n 8 PatientA      8         10          1\n 9 PatientA      9         10          1\n10 PatientA     10         10          1\n11 PatientB     11         20         11\n12 PatientB     12         20         11\n13 PatientB     13         20         11\n14 PatientB     14         20         11\n15 PatientB     15         20         11\n16 PatientB     16         20         11\n17 PatientB     17         20         11\n18 PatientB     18         20         11\n19 PatientB     19         20         11\n20 PatientB     20         20         11\n\n\nTaking the min and the max numbers for a patient is dependent upon a persistent group_by() and an arrange() and, if you are familiar with SQL, is similar to the Windows partitions MIN and MAX and would be written for this example as MIN(number) OVER(PARTITION BY id ORDER BY number). What SQL can’t do so easily though is to select the nth() number for example:\n\nlibrary(dplyr)\n\n# There are two patients A and B with A having a numbers 1:10 and patient B has numbers 11:20 and letters from\n\ndata &lt;- tibble(id = rep(c(\"PatientA\", \"PatientB\"), 10)) |&gt;\n  mutate(letters = letters[1:20]) |&gt; # occurs before grouping so that the letters don't get restricted to the 10 rows in a group\n  group_by(id) |&gt;\n  mutate(number = ifelse(id == \"PatientA\", 1:10, 11:20)) |&gt;\n  arrange(number) |&gt;\n  mutate(sixth_number = nth(letters, 6))\n\ndata\n\n# A tibble: 20 × 4\n# Groups:   id [2]\n   id       letters number sixth_number\n   &lt;chr&gt;    &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;       \n 1 PatientA a            1 k           \n 2 PatientA c            2 k           \n 3 PatientA e            3 k           \n 4 PatientA g            4 k           \n 5 PatientA i            5 k           \n 6 PatientA k            6 k           \n 7 PatientA m            7 k           \n 8 PatientA o            8 k           \n 9 PatientA q            9 k           \n10 PatientA s           10 k           \n11 PatientB b           11 l           \n12 PatientB d           12 l           \n13 PatientB f           13 l           \n14 PatientB h           14 l           \n15 PatientB j           15 l           \n16 PatientB l           16 l           \n17 PatientB n           17 l           \n18 PatientB p           18 l           \n19 PatientB r           19 l           \n20 PatientB t           20 l           \n\n\nand in this example the 6th character for PatientA is k and for PatientB (row 16) is l.\nUse case\nAnother potential use case for the nth() character selection is where analysis is looking for the last but one appointment date. For example, if a patient had appointments:\n\nappts &lt;- tibble(id = rep(c(\"PatientA\", \"PatientB\"), 3)) |&gt;\n  arrange(id) |&gt; \n  mutate(\n    appointments = rep(c(\"2023-01-01\", \"2023-02-01\", \"2023-03-01\"), 2),\n    team = rep(c(\"teamA\", \"teamB\", \"teamC\"), 2)\n  )\n\nappts\n\n# A tibble: 6 × 3\n  id       appointments team \n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;\n1 PatientA 2023-01-01   teamA\n2 PatientA 2023-02-01   teamB\n3 PatientA 2023-03-01   teamC\n4 PatientB 2023-01-01   teamA\n5 PatientB 2023-02-01   teamB\n6 PatientB 2023-03-01   teamC\n\n\nand we needed to know the details from the second to last appointment to see who they had been seen before teamC:\n\nappts |&gt; \n  filter(appointments == nth(appointments, n() - 1), .by = id)\n\n# A tibble: 2 × 3\n  id       appointments team \n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;\n1 PatientA 2023-02-01   teamB\n2 PatientB 2023-02-01   teamB\n\n\nThe nth() function needs to know which column to look at, appointments, and then which number to select. Instead of hard coding this as 2 as that will only be useful in this very small data set, the code uses n() to count all the rows and then minus 1. The reason why this is an operation applied to both patients is because of the .by = id which is new feature of {dplyr} v1.1.0. Code using the group_by() function will do the same thing but is just an extra two lines as it will also require ungroup() to remove:\n\nappts |&gt; \n  group_by(id) |&gt; \n  filter(appointments == nth(appointments, n() - 1)) |&gt; \n  ungroup()\n\n# A tibble: 2 × 3\n  id       appointments team \n  &lt;chr&gt;    &lt;chr&gt;        &lt;chr&gt;\n1 PatientA 2023-02-01   teamB\n2 PatientB 2023-02-01   teamB\n\n\nGetting involved\nIf you need any help with {dplyr} or would like to share your own use cases feel free to share them in the NHS-R Slack or submit a blog for this series.\nNHS-R Community also have a repository for demos and how tos which people are welcome to contribute code to either through pull requests or issues.\nFootnotes\nI’ve written more about IMD in a blog for the CDU Data Science Team.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/optimising-dplyr.html",
    "href": "blog/optimising-dplyr.html",
    "title": "Optimising dplyr",
    "section": "",
    "text": "suppressPackageStartupMessages({\n  library(tidyverse)\n  library(data.table)\n})\nset.seed(2214)\nRecently in a project I was working on I encountered an issue where one code chunk in my RMarkdown report started to take a significant amount of time to run, and it was using almost 100% of my RAM. The data I was working with was an extract of various tables from HES: each row was relating to some activity that was undertaken.\nThe particular bit of code that was causing me problems was trying to create a simple chart that showed what percentage of people had particular types of activity. As each person may have had more than one of any of the activity types I had to take distinct sets of rows first, then calculate the percentage of individuals having that activity type.\nHowever, there was a slight complication in our data: as this was part of a wider report we would group some of the activity (the Critical Care Bed Day’s) together at one part in the report, but also be able to break these records down into Elective/Emergency.\nThe table below shows what some of this activity data looked like (we just show the type’s of activity and the subgroups below).\nactivity_data &lt;- tribble(\n  ~type,                  ~sub_group,                     ~alt_sub_group,        ~n,\n  \"Urgent Service Event\", \"Emergency Admission\",          NA,                    2.00,\n  \"Urgent Service Event\", \"A&E Attendance\",               NA,                    3.00,\n  \"Urgent Service Event\", \"111 Call\",                     NA,                    2.00,\n  \"Planned Contact\",      \"Outpatient Attendance\",        NA,                    4.00,\n  \"Planned Contact\",      \"Mental Health Contact\",        NA,                    0.12,\n  \"Planned Contact\",      \"IAPT Contact\",                 NA,                    0.10,\n  \"Planned Admission\",    \"Day Case Admission\",           NA,                    0.25,\n  \"Planned Admission\",    \"Regular Attendance Admission\", NA,                    0.07,\n  \"Planned Admission\",    \"Elective Admission\",           NA,                    0.10,\n  \"Bed\",                  \"Critical Care Bed Day\",        \"Elective Admission\",  0.08,\n  \"Bed\",                  \"Critical Care Bed Day\",        \"Emergency Admission\", 0.10\n)\n\n# don't show the n column here, this is just used to generate rows later\nactivity_data %&gt;% select(-n)\n\n# A tibble: 11 × 3\n   type                 sub_group                    alt_sub_group      \n   &lt;chr&gt;                &lt;chr&gt;                        &lt;chr&gt;              \n 1 Urgent Service Event Emergency Admission          &lt;NA&gt;               \n 2 Urgent Service Event A&E Attendance               &lt;NA&gt;               \n 3 Urgent Service Event 111 Call                     &lt;NA&gt;               \n 4 Planned Contact      Outpatient Attendance        &lt;NA&gt;               \n 5 Planned Contact      Mental Health Contact        &lt;NA&gt;               \n 6 Planned Contact      IAPT Contact                 &lt;NA&gt;               \n 7 Planned Admission    Day Case Admission           &lt;NA&gt;               \n 8 Planned Admission    Regular Attendance Admission &lt;NA&gt;               \n 9 Planned Admission    Elective Admission           &lt;NA&gt;               \n10 Bed                  Critical Care Bed Day        Elective Admission \n11 Bed                  Critical Care Bed Day        Emergency Admission\nTo demonstrate the issue with the initial summary code I produced we can generate some synthetic data. The real dataset included far more columns (e.g. the date of the activity, the organisation where the activity happened at), but for this demonstration we just generate a patient id.\nactivity_synthetic &lt;- function(people) {\n  activity_data %&gt;%\n    mutate(pid = map(n, function(.x) {\n      rpois(people, .x) %&gt;%\n        imap(~rep(.y, .x)) %&gt;%\n        flatten_dbl()\n    })) %&gt;%\n    unnest(pid) %&gt;%\n    select(-n)\n}\n\nactivity_100k &lt;- activity_synthetic(100000)\nactivity_10k &lt;- filter(activity_100k, pid &lt;= 10000)\nHere is a sample of this data:\nsample_n(activity_10k, 10)\n\n# A tibble: 10 × 4\n   type                 sub_group             alt_sub_group         pid\n   &lt;chr&gt;                &lt;chr&gt;                 &lt;chr&gt;               &lt;dbl&gt;\n 1 Planned Contact      Outpatient Attendance &lt;NA&gt;                 6253\n 2 Urgent Service Event A&E Attendance        &lt;NA&gt;                 1773\n 3 Planned Contact      Outpatient Attendance &lt;NA&gt;                 3300\n 4 Urgent Service Event 111 Call              &lt;NA&gt;                 9349\n 5 Bed                  Critical Care Bed Day Emergency Admission  5617\n 6 Planned Contact      Outpatient Attendance &lt;NA&gt;                 6971\n 7 Urgent Service Event Emergency Admission   &lt;NA&gt;                 1552\n 8 Urgent Service Event 111 Call              &lt;NA&gt;                 9607\n 9 Urgent Service Event A&E Attendance        &lt;NA&gt;                 8370\n10 Urgent Service Event Emergency Admission   &lt;NA&gt;                 4944\nNow, for rendering in this particular part of the report we wanted to slightly modify the labels of activity used. We wanted to show the alternative subgroup labels along with the subgroup labels for the Critical Care Bed Day records.\nIdeally we would have done this at the data loading stage. However, because these fields were being used at other parts in the report we couldn’t easily change the data at loading.\nThis is the code that I originally came up with, which I’m wrapping in a function for now so we can benchmark our different approaches later. First we update the type and subgroup columns, then we perform the summarisation steps (get the distinct rows for each individual, then count how many individuals there were for that activity).\ndplyr_naive &lt;- function(activity) {\n  activity %&gt;%\n    mutate(across(type, ~ifelse(sub_group == \"Critical Care Bed Day\",\n                                sub_group, .x)),\n           across(sub_group,\n                  ~ifelse(type == \"Critical Care Bed Day\",\n                          paste0(\"Critical Care (\",\n                                 word(alt_sub_group, 1),\n                                 \")\"),\n                          .x))) %&gt;%\n    group_by(type, sub_group) %&gt;%\n    distinct(pid) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    arrange(type, sub_group)\n}\nWe can test how long this takes to run with our 100,000 patient dataset.\nsystem.time( dplyr_naive(activity_100k) )[[\"elapsed\"]]\n\n[1] 18.34\nThis may not seem like a huge amount of time, but I was re-rendering this report upwards of 10 times an hour to ensure the pipeline was still working. Furthermore, we were rendering 12 versions of this report at a time (we had a report for each of the 11 STPs in the Midlands, and an overall Midlands report), and this particular bit of code was chewing up all of my available memory, meaning we couldn’t run in parallel.\nWhat is the problem with the approach above? When we update the type and sub_group columns in the mutate step R has to update all of the rows in the dataframe (in this case, 1,181,633 rows). But, R will not overwrite the original table in memory. Instead it will copy the entire data frame. This is what slows everything down.\nSo the question is, can we reduce the amount of rows we have to update? Yes! We can simply perform the summary steps before updating the labels! This is the approach I settled on: it’s largely the same as above, I just have to include the alt_sub_group column in the group by step, and remove this column from the results at the end.\ndplyr_improved &lt;- function(activity) {\n  activity %&gt;%\n    group_by(type, sub_group, alt_sub_group) %&gt;%\n    distinct(pid) %&gt;%\n    count() %&gt;%\n    ungroup() %&gt;%\n    mutate(across(type, ~ifelse(sub_group == \"Critical Care Bed Day\",\n                                sub_group, .x)),\n           across(sub_group,\n                  ~ifelse(type == \"Critical Care Bed Day\",\n                          paste0(\"Critical Care (\",\n                                 word(alt_sub_group, 1),\n                                 \")\"),\n                          .x))) %&gt;%\n    select(-alt_sub_group) %&gt;%\n    arrange(type, sub_group)\n}\nHow does this perform?\nsystem.time( dplyr_improved(activity_100k) )[[\"elapsed\"]]\n\n[1] 0.19\nSignificantly better!"
  },
  {
    "objectID": "blog/optimising-dplyr.html#data.table",
    "href": "blog/optimising-dplyr.html#data.table",
    "title": "Optimising dplyr",
    "section": "data.table",
    "text": "data.table\nMany will tell you when start working with larger datasets in R you have to resort to using the data.table package. I’m not the most proficient data.table user in the world, so if you can think of a better way of solving this problem please let me know!\nFirst, here is basically the first approach translated to data.table\n\ndata.table_naive &lt;- function(activity) {\n  adt &lt;- as.data.table(activity)\n\n  adt[, type := ifelse(sub_group == \"Critical Care Bed Day\",\n                       \"Critical Care Bed Day\",\n                       type)]\n  adt[, sub_group := ifelse(type == \"Critical Care Bed Day\",\n                            paste0(\"Critical Care (\",\n                            word(alt_sub_group, 1),\n                            \")\"),\n                            sub_group)]\n  unique(adt)[order(type, sub_group), .(n = .N),\n              c(\"type\", \"sub_group\")]\n}\n\nand here is the second approach\n\ndata.table_improved &lt;- function(activity) {\n  adt &lt;- as.data.table(activity) %&gt;%\n    unique() %&gt;%\n    .[, .(n = .N), c(\"type\", \"sub_group\", \"alt_sub_group\")]\n\n  adt[sub_group == \"Critical Care Bed Day\",\n      type := \"Critical Care Bed Day\"]\n  adt[type == \"Critical Care Bed Day\",\n      subgroup := paste0(\"Critical Care (\", word(alt_sub_group, 1), \")\")]\n  adt[, alt_sub_group := NULL]\n\n  adt[order(type, sub_group), ]\n}"
  },
  {
    "objectID": "blog/optimising-dplyr.html#benchmarking",
    "href": "blog/optimising-dplyr.html#benchmarking",
    "title": "Optimising dplyr",
    "section": "benchmarking",
    "text": "benchmarking\nNow, we can benchmark the different functions to see how they perform.\n\nbench::mark(\n  dplyr_naive(activity_10k),\n  data.table_naive(activity_10k),\n  dplyr_improved(activity_10k),\n  data.table_improved(activity_10k),\n  iterations = 10,\n  check = FALSE\n) %&gt;%\n   select(expression, min, median, mem_alloc, n_gc)\n\n# A tibble: 4 × 4\n  expression                             min   median mem_alloc\n  &lt;bch:expr&gt;                        &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 dplyr_naive(activity_10k)         864.18ms    1.15s   44.85MB\n2 data.table_naive(activity_10k)       1.38s    1.64s   46.16MB\n3 dplyr_improved(activity_10k)       36.48ms  46.75ms    9.34MB\n4 data.table_improved(activity_10k)   49.2ms  67.56ms    7.75MB\n\n\nThe table above shows how the improved approach is significantly better than the naive approach. We can see that the median time to run the function is significantly better (note that the naive functions are in seconds, but the improved functions are in milliseconds), but also the amount of memory that is being allocated is much, much lower. The n_gc value tells us how many times the “garbage collector” ran. We want this figure to be low.\nWe can also see how the function works with the 100,000 row dataset.\n\nbench::mark(\n  dplyr_naive(activity_100k),\n  data.table_naive(activity_100k),\n  dplyr_improved(activity_100k),\n  data.table_improved(activity_100k),\n  iterations = 1,\n  check = FALSE\n) %&gt;%\n  select(expression, min, median, mem_alloc, n_gc)\n\n# A tibble: 4 × 4\n  expression                              min   median mem_alloc\n  &lt;bch:expr&gt;                         &lt;bch:tm&gt; &lt;bch:tm&gt; &lt;bch:byt&gt;\n1 dplyr_naive(activity_100k)           18.06s   18.06s   455.3MB\n2 data.table_naive(activity_100k)      16.14s   16.14s   449.6MB\n3 dplyr_improved(activity_100k)      242.15ms 242.15ms    88.6MB\n4 data.table_improved(activity_100k)    1.09s    1.09s    69.6MB\n\n\nAs we can see from both of these results both approaches in dplyr and data.table are broadly the same, with a slight performance edge to using data.table over dplyr. The big difference is to more sensibly arrange the order of operations so that you summarise first, then perform the costly data manipulation steps on a smaller data set."
  },
  {
    "objectID": "blog/the-nhs-r-conference-2019.html",
    "href": "blog/the-nhs-r-conference-2019.html",
    "title": "The NHS-R Conference 2019",
    "section": "",
    "text": "We really enjoyed our second ever NHS-R conference in Birmingham, which was attended by about 300 delegates. We tried to ensure that there was something for colleagues who are new to R as well as for those who are more experienced. The conference was inspiring, exciting, friendly and tiring and we obtained some fantastic feedback from attendees about how much they enjoyed the conference.\n\n\nLet me take this opportunity to thank you for organising a brilliant conference. I learnt more in two days than I had in weeks on my own. Your enthusiasm is extremely catchy and inspirational. Although I am relatively new in R, I am hooked on to it and wonder why I didn’t learn it before. I have already converted one person to R, my husband a consultant Psychiatrist in Edinburgh 🙂 …Once again thank you for your wonderful interactions during two days.\n\nDr Nighat Khan – University of Edinburgh and novice R user\n\nthink that the organisation was excellent, and the quality of the workshops and talks offered was extremely high…. I thoroughly enjoyed catching up with people that I already knew as well as meeting new friends. I have also come away with new skills to hone and new techniques to adopt and share in my work.\n\nCaroline Kovacs – PhD Researcher, University of Portsmouth\n\nI just wanted to say I really enjoyed the event and (it was) probably the best conference I’ve attended.\n\nMatthew Francis – Principal Public Health Intelligence Analyst and advanced R user\n\nI don’t think I’ve got the words to describe how fantastic the event was, fantastic just isn’t enough. I even presented at this conference after only learning R in the last couple of years. Truly, this was only possible because of the NHS-R community.\n\nZoe Turner– Senior Information Analyst at Nottinghamshire Healthcare NHS Foundation Trust\n\nWe had some technical glitches on the day and some of the rooms were not ideal, so we want to thank our delegates for being so forgiving, patient and enthusiastic. Likewise, we want to thank our conference organisers, our helpers, our speakers, our funders – The Health Foundation – our sponsors (Mango Solution, Jumping Rivers, Healthcare Evaluation Data from University Hospitals Birmingham), our partners (AphA, The Strategy Unit, Improvement Academy, University of Bradford, HDRUK) and all the staff at the Edgbaston Cricket Ground for helping make our conference a huge success.\nWe have already started thinking about next year’s conference, and if you have any suggestions on how to make the next one better please let us know via email nhs.rcommunity@nhs.net.\nWe also made some important announcements about NHS-R Community 2.0. This will be covered in a subsequent blog, but for now there is an announcement of free tickets to the EARL (Enterprise Applications of the R Language) conference 2020 which has been described as “The highlight of the R calendar in Europe” by Andre Andrie de Vries of RStudio.\nFollowing a guest invitation for Mohammed A Mohammed to talk about the NHS-R Community at the EARL conference in Sep 2019, Liz Mathews (Head of Community and Education at Mango Solutions) has generously offered members of the NHS-R Community 6 FREE tickets for EARL 2020 to be held on the 8th – 10th September at The Tower Hotel, St Katharine’s Way, London E1W 1LD. These tickets are worth £900 each and includes a day of hands-on-training workshops. So how might you win a free ticket?\nIn 200 words maximum, tells us in an email to nhs.rcommunity@nhs.net “Why we should give you the free ticket and how will you ensure this will help the NHS-R Community?” The subject of the email should state “EARL 2020”. The last two lines of your email should state that: - you confirm that you will arrange your own travel and hotel expenses and that you will get approval from your employer where applicable. - In case you win a ticket but are unable to attend you will let us know ASAP so that we can allocate your ticket to someone else.\nFor recorded talks, photos and highlights from the EARL conference 2019, which provides an excellent overview of what the event entails, please see https://earlconf.com/. You have until 31 Dec 2019 to submit your entries. Winners will be announced in Jan 2020. A small panel will judge the best entries and put then put them in a hat. We will randomly select 6 of the best entries and notify the selected person by email. If you do not claim your ticket within 3 working days of the notification email, we will exclude you from the process and randomly select someone else for the ticket. We will repeat the process until all tickets are allocated. In the event of tickets not being allocated, the NHS-R Team will allocate them as they see fit. The NHS-R Community Team\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/join-the-fun-join-nhs-r.html",
    "href": "blog/join-the-fun-join-nhs-r.html",
    "title": "Join the fun! Join NHS-R",
    "section": "",
    "text": "The NHS-R Community is a friendly, welcoming community – open to all – and always looking for newbies (new to us, or R) or veterans to join the fun.\nWe have the same values as the NHS and the open source community. There are several ways to get involved and it’s bound to look cool on your CV and profile.\nExperience the joy of helping others to learn – the NHS-R Community offers free training courses – an introduction to R, automation using RMarkdown, shiny dashboards, time series forecasting. You can offer to lead a course, be an assistant on a course or design and deliver a new course. We will support you with finding a mentor, organising courses and administration, on boarding with RStudio Cloud and certification.\nJoin us on slack – we have a vibrant slack community, with ~1300+ members. There is lots of activity but do start with introducing yourself and then swim in the channels that take your fancy. A favourite is the “help-with-r” channel which shows just how amazing the community is at helping each other. You can also get involved with several on-going projects – for example conference2022, travel-times project, plotthedots and so on. So come and join us on slack:\nWrite a blog post – this can seem intimidating at first but some of our super stars in the community started with writing short blogs. We can offer editorial support and constructive peer review comments. Blog posts can be about anything related to R or open source or issues/problems that need to be addressed. Blogs can also be personal – about you and your journey/reflections. Bloggers help to disseminate knowledge, shape conversations and provoke discussion. Give it a try.\nPodcasts – are a friendly, fireside chat where you can suggest a topic for a podcast, agree to host a podcast or be a guest. The Podcast highlights the great work being done by analysts all over the country using open source tools and discuss the work of many of the super analysts and teams who comprise NHS-R as well as special guests who are notable for their work with open source tools. Listen to the latest episodes here: https://soundcloud.com/nhs-r-community\nWebinars – are a great way to share content with the community. They are monthly and usually 1 hour long . Once again you can suggest a topic for a webinar, agree to host a webinar or be a presenter. Our webinars are recorded and can be found on YouTube.\nHexitime – is the first and currently only national timebank in health and social care, and has won multiple awards for innovating in this space. It started as a Health Foundation Q Exchange winning project in 2018 and has steadily begun to be adopted by like-minded people across healthcare.\nIt works on the principles that everybody’s time is equal and potentially highly valuable to others who need it to progress their improvement work. As such, it utilises a virtual currency where an hour of your time = 1 credit, and you can share your skills across any borders. And because Hexitime is designed, run and owned by NHS staff, the platform is committed to operating as a not-for-profit, remaining free at the point of use. Learn more at this blog.\nSuggest Problems/Solutions – we have common data science related problems and finding a common solution would be so helpful. For example, the NHS-R Community developed a package (plotthedots) which makes plotting basic SPC charts easy. If you can think of a common data science headache then let us know (ideally via slack) and we can see if a solution is forthcoming. Previous solutions can be found here. https://nhsrcommunity.com/nhs-r-solutions/\nApply to the Academy – we continue to seek ways to promote and reward all the wonderful, kind, and talented people using R across health and social care. You can apply for an honorary title from the NHS-R Academy – for example champion, fellow, friend and so on. We can also help you prepare your application forms.\nNHS-R conference – is the largest data science conference in the NHS and care system. You can submit abstracts, offer a workshop, host a session, join the conference organising committee and make suggestions on how to improve the conference.\nThe next conference is set for 16-17 November 2022 and will be a hybrid event based in Birmingham, UK. Workshops will be online during the first two weeks of November 2022. See previous conference details at the NHS-R Events page..\nSuggestions – we are always open to new ideas on how to add value to the NHS-R Community and its members. For example, book club, podcasts, hexitime were all ideas that came from the community.\nSo, do get in touch and join the fun!\n\nemail: nhs.rcommunity@nhs.net\nSlack: nhsrcommunity.slack.com\n\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/how-to-extrapolate-data-from-data.html",
    "href": "blog/how-to-extrapolate-data-from-data.html",
    "title": "How to extrapolate data from data",
    "section": "",
    "text": "There are many occasions when a column of data needs to be created from an already existing column for ease of data manipulation. For example, perhaps you have a body of text as a pathology report and you want to extract all the reports where the diagnosis is ‘dysplasia’.\nYou could just subset the data using grepl so that you only get the reports that mention this word…but what if the data needs to be cleaned prior to subsetting like excluding reports where the diagnosis is normal but the phrase ‘No evidence of dysplasia’ is present. Or perhaps there are other manipulations needed prior to subsetting.\nThis is where data accordionisation is useful. This simply means the creation of data from (usually) a column into another column in the same dataframe.\nThe neatest way to do this is with the mutate function from the {dplyr} package which is devoted to data cleaning. There are also other ways which I will demonstrate at the end.\nThe input data here will be an endoscopy data set:\n\nAge &lt;- sample(1:100, 130, replace = TRUE)\nDx &lt;- sample(c(\"NDBE\", \"LGD\", \"HGD\", \"IMC\"), 130, replace = TRUE)\nTimeOfEndoscopy &lt;- sample(1:60, 130, replace = TRUE)\n\nlibrary(dplyr)\n\nEMRdf &lt;- data.frame(Age, Dx, TimeOfEndoscopy, stringsAsFactors = F)\n\nPerhaps you need to calculate the number of hours spent doing each endoscopy rather than the number of minutes\n\nEMRdftbb &lt;- EMRdf %&gt;% mutate(TimeOfEndoscopy / 60)\n\n# install.packages(\"knitr\")\nlibrary(knitr)\nlibrary(kableExtra)\n\n# Just show the top 20 results\n\nkable(head(EMRdftbb, 20))\n\n\n\nAge\nDx\nTimeOfEndoscopy\nTimeOfEndoscopy/60\n\n\n\n68\nNDBE\n53\n0.8833333\n\n\n62\nNDBE\n58\n0.9666667\n\n\n54\nLGD\n35\n0.5833333\n\n\n33\nHGD\n53\n0.8833333\n\n\n54\nHGD\n8\n0.1333333\n\n\n10\nHGD\n57\n0.9500000\n\n\n82\nNDBE\n47\n0.7833333\n\n\n88\nIMC\n30\n0.5000000\n\n\n12\nHGD\n8\n0.1333333\n\n\n68\nLGD\n2\n0.0333333\n\n\n29\nIMC\n43\n0.7166667\n\n\n82\nLGD\n7\n0.1166667\n\n\n72\nIMC\n23\n0.3833333\n\n\n83\nLGD\n58\n0.9666667\n\n\n43\nLGD\n60\n1.0000000\n\n\n36\nLGD\n13\n0.2166667\n\n\n78\nLGD\n5\n0.0833333\n\n\n43\nHGD\n46\n0.7666667\n\n\n53\nIMC\n2\n0.0333333\n\n\n6\nLGD\n35\n0.5833333\n\n\n\n\n\nThat is useful but what if you want to classify the amount of time spent doing each endoscopy as follows: &lt;0.4 hours is too little time and &gt;0.4 hours is too long.\nUsing ifelse() with mutate for conditional accordionisation.\nFor this we would use ifelse(). However this can be combined with mutate() so that the result gets put in another column as follows\n\nEMRdf2 &lt;- EMRdf %&gt;%\n  mutate(TimeInHours = TimeOfEndoscopy / 60) %&gt;%\n  mutate(TimeClassification = ifelse(TimeInHours &gt; 0.4, \"Too Long\", \"Too Short\"))\n\n# Just show the top 20 results\n\nkable(head(EMRdf2, 20))\n\n\n\nAge\nDx\nTimeOfEndoscopy\nTimeInHours\nTimeClassification\n\n\n\n68\nNDBE\n53\n0.8833333\nToo Long\n\n\n62\nNDBE\n58\n0.9666667\nToo Long\n\n\n54\nLGD\n35\n0.5833333\nToo Long\n\n\n33\nHGD\n53\n0.8833333\nToo Long\n\n\n54\nHGD\n8\n0.1333333\nToo Short\n\n\n10\nHGD\n57\n0.9500000\nToo Long\n\n\n82\nNDBE\n47\n0.7833333\nToo Long\n\n\n88\nIMC\n30\n0.5000000\nToo Long\n\n\n12\nHGD\n8\n0.1333333\nToo Short\n\n\n68\nLGD\n2\n0.0333333\nToo Short\n\n\n29\nIMC\n43\n0.7166667\nToo Long\n\n\n82\nLGD\n7\n0.1166667\nToo Short\n\n\n72\nIMC\n23\n0.3833333\nToo Short\n\n\n83\nLGD\n58\n0.9666667\nToo Long\n\n\n43\nLGD\n60\n1.0000000\nToo Long\n\n\n36\nLGD\n13\n0.2166667\nToo Short\n\n\n78\nLGD\n5\n0.0833333\nToo Short\n\n\n43\nHGD\n46\n0.7666667\nToo Long\n\n\n53\nIMC\n2\n0.0333333\nToo Short\n\n\n6\nLGD\n35\n0.5833333\nToo Long\n\n\n\n\n\nNote how we can chain the mutate() function together.\nUsing multiple ifelse()\nWhat if we want to get more complex and put several classifiers in? We just use more ifelse’s:\n\nEMRdf2 &lt;- EMRdf %&gt;%\n  mutate(TimeInHours = TimeOfEndoscopy / 60) %&gt;%\n  mutate(TimeClassification = ifelse(TimeInHours &gt; 0.8, \"Too Long\", ifelse(TimeInHours &lt; 0.5, \"Too Short\", ifelse(TimeInHours &gt;= 0.5 & TimeInHours &lt;= 0.8, \"Just Right\", \"N\"))))\n\n# Just show the top 20 results\n\nkable(head(EMRdf2, 20))\n\n\n\nAge\nDx\nTimeOfEndoscopy\nTimeInHours\nTimeClassification\n\n\n\n68\nNDBE\n53\n0.8833333\nToo Long\n\n\n62\nNDBE\n58\n0.9666667\nToo Long\n\n\n54\nLGD\n35\n0.5833333\nJust Right\n\n\n33\nHGD\n53\n0.8833333\nToo Long\n\n\n54\nHGD\n8\n0.1333333\nToo Short\n\n\n10\nHGD\n57\n0.9500000\nToo Long\n\n\n82\nNDBE\n47\n0.7833333\nJust Right\n\n\n88\nIMC\n30\n0.5000000\nJust Right\n\n\n12\nHGD\n8\n0.1333333\nToo Short\n\n\n68\nLGD\n2\n0.0333333\nToo Short\n\n\n29\nIMC\n43\n0.7166667\nJust Right\n\n\n82\nLGD\n7\n0.1166667\nToo Short\n\n\n72\nIMC\n23\n0.3833333\nToo Short\n\n\n83\nLGD\n58\n0.9666667\nToo Long\n\n\n43\nLGD\n60\n1.0000000\nToo Long\n\n\n36\nLGD\n13\n0.2166667\nToo Short\n\n\n78\nLGD\n5\n0.0833333\nToo Short\n\n\n43\nHGD\n46\n0.7666667\nJust Right\n\n\n53\nIMC\n2\n0.0333333\nToo Short\n\n\n6\nLGD\n35\n0.5833333\nJust Right\n\n\n\n\n\nUsing multiple ifelse() with grepl() or string_extract\nOf course we need to extract information from text as well as numeric data. We can do this using grepl() or string_extract() from the library(stringr).\nLet’s say we want to extract all the samples that had IMC. We don’t want to subset the data, just extract IMC into a column that says IMC and the rest say ‘Non-IMC’\nUsing the dataset above:\n\nlibrary(stringr)\n\nEMRdf$MyIMC_Column &lt;- str_extract(EMRdf$Dx, \"IMC\")\n\n# to fill the NAs we would do: EMRdf$MyIMC_Column &lt;- ifelse(grepl(\"IMC\",EMRdf$Dx),\"IMC\",\"NoIMC\")\n\n# Another way to do this (really should be for more complex examples when you want to extract the entire contents of the cell that has the match)\n\nEMRdf$MyIMC_Column &lt;- ifelse(grepl(\"IMC\", EMRdf$Dx), str_extract(EMRdf$Dx, \"IMC\"), \"NoIMC\")\n\nSo data can be usefully created from data for further analysis.\nHopefully this way of extrapolating data and especially using conditional expressions to categorise data according to some rules is a helpful way to get more out of your data.\nPlease follow @gastroDS on twitter\nThis article originally appeared on https://sebastiz.github.io/gastrodatascience/ and has been edited to render in Quarto and had NHS-R styles applied.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/nhs-r-community-conference-my-experience-of-the-day.html",
    "href": "blog/nhs-r-community-conference-my-experience-of-the-day.html",
    "title": "NHS-R Community Conference: My experience of the day",
    "section": "",
    "text": "I went to the NHS-R Community Conference in Birmingham on Tuesday.\nIt was great.\nHere are three observations about it.\nFirst, the old versus the new. Quite a few of the speakers alluded to the idea that R is sometimes seen in the NHS as this ‘new’ thing that is here to ‘replace’ the ‘old’ tools of Excel, SQL, SPSS and so on. It’s an interesting dichotomy to ponder upon, (a) because it is of course infinitely more complex than that, and (b) because there are so many ways that it’s possible to cast R as the new ‘good guy’ and Excel and so on. as the old ‘bad guy’. Having expressed caveats though, it was interesting to hear throughout the day how often people tended to explain what R was doing by reference to how Excel would do (or – more pertinently – fail to do) the same thing, only it would be more clunky in Excel.\nIn fact, in the workshop on patient flow that I co-presented with John MacKintosh, we subconsciously had cast ourselves in these roles. I was the old bad guy who was over-reliant on Excel; John was the younger good guy who shows how you can do it better – and you can do more with it – in R. The visualizations we were showcasing were ones that I’d originally done in Excel and that John had improved considerably by using R.\nSecond, and this next point follows on from the Excel versus R idea, I am intrigued by how ‘light on its feet’ R is. Can R respond to suggestions and edits from managers and clinicians ‘on the fly’? One reservation I’ve had about R as a tool for using at the clinical/managerial interface is that it looks too ‘data-y’, and therefore too forbidding, too exclusive and as a result it frightens the horses. Whereas one of Excel’s virtues is that it’s at least familiar to pretty much everyone, and therefore a bit less daunting as an interface, and you can make use of that familiarity by showing your workings in a way that has a chance of being understood.\nBut in general I think I am persuaded by the swiftness and elegance of R as a data analysis tool. It might indeed look more forbidding than Excel but we can probably edit and re-draft our work ten times more quickly than we could in Excel, so in terms of rapid iterations (including iterations while we’re actually in the meeting), R wins. Again, I’ll quote an example from my workshop: John attempted some live editing of the code while we were presenting, and yes, it worked, so – yes – it was reassuring to know it can be done, even in in the middle of a presentation to an audience of 24 subject-matter experts.\nThird, and apologies of this observation seems a bit self-congratulatory, but it needs to be said. The mood of the conference was good. It felt congenial. There was a general ‘niceness’ vibe throughout the day. People were respectful, people were inclusive, it was easy to network.\nI remember thinking on the train as I made my way to the event that I might suffer from imposter syndrome when I got there. I have had very little exposure to R. I’ve made a start on the tutorials in DataCamp but I really haven’t got very far. And I am utterly indebted to my collaborator John MacKintosh when it comes to having my awareness raised as to the possibilities and potential of R. So I was a bit anxious that I might be sniffed at by the other delegates as someone who wasn’t a bona fide (genuine) R geek, given that so many of the delegates had technical skills that were in a different class altogether.\nBut I needn’t have worried. It turns out that’s not how the NHS-R community works. It’s inclusive, not exclusive. It’s a multi-disciplinary forum, not a talking shop for geeks. Which means that when the final plenary session for the day was trying to identify the main themes to emerge from the conference, it was collaboration that emerged for me as the key word. We do need to find ways of collaborating better. Collaboration that cuts across disciplines, and across organisations.\nBut on the evidence of the mood and feel of Tuesday’s conference, collaboration should be easy, because this is a community of people who want to help one another.\nThis blog has been formatted to remove Latin Abbreviations.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/how-r-changed-me-as-an-analyst.html",
    "href": "blog/how-r-changed-me-as-an-analyst.html",
    "title": "How R changed me as an analyst",
    "section": "",
    "text": "I suspect there are many blogs about R and why it’s so great:\n\nIt’s free!\nIt’s open source!\nThere’s a great community!\nIt’s reproducible!\n\nYou can certainly read on social media (#rstats) about what R can do for you but what about what R does to you, particularly as an analyst in the NHS or social care?\nThe learning bit…\nBack in April 2018 when NHS-R Community was running its first introduction to R course in Leeds, my only knowledge of it had come from a free online base R course with edX that I hadn’t really understood and never finished. Online learning for me is like being back at school. I get the right answer, feel good, move on and promptly forget it all. After the NHS-R course I dabbled with the dplyr package, tried to run lots of other people’s scripts and generally failed a lot. It was a frustrating time of slow delivery of tasks and bafflement. When things did work, I had no idea why and I often wondered what all the fuss was about particularly as I could do the same things in familiar programmes.\nHindsight is a wonderful thing and I can now see my frustrations weren’t just one lump of confusion, but could be split down into how I used these ‘familiar programmes’, namely:\n\nSQL for data engineering and\nExcel for visualisations.\n\nAlthough I still used (and use) SQL to get my data, I was copying it to Excel and then loading it into R; once loaded I’d then realise I needed to group by and count or remove something I didn’t need and it seemed too long-winded going back to SQL, copying to Excel and then loading it.\nThe second frustration of visualising in R came with the inability to replicate the look of the Excel charts in R: getting the same colours, the same font size headers and so on. I’ve yet to resolve that completely but it was here that I realised the worth of R wasn’t in making it look like Excel, but rather that it could do so much more than Excel. I needed to start thinking about what I should be visualising and how to do it.\nSharing methodology\nOver my time in the NHS I have learned to be cautious – protective even – of data. But that has led to the misguided fear of sharing technical knowledge which was never a conscious thing, just that’s how it is. However, R has a reputation of sharing which has resulted in an expectation of sharing. And that isn’t just within your own team or organisation, it’s so much wider – it can even be the world.\nAs an example of why it’s harder to share Excel methodology, I’ve previously built a benchmarking Excel spreadsheet using MATCH and INDEX so that the bar charts automatically coloured the organisation I worked for and ordered the bars in graphs from the greatest to the smallest. It was one of those tasks that took a lot of effort to automate, looked simple when done but was heavily dependent on the data being in just the right cell or it would break.\nJust updating it with a new year’s data would take great care so the thought of writing out the methodology to share never occurred to me. Writing it out would involve describing the positions of data, what the formulae did and how bits all linked. That’s a laborious task which is not necessary if you don’t plan to share – and as there was no need to share, I didn’t.\nIt’s all about the data\nThe entire process, from SQL to Excel, is about the data, for example how it joins and what it’s counting. To get the data ‘just so’, it often requires so many clever solutions to so many problems that, as I now realise, it consumes so much thinking time that there’s often little energy left for considering why I am doing this – is it the best thing to do and how can I get more meaning from the data?\nIf I’d picked up someone else’s script or Excel document on ordering benchmarking data, perhaps the time I spend would be on improving it instead of building it. In a perfect world, I would feed back on what I’d done and share any improvements or corrections.\nBut what can R do that SQL and Excel can’t?\nAs a very simple example, consider creating a median average.\nIn Excel it’s reasonably easy to use the formula MEDIAN() but to make it dynamic (such that it automatically updates if you add more data), the formula becomes much more complicated. Here is a page explaining how it’s done:\nhttps://www.extendoffice.com/documents/excel/2611-excel-average-dynamic-range.html\nThere are lots of graphics are used to describe how to do it and you’ll note this is for AVERAGE which is mean rather than median.\nIn SQL, creating the MEDIAN can be solved various ways:\nhttps://stackoverflow.com/questions/1342898/function-to-calculate-median-in-sql-server\nThere are 204 examples to go through to solve this! I didn’t go through it as that’s too much needless thinking required when R can do this…\n\nmedian(table$column)\n\nSomething this simple removes all that otherwise inevitable necessary thinking to figure out the best way to get the median… and then having to check that it is right. Although that may be easy enough in Excel, I know I make mistakes and will have to repeat the exercise more times than I care to admit, and doing so in Excel will involve so many steps that each require checking. All of this uses up that precious resource of focused thinking. With R doing the job so quickly and reliably I now have time to consider if median is actually the right thing to use, particularly if it’s a case of “we’ve always done that”. Then I can ponder on what is it telling me; is it increasing/decreasing and why is that? Is there any other data that could help explain this change?\nLike any great piece of analysis or coding, time invested at the beginning pays off. With the benchmarking example of reordered bar charts, spending a couple of days getting it right made something interactive that was engaging for the users. But Excel continues to require thinking and time checking whereas R doesn’t; once it’s coded, that’s it. And that’s where the analyst should be, that space after the work is done. That’s where you can think about the very analysis itself; was it enough? What does it tell you? What else can you look at? Although I’ve had analyst in my job title for a long time, this is what analysis is all about and it’s something, I now realise, that I’ve not been doing because my “thinking time” has been used up elsewhere.\nThis blog was written Zoë Turner, Senior Information Analyst at Nottinghamshire Healthcare NHS Trust.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/guest-blogger-chatgpt.html",
    "href": "blog/guest-blogger-chatgpt.html",
    "title": "Guest blogger – ChatGPT",
    "section": "",
    "text": "Today we have a guest blogger for the NHS-R community blog- ChatGPT. Let’s see what ChatGPT thinks about the NHS-R Community.\nPrompt: Please write a short essay about NHS-R\n“The National Health Service in the United Kingdom (NHS) is one of the largest publicly funded healthcare systems in the world. In recent years, a growing community of healthcare professionals, researchers, and data scientists have started to use the R programming language to analyze and visualize data within the NHS. This movement has come to be known as NHS-R.\nNHS-R aims to promote the use of R within the NHS, to enhance the quality of data analysis and decision making, and to foster collaboration and knowledge sharing among members of the NHS-R community. The community provides support and training to individuals within the NHS to help them develop their skills in using R, and hosts events such as workshops, meetups, and online webinars to facilitate collaboration and learning.\nOne of the key benefits of using R within the NHS is its ability to handle and process large and complex data sets. R has a rich library of packages and functions that allow for the manipulation, analysis, and visualization of data, making it an ideal tool for exploring complex health data. Furthermore, the open-source nature of R means that it is readily available and accessible to all members of the NHS-R community, promoting collaboration and knowledge sharing.\nIn conclusion, the NHS-R community is making a significant contribution to the NHS by promoting the use of R within the healthcare system. By providing support and training, and fostering collaboration and knowledge sharing, NHS-R is helping to enhance the quality of data analysis and decision making within the NHS, and ultimately improving patient care.”\nThanks, ChatGPT, I could hardly have put it better myself \n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html",
    "href": "blog/using-rmd-for-academic-writing.html",
    "title": "Using RMD for academic writing",
    "section": "",
    "text": "This is a blog post that describes my experience of using RMD for academic writing. By choosing the right YAML and {knitr} package settings, and creating appropriate .docx, .bib, and .csl files (which need to be located in the main project directory), one can use the {citr} package to easily add citations to an RMD file, which are correctly rendered in the main body of the text as well as the references section of the document."
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#yaml-setup-in-rmd",
    "href": "blog/using-rmd-for-academic-writing.html#yaml-setup-in-rmd",
    "title": "Using RMD for academic writing",
    "section": "YAML setup in RMD",
    "text": "YAML setup in RMD\nRMD files start with Yet Another Markup Language (YAML) code. This is how I’ve set up my YAML in order to render an academic report in RMD.\ntitle: \"Title of manuscript\"\nauthor: FALSE \ndate: \"2025-06-23\"\noutput: \n  word_document: \n    reference_docx: word-styles-reference-01.docx #A Word document that has been appropriately edited. Importantly, you must use the \"Styles\" function of MS Word to control text formatting. The Word file needs to be located in the main project folder.\nbibliography: biblio.bib #This is the .bib file, created using Zotero. Key to successfully using Zotero is installing add-ons, such as the Better BibTex for Zotero add-on, which can be found at: https://retorque.re/zotero-better-bibtex/. The .bib file needs to be located in the main project folder.\ncsl: elsevier-harvard.csl #this is the .csl file used to style references in line with journal guidelines. further styles can be found at: https://www.zotero.org/styles, the Zotero Style Repository. The .csl file needs to be located in the main project folder.\nlink-citations: yes"
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#requirements",
    "href": "blog/using-rmd-for-academic-writing.html#requirements",
    "title": "Using RMD for academic writing",
    "section": "Requirements",
    "text": "Requirements\nAfter completing YAML setup, the next part of my RMD file outlines which packages need to be loaded. Other packages may be needed to complete your analysis and writeup, but these are the bare minimum to render an academic report in RMD.\n\nlibrary(here) #For correctly loading files in RMD\nlibrary(knitr) #General-purpose tool for dynamic report generation in R\nlibrary(citr) #RStudio Addin to Insert Markdown Citations"
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#knitr-package-setup",
    "href": "blog/using-rmd-for-academic-writing.html#knitr-package-setup",
    "title": "Using RMD for academic writing",
    "section": "{knitr} package setup",
    "text": "{knitr} package setup\nAfter completing YAML setup, the next part of my RMD file outlines the {knitr} options.\nknitr::opts_chunk$set(echo = FALSE, #Whether to display the source code in the output document. \n                      warning = FALSE, #If FALSE, all warnings will be printed in the console instead of the output document.\n                      error = FALSE, #By default, the code evaluation will not stop even in case of errors! If we want to stop on errors, we need to set this option to FALSE.\n                      message = FALSE, #Whether to preserve messages emitted by message() (similar to the option warning)\n                      strip.white = TRUE,  #Whether to remove blank lines in the beginning or end of a source code block in the output\n                      tidy = FALSE, #Whether to reformat the R code.\n                      dev='jpeg', #The graphical device to generate plot files. \n                      dpi = 300, #The DPI (dots per inch) for bitmap devices (default = 72)\n                      fig.path = 'figures/',\n                      fig.width = 6, #default is 7\n                      fig.asp = 0.618, #the golden ratio\n                      # fig.height = 6, #default is 7\n                      fig.align = 'center', #Possible values are default, left, right, and center. \n                      out.width = \"70%\", \n                      cache.path = 'cache/',\n                      cache = TRUE) #Whether to cache a code chunk. When evaluating code chunks for the second time, the cached chunks are skipped (unless they have been modified)."
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#the-here-package",
    "href": "blog/using-rmd-for-academic-writing.html#the-here-package",
    "title": "Using RMD for academic writing",
    "section": "The {here} package",
    "text": "The {here} package\nTo ensure that the RMD document renders correctly, I’ve found the {here} package to be an absolute life saver. As per the package’s R documentation, it uses a reasonable heuristics to find your project’s files, based on the current working directory at the time when the package is loaded."
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#using-the-citr-package",
    "href": "blog/using-rmd-for-academic-writing.html#using-the-citr-package",
    "title": "Using RMD for academic writing",
    "section": "Using the {citr} package",
    "text": "Using the {citr} package\nZotero can be used to put references in a plain text file with the extension .bib, in BibTex format. For example, we may wish to cite the following journal article: Anhøj, J., Olesen, A.V., 2014. Run charts revisited: A simulation study of run chart rules for detection of non-random variation in health care processes. PLoS One 9, e113825\nIn your text, citations go inside square brackets, with each journal article referenced using a single string of characters defined using Zotero. If you wish to cite multiple sources, separate these by semicolons. For an easy way to insert citations, try the citr RStudio add-in."
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#example-reference",
    "href": "blog/using-rmd-for-academic-writing.html#example-reference",
    "title": "Using RMD for academic writing",
    "section": "Example Reference",
    "text": "Example Reference\nFor example…\nBlah blah [@anhoj2014run].\n@anhoj2014run says blah.\nBlah blah [see @anhoj2014run].\n…turns into:\nBlah blah (Anhøj and Olesen, 2014).\nAnhøj and Olesen (2014) says blah.\nBlah blah (see Anhøj and Olesen, 2014)."
  },
  {
    "objectID": "blog/using-rmd-for-academic-writing.html#references",
    "href": "blog/using-rmd-for-academic-writing.html#references",
    "title": "Using RMD for academic writing",
    "section": "References",
    "text": "References\nReferences can be added to the end of a rendered document by adding the following code snippet:\n::: {#refs} :::\nAnhøj, J., Olesen, A.V., 2014. Run charts revisited: A simulation study of run chart rules for detection of non-random variation in health care processes. PLoS One 9, e113825."
  },
  {
    "objectID": "blog/apha-june-blog.html",
    "href": "blog/apha-june-blog.html",
    "title": "AphA June Blog",
    "section": "",
    "text": "First published for the AphA June 2023 newsletter:"
  },
  {
    "objectID": "blog/apha-june-blog.html#haca23-11-12-july",
    "href": "blog/apha-june-blog.html#haca23-11-12-july",
    "title": "AphA June Blog",
    "section": "HACA23 11-12 July",
    "text": "HACA23 11-12 July\nJust a few weeks left until the HACA23! Tickets are free and you can still sign up for virtual tickets. Follow the Twitter account HACA_Conf for more information."
  },
  {
    "objectID": "blog/apha-june-blog.html#nhsr-community-conference-17-18-october",
    "href": "blog/apha-june-blog.html#nhsr-community-conference-17-18-october",
    "title": "AphA June Blog",
    "section": "NHSR Community Conference 17-18 October",
    "text": "NHSR Community Conference 17-18 October\nAbstracts for the NHS-R Conference was extended to 23rd June and, by the time this is published, will have passed. Now it’s time to watch this space for information on how to get tickets and notices on who will be speaking at both the virtual and live days!"
  },
  {
    "objectID": "blog/apha-june-blog.html#new-open-analytics-resources-book",
    "href": "blog/apha-june-blog.html#new-open-analytics-resources-book",
    "title": "AphA June Blog",
    "section": "New Open Analytics Resources book!",
    "text": "New Open Analytics Resources book!\nThis book is new and not new at the same time! AphA had originally commissioned a Google document released under a CC-BY-SA licence in 2020. Because this is an open licence I’d copied this originally to my own GitHub to tidy up some of the dead links and to add more. As with many personal projects, I’d left this for quite a long time as I worked on other things and then, more recently, decided it would be useful to move this book to the new Quarto format and update the links. Lots of those links had disappeared, particularly relating to Covid, and proves just how books/documents like this are living documents. Following a talk I gave to the Government Data Science Community at their Toolshed meetup I took my own suggestion and passed the book over to NHS-R Community which is where you can now find it! https://nhs-r-community.github.io/open-analytics-resources/. Comments, corrections and thoughts on the book can be made through issues and pull requests directly to the repository."
  },
  {
    "objectID": "blog/apha-june-blog.html#contributing-to-github-requires-knowledge-of-git",
    "href": "blog/apha-june-blog.html#contributing-to-github-requires-knowledge-of-git",
    "title": "AphA June Blog",
    "section": "Contributing to GitHub requires knowledge of Git",
    "text": "Contributing to GitHub requires knowledge of Git\nAlthough the NHS-R Community work is on GitHub there are a couple of issues people can have with that, firstly contributing can be difficult if you don’t know how to use GitHub/Git. As part of some training required for the Midland Decision Support Network I’ve been writing training slides for an Introduction to Git/GitHub which differs substantially from many training materials for Git as it relies much more on R and R Studio rather than the command line. The course is still a work in progress but will be finished shortly – I hope – so analysts in the Midlands area can attend free courses as part of MDSN and I’ll be doing a couple for the NHS-R Community."
  },
  {
    "objectID": "blog/apha-june-blog.html#access-to-github",
    "href": "blog/apha-june-blog.html#access-to-github",
    "title": "AphA June Blog",
    "section": "Access to GitHub",
    "text": "Access to GitHub\nThe second issue is just any access to GitHub, particularly published things like books and these training slides as not every organisation allows access to the github.io url. We’ve got around this for some things by using the Netlify publishing which has a free plan up to a certain amount of usage, however, this was through my own personal account. That’s not ideal and also not a solution as some really secure IT have blocked both publishing sites. Consequently, we’ve started looking into the possibility of using the nhsrcommunity.com url which is where we currently publish the main WordPress site."
  },
  {
    "objectID": "blog/apha-june-blog.html#why-so-many-books",
    "href": "blog/apha-june-blog.html#why-so-many-books",
    "title": "AphA June Blog",
    "section": "Why so many books?",
    "text": "Why so many books?\nThe reason why having books is so nice is that I now only need to write things out in one place and can refer to them repeatedly. Quarto makes this so simple too as it can be used by both R and Python users. Whereas the previous version (still available and not going anywhere) RMarkdown was well loved by R users, it did require a bit more work to publish things like books and website, using other packages and requiring many specific files to work correctly. Moving to Quarto was more straight forward than I thought and I did use the R for Data Science (2nd Edition) Unofficial Solutions book, set up by my colleague Tom Jemmett for our book club, as a guide."
  },
  {
    "objectID": "blog/apha-june-blog.html#book-club",
    "href": "blog/apha-june-blog.html#book-club",
    "title": "AphA June Blog",
    "section": "Book club!",
    "text": "Book club!\nThe book club in NHSR started many years ago when someone asked in the Slack group if anyone else would be interested in reading “The Art of Statistics” by David Spiegelhalter with them. There was a lot of interested people and it was one of the most popular book clubs we’ve had! Since then we’ve tackles more statistics books like “The Book of Why” by Judea Pearl (I’m not going to lie, that was a tough one!), “Invisible Women” by Caroline Criado Perez (that was interesting, particularly when we looked up some of the references which didn’t really satisfy our analyst curiosity) and “R for Data Science (1st Edition)” by Hadley Wickham and Garret Grolemund. We started the Unofficial solutions book, inspired by the work of Jeffrey B Arnold.\nOriginally I’d thought about updating his book as the numbers had got out of line with the text – being online R4DS had had revisions, however, with the second edition coming out we decided to work through this together at NHS-R https://nhs-r-community.github.io/r4ds-ed2-exercise-solutions/ Book group meets every two weeks (which has changed due to holidays) 3-4pm GMT/BST (we do have some people join us internationally) and details are shared in the Slack group."
  },
  {
    "objectID": "blog/apha-june-blog.html#join-the-slack-groups",
    "href": "blog/apha-june-blog.html#join-the-slack-groups",
    "title": "AphA June Blog",
    "section": "Join the Slack groups",
    "text": "Join the Slack groups\nNow that I have one link to the Slack groups check out our book for links https://nhs-r-community.github.io/open-analytics-resources/forum-groups.html#slack!\nContact NHS-R Community via email at nhs.rcommunity@nhs.net"
  },
  {
    "objectID": "blog/announcing-a-new-nhs-r-hexitime-partnership.html",
    "href": "blog/announcing-a-new-nhs-r-hexitime-partnership.html",
    "title": "Announcing a new NHS-R + Hexitime partnership",
    "section": "",
    "text": "It all began with one member of the NHS-R Community, reaching out and asking how she could get someone more experienced with R to give her some mentoring support.\nA fair ask but quite tricky to resolve when people asking for help and those willing to provide it are based in different organisations and without involving money and contracts.\nIn our quest to address this issue, we came across Hexitime – a platform for skills exchange – developed by two NHS staff – John Lodge and Hesham Abdallah.\nHexitime is the first and currently only national timebank in health and social care, and has won multiple awards for innovating in this space. It started as a Health Foundation Q Exchange winning project in 2018 and has steadily begun to be adopted by like-minded people across healthcare. It works on the principles that everybody’s time is equal and potentially highly valuable to others who need it to progress their improvement work. As such, it utilises a virtual currency where an hour of your time = 1 credit, and you can share your skills across any borders. And because Hexitime is designed, run and owned by NHS staff, the platform is committed to operating as a not-for-profit, remaining free at the point of use. Your data and participation are governed by strict GDPR guidelines at all times.\nSee this YouTube video on Hexitime works in practice.\nNHS-R is now an official strategic partner with Hexitime. We are super excited about the prospects. Hexitime will have a formal launch at the NHS-R virtual conference in November. Meanwhile here are some highlights about our current soft launch.\n\nYou can start using Hexitime now – it is free and you will join 800+ members. Just register with your details, including NHS or other organisation, and tag that you are in the NHS-R Community.\n\n\n\nIf you have an ‘academic’ title such as NHS-R Fellow, Associate -then please tag the NHS-R Academy as well.\n\n\n\nAll NHS-R Fellows, Associates and Champions will be encouraged to register and offer their time and expertise via Hexitime.\n\n\n\nYou can offer your time to be a mentor, advisor, critical friend and so on\n\n\n\nYou can create projects and set up a project team.\n\n\n\nWe have set up a NHS-R Wishing Well – where you can drop any ideas on what you would like from us – training, support, solutions to data analytics problems you see in practice, and so on.\n\nA future blog post, hopefully by people who are using Hexitime, will report on the progress…\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/code-snippets-2-scale-y-axes-in-ggplot2.html",
    "href": "blog/code-snippets-2-scale-y-axes-in-ggplot2.html",
    "title": "Code snippets – 2 scale y axes in {ggplot2}",
    "section": "",
    "text": "Inspired by conversations on the NHS-R Slack where code answers are lost over time (it’s not a paid account), and also for those times when a detailed comment in code isn’t appropriate but would be really useful, this blog is part of a series of code snippet explanations."
  },
  {
    "objectID": "blog/code-snippets-2-scale-y-axes-in-ggplot2.html#code-answer",
    "href": "blog/code-snippets-2-scale-y-axes-in-ggplot2.html#code-answer",
    "title": "Code snippets – 2 scale y axes in {ggplot2}",
    "section": "Code answer",
    "text": "Code answer\nThis was adapted from the answer on the NHS-R Slack and uses the {NHSRdatasets} package which is available through CRAN\n\nlibrary(dplyr, warn.conflicts = FALSE)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(NHSRdatasets)\nlibrary(janitor, warn.conflicts = FALSE) # to clean the titles and making them snake_case\n\nons_mortality &lt;- NHSRdatasets::ons_mortality\n\ndeaths_data &lt;- ons_mortality |&gt;\n  filter(date &gt; \"2020-01-01\",\n         category_2 %in% c(\"all ages\", \"Yorkshire and The Humber\")) |&gt;\n  pivot_wider(\n    id_cols = c(date, week_no),\n    values_from = counts,\n    names_from = category_2\n  ) |&gt;\n  clean_names()\n\nggplot(data = deaths_data) +\n  geom_line(aes(\n    x = date, \n    y = all_ages,\n    col = \"all ages\"\n  )) +\n  geom_line(aes(\n    x = date, \n    y = yorkshire_and_the_humber,\n    col = \"Yorkshire and The Humber\"\n  )) +\n  scale_y_continuous(\n    name = \"Yorkshire and The Humber\",\n    breaks = scales::pretty_breaks(10),\n    sec.axis = sec_axis(~ . * 10,\n                        name = \"all ages\",\n                        breaks = scales::pretty_breaks(10)\n    )\n  ) +\n  scale_colour_manual(\n    name = NULL,\n    values = c(\n      \"all ages\" = \"#CC2200\",\n      \"Yorkshire and The Humber\" = \"black\"\n    )\n  )"
  },
  {
    "objectID": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html",
    "href": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html",
    "title": "NHS-R/NHS.Pycom conference 2023 my experience of the event",
    "section": "",
    "text": "The NHS-R/NHS.Pycom conference, is a great event to share on a yearly basis, our passion for advanced healthcare, promoting the use of our core R language for our healthcare analysis and also other exciting languages such as Python.\nThis is a small contribution as a blog post trying to summarize some reflections after attending for the first time last year NHS-R/NHS.pycom 2023 Online Conference. Right after the conference, I drafted some impressions in a LinkedIn blog post: https://www.linkedin.com/feed/update/urn:li:activity:7121902571715223552/ , but there were so much buzz and interesting ideas to develop months after the conference ended, that they could constitute an entire blog post.\nI just want to stress that I have tried to convey some general ideas and thoughts about last year conference, but the share amount of code, projects diversity and different background of the speakers and organisations that took place in the 2023 Conference was enough to inspire me and try new coding projects for the rest of the year.\nThere was a great range of talks from specific insights on using R packages like {purr}, to a more technical sessions like the one about Modelling non-linear data with Generalized Additive Models (GAMs), using the {mgcv} package with Chris Mainey. https://nhsrcommunity.com/events/nhs-r-nhs-pycom-online-conference-workshop-2023-modelling-non-linear-data-with-generalized-additive-models-gams-using-the-mgcv-package/\nWhat I most liked about the Conference was the mix of different speakers, ranging from Community developers, International Speakers and well known companies promoting the use of R and Rstudio such as Posit.\nSome talks were Technical about specific models or packages, and others described problem solving situations in the NHS, where R has been used to create apps, like the talk where they used R to map EHR data to OMOP Common Data model on the 9th of October Online Conference or the Automatic reporting pack generation talk on the 11th of October.\nOn top of that, I really enjoyed other talks, where Python and PowerBI examples and projects were presented. It is a very inclusive event and seeing examples of different programming languages, broads our interest as analysts. There was always a good takeaway to try after the conference in R, Python or PowerBI.\nI find really inspiring how the event manages to empower all analysts working in the NHS to add more languages to our toolbox, helping us in our daily work.\nIn this section below, I have listed the links to conference contents for each of the three main days 9, 10 and 11th of October 2023.\nNHS- R/NHS.pycom Conference: Online Conference Talks – Monday 9th October 2023 Conference recording on YouTube:https://www.youtube.com/watch?v=X-fyKq1GXQI\nNHS- R/NHS.pycom Conference: Online Conference Talks – Tuesday 10th October 2023 Conference recording on YouTube:https://www.youtube.com/watch?v=laKYVuElROs&t=1s\nNHS- R/NHS.pycom Conference: Online Conference Talks – Wednesday 11th October 2023 Conference recording on YouTube:https://www.youtube.com/watch?v=3uSbLT7ISPc\nAlso I really appreciated that during and after the Conference, on the slack channel and on the NHS-R Conference website, all the GitHub repositories were made available with the talk slides and materials presented: https://github.com/chrismainey/GAMworkshop"
  },
  {
    "objectID": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#general-thoughts-about-the-conference",
    "href": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#general-thoughts-about-the-conference",
    "title": "NHS-R/NHS.Pycom conference 2023 my experience of the event",
    "section": "",
    "text": "The NHS-R/NHS.Pycom conference, is a great event to share on a yearly basis, our passion for advanced healthcare, promoting the use of our core R language for our healthcare analysis and also other exciting languages such as Python.\nThis is a small contribution as a blog post trying to summarize some reflections after attending for the first time last year NHS-R/NHS.pycom 2023 Online Conference. Right after the conference, I drafted some impressions in a LinkedIn blog post: https://www.linkedin.com/feed/update/urn:li:activity:7121902571715223552/ , but there were so much buzz and interesting ideas to develop months after the conference ended, that they could constitute an entire blog post.\nI just want to stress that I have tried to convey some general ideas and thoughts about last year conference, but the share amount of code, projects diversity and different background of the speakers and organisations that took place in the 2023 Conference was enough to inspire me and try new coding projects for the rest of the year.\nThere was a great range of talks from specific insights on using R packages like {purr}, to a more technical sessions like the one about Modelling non-linear data with Generalized Additive Models (GAMs), using the {mgcv} package with Chris Mainey. https://nhsrcommunity.com/events/nhs-r-nhs-pycom-online-conference-workshop-2023-modelling-non-linear-data-with-generalized-additive-models-gams-using-the-mgcv-package/\nWhat I most liked about the Conference was the mix of different speakers, ranging from Community developers, International Speakers and well known companies promoting the use of R and Rstudio such as Posit.\nSome talks were Technical about specific models or packages, and others described problem solving situations in the NHS, where R has been used to create apps, like the talk where they used R to map EHR data to OMOP Common Data model on the 9th of October Online Conference or the Automatic reporting pack generation talk on the 11th of October.\nOn top of that, I really enjoyed other talks, where Python and PowerBI examples and projects were presented. It is a very inclusive event and seeing examples of different programming languages, broads our interest as analysts. There was always a good takeaway to try after the conference in R, Python or PowerBI.\nI find really inspiring how the event manages to empower all analysts working in the NHS to add more languages to our toolbox, helping us in our daily work.\nIn this section below, I have listed the links to conference contents for each of the three main days 9, 10 and 11th of October 2023.\nNHS- R/NHS.pycom Conference: Online Conference Talks – Monday 9th October 2023 Conference recording on YouTube:https://www.youtube.com/watch?v=X-fyKq1GXQI\nNHS- R/NHS.pycom Conference: Online Conference Talks – Tuesday 10th October 2023 Conference recording on YouTube:https://www.youtube.com/watch?v=laKYVuElROs&t=1s\nNHS- R/NHS.pycom Conference: Online Conference Talks – Wednesday 11th October 2023 Conference recording on YouTube:https://www.youtube.com/watch?v=3uSbLT7ISPc\nAlso I really appreciated that during and after the Conference, on the slack channel and on the NHS-R Conference website, all the GitHub repositories were made available with the talk slides and materials presented: https://github.com/chrismainey/GAMworkshop"
  },
  {
    "objectID": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#last-year-nhs-rnhs.pycom-2023-conference-was-a-great-opportunity-to-listen-live-to-analysts-and-keynote-speakers-i-have-been-reading-recently.-and-to-inspire-existing-ongoing-projects",
    "href": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#last-year-nhs-rnhs.pycom-2023-conference-was-a-great-opportunity-to-listen-live-to-analysts-and-keynote-speakers-i-have-been-reading-recently.-and-to-inspire-existing-ongoing-projects",
    "title": "NHS-R/NHS.Pycom conference 2023 my experience of the event",
    "section": "2. Last year NHS-R/NHS.pycom 2023 conference was a great opportunity to listen live to analysts and Keynote speakers I have been reading recently. And to inspire existing ongoing projects",
    "text": "2. Last year NHS-R/NHS.pycom 2023 conference was a great opportunity to listen live to analysts and Keynote speakers I have been reading recently. And to inspire existing ongoing projects\nEarlier that year, I bought and read the New Book from Bruno Rodrigues “Building reproducible analytical pipelines with R”. https://www.amazon.com/dp/B0C87H6MGF/\nThat book has been extremely helpful for me to lean how to write more robust code, collaborate on GitHub with other projects, and also learn in more detail about functional programming. In fact, I used this knowledge to create the materials I used on the Lightning Talk I had the opportunity to present on the 10th of October. “Create a website using Quarto linking Rstudio with GitHub”: https://nhsrcommunity.com/events/nhs-r-nhs-pycom-online-conference-talks-2023-ticket-for-virtual-attendance-on-tuesday-10th-october-2023/\nThe presentation for that Lightning talk is available on GitHub: https://pablo-source.github.io/NHS_R_Pycom_2023.html#/title-slide. Alongside with the materials used to build it :https://github.com/Pablo-tester/presentations/tree/main/NHS_R_NHS_Pycom_Oct23\nOverall, the topics covered during the NHS-R/NHS.pycom 2023 Conference allowed me to improve the quality of my R and Python code, and improved my collaboration with other analysts using Git and GitHub in a more efficient way.\nThis year I have tested these new skills on a new project using {target} library to build a small pipeline to build a univariate Time Series model using ARIMA and PROPHET models. I am sill expanding this initial pipeline project. https://github.com/Pablo-source/targets-test"
  },
  {
    "objectID": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#conference-workshops",
    "href": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#conference-workshops",
    "title": "NHS-R/NHS.Pycom conference 2023 my experience of the event",
    "section": "3. Conference workshops",
    "text": "3. Conference workshops\nThere were plenty of workshops available through the NHS-R/NHS.pycom Online Conference. These workshops were running in parallel to the main talk sessions, and running with a small groups of attendants. Its hands on nature was great to learn quickly joining efforts with other analysts. These workshop websites can be found in the NHS-R website past events section: https://nhsrcommunity.com/past-events/\nI would have loved to be able to attend all the workshops, but unfortunately it was not possible, but I enjoyed the following five ones, with a great combination of advanced statistical analysis and visualization techniques.\nNHS-R/NHS.pycom Online Conference Workshop 2023 – Modelling non-linear data with Generalized Additive Models (GAMs), using the {mgcv} package\nhttps://nhsrcommunity.com/events/nhs-r-nhs-pycom-online-conference-workshop-2023-modelling-non-linear-data-with-generalized-additive-models-gams-using-the-mgcv-package/\nNHS-R/NHS.pycom Online Conference Workshop 2023 – Create your own API with Python and FastAPI Workshop recording on YouTube:https://www.youtube.com/watch?v=_D4llf41KC8\nNHS-R/NHS.pycom Online Conference Workshop 2023 – Introduction to Regression Modelling in R\nNHS-R/NHS.pycom Online Conference Workshop 2023 – Data Visualisation with Seaborn Workshop recording on YouTube: https://www.youtube.com/watch?v=LZdbCOOdsPE\nAlso I do like that across all Conference workshops, RAP principles and Version control talks were present on several of the available workshops/\nNHS-R/NHS.pycom Online Conference Workshop 2023 – Introduction to Version Control (Git/GitHub) Workshop recording on YouTube: https://www.youtube.com/watch?v=wOk7wDsHD8o"
  },
  {
    "objectID": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#final-takeaway-ideas-from-last-year-conference",
    "href": "blog/nhs-r-community-conference-2023-my-experience-of-the-event.html#final-takeaway-ideas-from-last-year-conference",
    "title": "NHS-R/NHS.Pycom conference 2023 my experience of the event",
    "section": "4. Final takeaway ideas from last year conference",
    "text": "4. Final takeaway ideas from last year conference\n\nThe NHS-R NHS-R/NHS.pycom Online Conference Talks 2023 is a source of inspiration for Analysts working with R and Python, from both Keynote, Plenary and Lightning Talks.\nIs also a great opportunity to meet analysts and developers from the NHS and also other Organisations and companies, allowing you to network and match specific projects with the people and teams developing it. Great to see the faces behind all these big projects.\nIt is great to listen live international speakers from R and Posit and many other companies, explaining how they have designed the packages and tools sometimes we use in our day to day job. It is fantastic to see at the conference the developers presenting the most popular packages and tools we use in our day to day job.\nThanks to the speakers and organisation making most of the talk materials available online, it is perfect to learn at our own pace in the months after the Conference has taken place.\nAlso most of the tools used are R and Python, making open software and scripts used in the demos available on GitHub. That gives us plenty of materials to explore and learn from our own GitHub repos right after each conference day."
  },
  {
    "objectID": "blog/nhs-number-validation.html",
    "href": "blog/nhs-number-validation.html",
    "title": "NHS Number Validation",
    "section": "",
    "text": "NHS Numbers are the best. They are numbers that follow you through life, from birth to death, changing only with certain situations and are easy to check for validity from just the numbers themselves.\nWhat? You can check to see if a number is a genuine NHS Number without any other information?!\nNow, I’ve worked in the NHS for more years than I care to count and I never realised there was an algorithm to these numbers. I stumbled across this fact through Twitter of all places. Yes, Twitter, the place you’ll find NHS-R enthusiasts tweeting things like this:\nhttps://twitter.com/sellorm/status/1171858506057682944 which I saw retweeted by @HighlandR (John MacKintosh). - Link no longer works\nI’ve only just started doing things on GitHub but @Sellorm wasn’t very close in saying there may be millions of packages as there are, in fact there are only 2 for R, but surprisingly, not a single one in SQL.\nI installed the package, had a quick play around and looked at the code on GitHub. The downside for this package, for me, was that you need to feed in your data using vectors and I like using data tables. Plus, I didn’t necessarily want a list of outputs like TRUE, TRUE, FALSE, TRUE but I wanted to see the NHS numbers that aren’t valid. Still, I wouldn’t have got so far, so quickly, without @Sellorm’s code, excellent notes and thoroughly written Readme file and so the moral of the story is, even if the package doesn’t work for a task or problem, you may still be able to use parts of the code.\nThe Readme on https://github.com/sellorm/nhsnumber, which you can see when you scroll down past all of the file directory looking things, included a link to the wiki page on NHS Numbers and its algorithm check https://en.Wikipedia.org/wiki/NHS_number. Again, I had no idea this existed!\nWiki, like R, is open sourced and one of the links was out of date. I’d found a really useful document on NHS Numbers a while back https://www.closer.ac.uk/wp-content/uploads/CLOSER-NHS-ID-Resource-Report-Apr2018.pdf, so, in the spirit of open source, I updated the Wiki page. Proud moment!\nThe algorithm is quite simple and another package on GitHub https://github.com/samellisq/nhsnumbergenerator generates numbers using it but I didn’t spend too long on these packages as I decided to do my own script, nothing fancy, no loops, functions or packages…\n\nlibrary(tidyverse)\n\nValidity &lt;- df %&gt;%\n  mutate(\n    length = nchar(NHSNumber),\n    A = as.numeric(substring(NHSNumber, 1, 1)) * 10,\n    B = as.numeric(substring(NHSNumber, 2, 2)) * 9,\n    C = as.numeric(substring(NHSNumber, 3, 3)) * 8,\n    D = as.numeric(substring(NHSNumber, 4, 4)) * 7,\n    E = as.numeric(substring(NHSNumber, 5, 5)) * 6,\n    G = as.numeric(substring(NHSNumber, 6, 6)) * 5,\n    H = as.numeric(substring(NHSNumber, 7, 7)) * 4,\n    I = as.numeric(substring(NHSNumber, 8, 8)) * 3,\n    J = as.numeric(substring(NHSNumber, 9, 9)) * 2,\n    End = as.numeric(substring(NHSNumber, 10, 10)),\n    Total = A + B + C + D + E + G + H + I + J,\n    Remainder = Total %% 11,\n    Digit = 11 - Remainder,\n    Summary = case_when(\n      Digit == 10 ~ 999,\n      Digit == 11 ~ 0,\n      TRUE ~ Digit\n    ),\n    Valid = case_when(\n      Summary == End & length == 10 ~ TRUE,\n      TRUE ~ FALSE\n    )\n  ) %&gt;%\n  filter(Valid == FALSE)\n\nImporting data\nData is imported into R via Excel or, in my case, I used a SQL connection which worked better as a couple of our systems hold many hundreds of thousands of NHS Numbers and I wanted to check them all. Excel might have got a bit of indigestion from that. Also, it meant I wasn’t storing NHS Numbers anywhere to then run through R. My systems are secure but it’s always worrying having such large amounts of sensitive data in one place and outside of an even securer system like a clinical database.\nData doesn’t always import into R the same way and for mine I needed to remove NULLs and make the NHSNumber column, which was a factor, into a character and then numeric format:\n\ndf &lt;- data %&gt;%\n  filter(!is.na(NHSNumber)) %&gt;%\n  mutate(NHSNumber = as.numeric(as.character(NHSNumber)))\n\nFactors\nFactors are a new thing for me as but, as I understand it, they put data into groups but in the background. For example, if you had male, female and other these would be 3 factors and if you join that to another data set which doesn’t happen to have “other” as a category the factor would still linger around in the data, appearing in a count as 0 as the table still has the factor information but no data in that group.\nTo manipulate factor data I’ve seen others change the format to character and so that’s what I did. I’ve done similar things in SQL with integers and dates; sometimes you have to change the data to an ‘intermediate’ format.\nValidity code – explanation\nThis code uses dplyr from tidyverse to add columns:\nValidity &lt;- df %&gt;% \n  mutate(length = nchar(NHSNumber)),\nAn NHS number must be 10 characters and nchar() reminds me of LEN() in SQL which is what I would use to check the length of data.\nOne thing I didn’t code, but I guess people may want to check, is that all characters are numeric and no letters have been introduced erroneously. That’s something to consider.\n     A = as.numeric(substring(NHSNumber,1,1)) *10,\n     B = as.numeric(substring(NHSNumber,2,2)) *9,\n     C = as.numeric(substring(NHSNumber,3,3)) *8,\n     D = as.numeric(substring(NHSNumber,4,4)) *7,\n     E = as.numeric(substring(NHSNumber,5,5)) *6,\nI didn’t use F as it’s short for FALSE which changed the meaning and colour of the letter. It can be used as a column name but I missed it out for aesthetics of the script!\n     G = as.numeric(substring(NHSNumber,6,6)) *5,\n     H = as.numeric(substring(NHSNumber,7,7)) *4,\n     I = as.numeric(substring(NHSNumber,8,8)) *3,\n     J = as.numeric(substring(NHSNumber,9,9)) *2,\nThere is possibly a much smarter way of writing this, perhaps a loop. Samuel Ellis’ package nhsnumbergenerator creates a table using sequences:\ncheckdigit_weights = data.frame(digit.position=seq(from=1, to = 9, by =1),\n                                weight=seq(from=10, to = 2, by =-1)\n)\nnote he uses = rather than &lt;-\nI like this code. It’s very concise, but it just was easier writing out the multiplications for each part of the NHS Number; the first number is multiplied by 10, the second by 9, the third by 8 and so on.\n    End = as.numeric(substring(NHSNumber,10,10)),\nput this in as the final number in the sequence is the check for the later Digit (also called checksum).\n     Total = A+B+C+D+E+G+H+I+J,\nJust adding all the multiplied columns together.\n     Remainder = Total %% 11,\nThis gives the remainder from a division by 11.\n     Digit = 11- Remainder,\n11 take away the remainder/checksum.\n     Summary = case_when(Digit == 10 ~ 999,\n                         Digit == 11 ~ 0,\n                         TRUE ~ Digit),\nLots of people use if(else()) in R but I like case_when() because it’s like SQL. The lines run in logical order:\n\nwhen the digit = 10 then it’s invalid so I put in 999 as that’s so high (in the hundreds) it shouldn’t match the Digit/Checksum (which is a unit),\nif it’s 11 then change that to 0 following the methodology,\nelse just use the digit number.\n\nActually, thinking about it I probably don’t need the 10 becomes 999 as 10 could never match a single unit number. Perhaps that’s redundant code.\n     Valid = case_when(Summary == End & length == 10 ~ TRUE,\n                       TRUE ~ FALSE\n                       )) %&gt;% \ncase_when() again but this time to get the TRUE/FALSE validity. If the number generated is the same as the last digit of the NHS Number AND the length of the NHS Number is 10 then TRUE, else FALSE.\nI liked this like of code as it was a bit strange saying TRUE then FALSE but it’s logical!\n  filter(Valid == FALSE)\nJust bring back what isn’t valid.\nDid it work?\nI think it did. If I got anything wrong I’d love to get feedback but I ran through many hundreds of thousands of NHS Numbers through it and found…..\nNo invalid numbers\nI possibly should have checked beforehand but I suspect our clinical systems don’t allow any incorrect NHS Numbers to be entered in at source. Still, it was fun and could be applied to manually entered data from data kept in spreadsheets for example.\nAn interesting blog\nAs here were no SQL code scripts on GitHub for NHS Number validations I did a quick search on the internet and found this: https://healthanalyst.wordpress.com/2011/08/21/nhs-number-validation/ which is a blog by @HealthAnalystUK from 2011. The reason I’m referring to it in this blog is because HealthAnalystUK not only shared SQL code that looked very similar to the code here but also R and uses it as a function:\n\nNHSvalidation &lt;- function(NHSnumber) {\n  NHSlength &lt;- length(NHSnumber)\n\n  A &lt;- as.numeric(substr(NHSnumber, 1, 1))\n  B &lt;- as.numeric(substr(NHSnumber, 2, 2))\n  C &lt;- as.numeric(substr(NHSnumber, 3, 3))\n  D &lt;- as.numeric(substr(NHSnumber, 4, 4))\n  E &lt;- as.numeric(substr(NHSnumber, 5, 5))\n  F &lt;- as.numeric(substr(NHSnumber, 6, 6))\n  G &lt;- as.numeric(substr(NHSnumber, 7, 7))\n  H &lt;- as.numeric(substr(NHSnumber, 8, 8))\n  I &lt;- as.numeric(substr(NHSnumber, 9, 9))\n  J &lt;- as.numeric(substr(NHSnumber, 10, 10))\n\n  if ((A == B) & (B == C) & (C == D) & (D == E) & (E == F) & (F == G) & (G == H) & (H == I) & (I == J)) {\n    UniformNumberCheck &lt;- 1\n  } else {\n    UniformNumberCheck &lt;- 0\n  }\n\n  Modulus &lt;- ((A * 10) + (B * 9) + (C * 8) + (D * 7) + (E * 6) + (F * 5) + (G * 4) + (H * 3) + (I * 2))\n  Modulus &lt;- (11 - (Modulus %% 11))\n\n  if (\n\n    ((Modulus == J) & (UniformNumberCheck != 1) & (NHSlength == 10)) | ((Modulus == 11) & (J == 0) & (UniformNumberCheck != 1) & (NHSlength == 10))) {\n    ReturnValue &lt;- 1\n  } else {\n    ReturnValue &lt;- 0\n  }\n\n  return(ReturnValue)\n}\n\nI hadn’t coded the check for repeating numbers:\nif ((A==B)&(B==C)&(C==D)&(D==E)&(E==F)&(F==G)&(G==H)&(H==I)&(I==J))\nand I couldn’t find any reference to this in the Wiki page or the document from the University of Bristol so I’m unsure if this is a part of the methodology. If it is, then I’ve seen at least 1 NHS Number that would fail this test.\nA conclusion\nIf anyone has created a package or script for NHS number checks and wants to share please feel free to write a blog. NHS-R Community also has a GitHub repository at https://github.com/nhs-r-community where code like this blog can go (I wrote this in RMarkdown).\nBlogs can be emailed to nhs.rcommunity@nhs.net and get checked by a group of enthusiast volunteers for publishing.\nThis blog was written Zoë Turner, Senior Information Analyst at Nottinghamshire Healthcare NHS Trust.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/pareto-chart-in-ggplot2.html",
    "href": "blog/pareto-chart-in-ggplot2.html",
    "title": "Pareto Chart in ggplot2",
    "section": "",
    "text": "library(ggplot2)\n\nBackground of Pareto Charts\nA Pareto chart, named after Vilfredo Pareto, is a type of chart that contains both bars and a line graph, where individual values are represented in descending order by bars, and the cumulative total is represented by the line.\nThese charts are highly utilised in Six Sigma circles and conform to the Pareto Principle. The construction of the tool, in R, is shown in the following post.\nCreating the R Data Frame\nTo create the R data frame we can use the following code:\n\ncount &lt;- c(300, 430, 500, 320, 321, 543)\nxVariable &lt;- c(\"Nottingham\", \"Bristol\", \"Leeds\", \"Keswick\", \"Derby\", \"Norfolk\")\n\nThis creates two vectors using the combine function (denoted with a c). These two code lines are run in by either selecting the code blocks and using the Run command in R Studio, or by using the key combination of Control + Enter.\nNext, the data frame will be created:\n\nmyDf &lt;- data.frame(\n  count = count,\n  UKCity = xVariable,\n  stringsAsFactors = FALSE\n)\n\nThis creates a data frame called myDf and uses the data.frame command to create a data frame with two fields, one named count using the count vector and another named UKCity using the xVariable vector.\nAdding the Pareto components\nThus far, we have simply created a data frame with two columns. The next step will be to sort the categorical variable (UKCity) by the count associated with that field, so the highest counts will be grouped first on the Pareto Chart and we will add a cumulative statistic to the data frame in order to draw the traditional Pareto cumulative sum line.\nPerforming the descending sort\nThe R code here shows how to do the descending sort:\n\nmyDf &lt;- myDf[order(myDf$count, decreasing = TRUE), ]\n\nWhat this does is uses the myDf data frame and then changes the order of the counts to show a decreasing count. This will put the highest counts at the top and the lowest at the bottom of the dataset. You could change the decreasing to FALSE, and this would force an ascending order.\nConverting the categorical variable to a factor\nWe use the add field notation of $ (dollar) to add a new column to the data frame:\n\nmyDf$UKCity &lt;- factor(myDf$UKCity, levels = myDf$UKCity)\n\nThis says use the myDf data frame and overwrite the existing column = UKCity. Then, create a unique factor for each of those categorical variables, so if you had repeating categories say three cities that equalled Nottingham, this would show as just one factor instead of three. This is a way to group multiple categorical variables together. The levels = command specifies what levels to set the factor to.\nAdding a cumulative sum to the data frame\nThis is simple: taking the count field we add each of the counts together to form a cumulative total:\n\nmyDf$cumulative &lt;- cumsum(myDf$count)\n\nThe code block shows that a field called cumulative has been added to the data frame (myDf) and the cumsumfunction has been used to create that cumulative total.\nThe data frame now looks like this:\n\n\n\n\n\n\ncount\nUKCity\ncumulative\n\n\n\n543\nNorfolk\n543\n\n\n500\nLeeds\n1043\n\n\n430\nBristol\n1473\n\n\n321\nDerby\n1794\n\n\n320\nKeswick\n2114\n\n\n300\nNottingham\n2414\n\n\n\n\n\n\nIn terms of data preparation, everything is now in place to start working with {ggplot2}.\nCreating the Pareto Chart (with ggplot2)\nThe following code with be included in full, followed by an explanation of what the code is doing:\n\nlibrary(ggplot2)\nggplot(myDf, aes(x = myDf$UKCity)) +\n  geom_bar(aes(y = myDf$count), fill = \"blue\", stat = \"identity\") +\n  geom_point(aes(y = myDf$cumulative), color = rgb(0, 1, 0), pch = 16, size = 1) +\n  geom_path(aes(y = myDf$cumulative, group = 1), colour = \"slateblue1\", lty = 3, size = 0.9) +\n  theme(axis.text.x = element_text(angle = 90, vjust = 0.6)) +\n  labs(\n    title = \"Pareto Plot\", subtitle = \"Produced by Gary Hutson\", x = \"UK Cities\", y =\n      \"Count\"\n  )\n\nFully explained: the library for {ggplot} is loaded into the current environment. Secondly, the {ggplot} function is used to specify the data and the aesthetics of the column to be used. This is then added to specify a bar chart (geom_bar) using the count field (y axis) and filling the bars with the colour blue.\nThe purpose of the stat = \"identity\" is to force {ggplot} to identify the count field stipulated on the y axis. Again, the bar is then added to a point geometry (geom_point) and the point is the cumulative total we calculated with the colour defined using an RGB value, with the point type (pch) equal to a circle.\nThen, the geom_path function is used to add a path to the points and to specify what data type to group to, this is normally defaulted to group = 1.\nFinally, the last part is to apply a custom theme to specify that the element_text should be at a 90 degree angle and the vertical justification equal to 0.6.\nRunning this code produces the Pareto plot, as below:\n\n\n\n\n\n\n\n\nThat’s it, we now have an excellent looking chart that can be reproduced every time R is refreshed.\nThis blog was written by Gary Hutson, Principal Analyst, Activity & Access Team, Information & Insight at Nottingham University Hospitals NHS Trust, and was originally posted at Hutson-Hacks.\nThis blog has been edited for NHS-R Style.\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/16th-january-newscast.html",
    "href": "blog/16th-january-newscast.html",
    "title": "NHS-R newscast 16th January 2023",
    "section": "",
    "text": "Time for another NHS-R newscast! You can listen to the episode, but for those of you who don’t listen to podcasts here is a roundup of what we covered\nThe first item of news is that The Strategy Unit is the new home of NHS-R. Chris Beeley and Zoë Turner both now work at The Strategy Unit and have NHS-R in their job descriptions. They plan to be doing much more work in the future on communicating better with members (maybe a newsletter, more blogs…), organising hacks (which would be a first for the community), and a mentoring scheme (more on which later).\nAnother bit of news that was teased at the NHS-R conference was the new national conference for healthcare analytics, which does not yet have a proper name. Abstract submission will open soon and NHS-R will be announcing mentoring to go alongside the conference to help people to submit abstracts and prepare their talks. The conference will be focused on strategic analysis and decision making (as opposed to NHS-R conference which tends to be more about methods) and so do be thinking about what you might like to bring to the conference. If you feel you would like help, watch the skies because NHS-R will be offering assistance all the way through the process, from getting your talk accepted to actually presenting it on the day.\nWe talked about the importance of having diverse voices in NHS-R and making sure we are welcoming to as many people as possible. Tech communities can often be quite white and male and NHS-R needs to make sure that it engages people from all walks of life.\nThere was a roundup of interesting issues on the NHS-R GitHub, and a discussion about how helpful tackling these issues can be, for us as a community but also for you in terms of skills development and having work in the open for your CV. Please have a look at these results and others throughout the repos:\nhttps://github.com/nhs-r-community/NHSRplotthedots/issues/141\nhttps://github.com/nhs-r-community/ISR-iterative-sequential-regression-/issues\nhttps://github.com/nhs-r-community/git_training/issues/4\nhttps://github.com/nhs-r-community/statements-on-tools/issues\nIf you need any help getting started, say so on the Slack and a member of the community will be sure to come along to assist you. Note also that many of these issues are about documentation, not code, so you can make a really valuable contribution without writing code.\nWe talked about semantic versioning and how useful it can be.\nChris made a passing reference to the Python 2/ 3 mess, particularly in the Linux world, and as always there is an XKCD for that.\nLastly we talked about the amazing John MacKintosh’s patientcounter package, as well as some more of his packages such as rock themes and pop themes:\nhttps://github.com/johnmackintosh/rockthemes\nhttps://github.com/johnmackintosh/popthemes\nComments? Questions? Email nhs.rcommunity@nhs.net\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/building-a-quarto-website.html",
    "href": "blog/building-a-quarto-website.html",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "A few years ago Mohammed A Mohammed, who was integral to getting NHS-R Community set up, suggested I might like to get involved by creating a website for NHS-R Community. At that time I had no idea where to start so my involvement stalled and a website was built, with help from a 3rd party, in WordPress. WordPress is a great website tool that can support very complicated sites like NHS-R Community’s, as we have both a static site and an Events Management system which means that we don’t need to rely (or pay!) for Eventbrite. We’ve used the system for sign up to hundreds of webinars and workshops, not to mention several hundred people attending conferences over the years.\n\n\nIn the time it’s taken from that initial suggestion about building the website to now, where I help to manage the site, I’ve had the opportunity to build websites for myself using {distill} and Hugo Apéro as well as the release of Quarto which I was incredibly keen to try out for website creation.\nI’d used Quarto for slides and reports and been hugely impressed with it. Getting started with Quarto is relatively easy if you’ve coded for a while, but like with anything, you have to start off at the basics and quickly want more complex functionality as you get going. This is even more of an issue when you are moving from one system to another and it’s more of a translation than a build-from-scratch project.\n\n\nAlthough Quarto has come from the R side of coding it can be used by any other language coder as it supports Python, Julia and Observable. Quarto output can be built in RStudio but equally as easily in other programs like VS Code.\nLuckily for me, many people had already jumped into Quarto websites and their code was available online to delve into. I particularly was keen to follow the work of the Brian Tarran for the Royal Statistical Society’s Real World Data Science website because, at first glance, this doesn’t give itself away as being built on Quarto! And I also really like Silvia Canelón’s personal blog site for its beauty and functionality. Silvia has particularly worked hard on accessibility of her website and has written a lot of CSS code which I’m learning through what she has shared.\nAccessibility is something that the WordPress site needed considerable work on, particularly when checked with a website called the Web Accessibility Evaluation Tool which Silvia recommends on her blog site. The new Quarto site will need this too of course but now that the code is out in the open, anyone can contribute. That can be either making changes to the code but also highlighting problems as issues that can then be open to everyone to view.\n\n\n\n\n\nI’ll mention accessibility again as that’s hugely important and I like the fact that the people working on Quarto also think a lot about accessibility. Adding alt text has never been easier and any code (in the YAML) that refers to an image nearly always has functionality to add image alt text too.\n\n\n\nThe search function for Quarto books is really impressive and the same functionality can be found in Quarto websites. There might be a plug in within WordPress to make searching through blogs for particular words but with Quarto it’s not something that needs adding - it’s there from the start. I now no longer have to think about tags or categories for blogs to make them easier to find (and then remember them to find the blogs which is the trickier part for me!).\n\n\n\nThis was a bit of a niggle with WordPress in that it’s not really a website for coders to show their code examples (although there may be a plug in for that!). Quarto handles code snippets and examples by beautifully formatting it to look different to text but also runs the code for you if you want. It’s meant that the charts that are talked about in some blogs are produced as the code is run, not as static pictures that also can’t be copied easily as a picture doesn’t give the functionality for highlighting to copy.\nUsually a blogger will choose a nice image to complement their writing and Quarto gives that functionality but I had a lovely surprise when charts that are created by the blog code became the thumbnail image automatically.\n\n\n\nHaving a website that can publish R or Python code is an absolute joy. I used to write blogs in RMarkdown, render them to html and then copy the output to WordPress and in the copying and pasting often lost links or formatting.\nBeing open to contribution of course also means that the repository will be available to people to do pull requests. I’ve set the repository so that it’s not necessary to Render the website to contribute so any additions or changes just need to be made to the qmd files.\n\n\n\nNow this point could have come from any change in website but I’ve taken the opportunity to go through each blog and transfer it to Quarto. There are no doubt quicker ways to do this as I can export from WordPress and then work on code to to get it to a format that can be published, but I started with moving a couple of blogs to get started and I’ve really enjoyed this even though it takes a bit more time to do.\nStarting in 2018 when the blogs first started I didn’t know R at all. I started learning with NHS-R Community so these early blogs meant very little to me at the time. As I’ve gone through each blog, formatting them and checking the R code runs (I found a couple of tiny mistakes this way) and the links still work I’ve been more like an archivist than a coder. My favourite discovery, and shock, was that I found that RAP (Reproducible Analytical Pipelines) was shared with the NHS-R Community in 2018 although I only really heard about it from 2020. It was very much like the moment I experienced when I went through my old Geography school books and found I’d been taught in those lessons about Index of Multiple Deprivation (IMD) but I had absolutely no recollection of it at all! It hadn’t been the right time for me to really hear about it and yet in time, it’s become something that I now really love working with. You can read more about IMD in a Quarto collaborative book for NHS-R Community called Health Inequalities.\n\n\n\n\nKnowing where to start with contributions can be a barrier so I’ve written out a few things to try to help people do this. Firstly, our Quarto book (yes another one - they are additive be warned!) Statement on Tools includes some technical appendices including Contributing to GitHub repositories.\nIf you want to get involved with the NHS-R Community more generally we are building on the NHS-R Way which essentially documents the community. We’ve got a number of activities listed out and these are all open to get involved with, or set up something else.\nWe can be found, as a community, in Slack and our central email is nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#moving-to-purely-coded-websites",
    "href": "blog/building-a-quarto-website.html#moving-to-purely-coded-websites",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "In the time it’s taken from that initial suggestion about building the website to now, where I help to manage the site, I’ve had the opportunity to build websites for myself using {distill} and Hugo Apéro as well as the release of Quarto which I was incredibly keen to try out for website creation.\nI’d used Quarto for slides and reports and been hugely impressed with it. Getting started with Quarto is relatively easy if you’ve coded for a while, but like with anything, you have to start off at the basics and quickly want more complex functionality as you get going. This is even more of an issue when you are moving from one system to another and it’s more of a translation than a build-from-scratch project.\n\n\nAlthough Quarto has come from the R side of coding it can be used by any other language coder as it supports Python, Julia and Observable. Quarto output can be built in RStudio but equally as easily in other programs like VS Code.\nLuckily for me, many people had already jumped into Quarto websites and their code was available online to delve into. I particularly was keen to follow the work of the Brian Tarran for the Royal Statistical Society’s Real World Data Science website because, at first glance, this doesn’t give itself away as being built on Quarto! And I also really like Silvia Canelón’s personal blog site for its beauty and functionality. Silvia has particularly worked hard on accessibility of her website and has written a lot of CSS code which I’m learning through what she has shared.\nAccessibility is something that the WordPress site needed considerable work on, particularly when checked with a website called the Web Accessibility Evaluation Tool which Silvia recommends on her blog site. The new Quarto site will need this too of course but now that the code is out in the open, anyone can contribute. That can be either making changes to the code but also highlighting problems as issues that can then be open to everyone to view."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#benefits-to-quarto",
    "href": "blog/building-a-quarto-website.html#benefits-to-quarto",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "I’ll mention accessibility again as that’s hugely important and I like the fact that the people working on Quarto also think a lot about accessibility. Adding alt text has never been easier and any code (in the YAML) that refers to an image nearly always has functionality to add image alt text too.\n\n\n\nThe search function for Quarto books is really impressive and the same functionality can be found in Quarto websites. There might be a plug in within WordPress to make searching through blogs for particular words but with Quarto it’s not something that needs adding - it’s there from the start. I now no longer have to think about tags or categories for blogs to make them easier to find (and then remember them to find the blogs which is the trickier part for me!).\n\n\n\nThis was a bit of a niggle with WordPress in that it’s not really a website for coders to show their code examples (although there may be a plug in for that!). Quarto handles code snippets and examples by beautifully formatting it to look different to text but also runs the code for you if you want. It’s meant that the charts that are talked about in some blogs are produced as the code is run, not as static pictures that also can’t be copied easily as a picture doesn’t give the functionality for highlighting to copy.\nUsually a blogger will choose a nice image to complement their writing and Quarto gives that functionality but I had a lovely surprise when charts that are created by the blog code became the thumbnail image automatically.\n\n\n\nHaving a website that can publish R or Python code is an absolute joy. I used to write blogs in RMarkdown, render them to html and then copy the output to WordPress and in the copying and pasting often lost links or formatting.\nBeing open to contribution of course also means that the repository will be available to people to do pull requests. I’ve set the repository so that it’s not necessary to Render the website to contribute so any additions or changes just need to be made to the qmd files.\n\n\n\nNow this point could have come from any change in website but I’ve taken the opportunity to go through each blog and transfer it to Quarto. There are no doubt quicker ways to do this as I can export from WordPress and then work on code to to get it to a format that can be published, but I started with moving a couple of blogs to get started and I’ve really enjoyed this even though it takes a bit more time to do.\nStarting in 2018 when the blogs first started I didn’t know R at all. I started learning with NHS-R Community so these early blogs meant very little to me at the time. As I’ve gone through each blog, formatting them and checking the R code runs (I found a couple of tiny mistakes this way) and the links still work I’ve been more like an archivist than a coder. My favourite discovery, and shock, was that I found that RAP (Reproducible Analytical Pipelines) was shared with the NHS-R Community in 2018 although I only really heard about it from 2020. It was very much like the moment I experienced when I went through my old Geography school books and found I’d been taught in those lessons about Index of Multiple Deprivation (IMD) but I had absolutely no recollection of it at all! It hadn’t been the right time for me to really hear about it and yet in time, it’s become something that I now really love working with. You can read more about IMD in a Quarto collaborative book for NHS-R Community called Health Inequalities."
  },
  {
    "objectID": "blog/building-a-quarto-website.html#contributions-are-welcome",
    "href": "blog/building-a-quarto-website.html#contributions-are-welcome",
    "title": "Building a Quarto website for NHS-R Community",
    "section": "",
    "text": "Knowing where to start with contributions can be a barrier so I’ve written out a few things to try to help people do this. Firstly, our Quarto book (yes another one - they are additive be warned!) Statement on Tools includes some technical appendices including Contributing to GitHub repositories.\nIf you want to get involved with the NHS-R Community more generally we are building on the NHS-R Way which essentially documents the community. We’ve got a number of activities listed out and these are all open to get involved with, or set up something else.\nWe can be found, as a community, in Slack and our central email is nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "blog/learned-from-community.html",
    "href": "blog/learned-from-community.html",
    "title": "Learned from Community",
    "section": "",
    "text": "How many instant message notifications do you get a day? We get message notifications for conversations we have with others, news we subscribe from our computers, smartphones, smart home devices, wearable techs and so on. We live in a time where we learn anything by ‘googling’ for questions or we ask two well-known people living in the cloud called Siri or Alexa who retrieve answers for us within seconds.\nIn my case, most of the time when self-murmuring and talking to tech devices have proven that I am completely lonely and isolated, I turn to Slack to post my questions and wait patiently for a response.\nSlack is one of the many daily notifications I receive. NHS-R Slack is one of many communities I am part of.\nAt NHS-R Slack, we have 40 active channels and over 700 members in our communities.\n\nTo me, NHS-R Slack is unique in many ways. Not only it is a place I learn programming knowledge on R, Python, Julia and Linux but also how colleagues apply these skills into practice.\nAnd that is exciting! Why?\nWell, recently I have come to understand my learning style and appreciate that a powerful way to improve learning and memory is by practising it. I hope this resonates with you that, sometimes, learning is hard; finding time to learn and memorise new skills. My takeaway is, we can convert short term memory into long term memory by practising it via active learning approaches. I guess this is why I suggested starting a #book-club to cultivate my cognitive learning ability.\nTo me, Slack is not just a collaborative working space but also a learning platform. As the saying goes – “You don’t know what you don’t know”. Let me give you an example.\nA few months ago, this thread started. I have heard of DES (Discrete Event Simulation) in theory however, following this thread has led me to learn about PathSimR developed by Bristol, North Somerset and South Gloucestershire CCG with Health Foundation.\n\n\nEager to find out and learn more, I messaged Ben Murch for a chat. After that, I used Hexitime to request his help on how to use the package. By the beginning of January, I test drove this R package to apply DES model with social care homes demands by pathway. It is still work in progress but this is one of many scenarios where I’ve learned new skills from the NHS-R slack community.\nTextbooks aren’t the only source for knowledge. Information sources are abundant. Learning new skills can be tough and requires our cognitive resources to store new knowledge gained. Learning a new skill and immediately applying it can maximise your learning cognitive ability. Come join me at NHS-R Slack Community and learn new skills together.\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "blog/alignment-cheatsheet.html",
    "href": "blog/alignment-cheatsheet.html",
    "title": "Alignment cheatsheet",
    "section": "",
    "text": "I’ve used ggtext’s geom_textbox() a lot in my recent data visualisations, but, every time, I end up resorting to a bit of trial and error to get the alignment just right. Time to create a cheatsheet that will be useful to my future self, and hopefully to a few others also."
  },
  {
    "objectID": "blog/alignment-cheatsheet.html#first-why-geom_textbox",
    "href": "blog/alignment-cheatsheet.html#first-why-geom_textbox",
    "title": "Alignment cheatsheet",
    "section": "First, why geom_textbox()?",
    "text": "First, why geom_textbox()?\nI like it for its versatility. It allows for fun things with markdown and a limited number of html tags, which makes it easy to format the text on the fly. I’ll demo a few tricks in the code below but cover them in more detail elsewhere. It also allows for easy placement and manipulation of the box itself, tweaking the radius of the corners, the colour, how thick the border is, and many more things.\nWhat are the alignment parameters?\n\nvjust\nvalign\nhjust\nhalign\n\nThese can be set per textbox using values within aes() to allow for different alignments for different textboxes, or across the board outside of aes() so that the same alignment settings are applied to all the textboxes. In this demo, I’ll set different values for each box."
  },
  {
    "objectID": "blog/alignment-cheatsheet.html#ok-lets-go",
    "href": "blog/alignment-cheatsheet.html#ok-lets-go",
    "title": "Alignment cheatsheet",
    "section": "OK, let’s go!",
    "text": "OK, let’s go!\nFirst, we need to load the necessary libraries:\n\nlibrary(tidyverse)\nlibrary(ggtext)\nlibrary(extrafont)\n\nNext, let’s put a tibble together to provide data for our textboxes.\nalignments &lt;- tibble(\"h_align\" = sort(rep(c(0, 0.5, 1), 3)),\n                     \"h_just\" = rep(c(0, 0.5, 1), 3), \n                     \"v_align\" = rep(c(0, 0.5, 1), 3),\n                     \"v_just\" = sort(rep(c(0, 0.5, 1), 3))) %&gt;%\n  mutate(\"content\" = \n           paste0(\"&lt;span style=\"font-size:36px\"&gt;↕️&lt;/span&gt; **vjust = \", v_just,\n                  \", valign = \", v_align, \"**&lt;br&gt;\",\n                  \"&lt;span style=\"font-size:36px\"&gt;↔️&lt;/span&gt; **hjust = \", h_just,\n                  \", halign = \", h_align, \"**&lt;br&gt;&lt;br&gt;\",\n                  \"Here's some text in a box, and *this* is how everything aligns!\"))\nAnd finally, let’s create a plot to see how everything aligns.\nggplot(alignments) +\n  geom_textbox(aes(x = h_just, y = v_just, label = content,\n                   halign = h_align, hjust = h_just,\n                   valign = v_align, vjust = v_just),\n               size = 6.4,\n               colour = \"#232a27\",\n               box.colour = \"#705c70\", \n               fill = \"#f1f4f3\",\n               family = \"Lato\",\n               width = unit(16, \"lines\"),\n               height = unit(15, \"lines\"),\n               lineheight = 1.3,\n               show.legend = F) +\n  geom_point(aes(x = h_just, y = v_just), \n             size = 10, alpha = 0.5, colour = \"#232a27\") +\n  labs(title = \"ngeom_textbox() alignment cheatsheet\",\n       subtitle = \"nThe dots indicate the x/y coordinates of each box\",\n       caption = \"Graphic: @cararthompson | cararthompson.comn\") +\n  xlim(c(-0.1, 1.1)) +\n  ylim(c(-0.1, 1.1)) +\n  theme_void()%+replace%\n  theme(plot.title = element_text(colour = \"#232a27\", \n                                  size = 60, family = \"Abel\"),\n        plot.subtitle = element_text(colour = \"#232a27\", \n                                     size = 40, family = \"Lato\"),\n        plot.caption = element_text(colour = \"#232a27\", size = 20, \n                                    family = \"Abel\", hjust = 0.95))\n\n\nPlot demonstrating how each alignment parameter affects the position of the box and the position of the text therein"
  },
  {
    "objectID": "blog/alignment-cheatsheet.html#so-what-does-what",
    "href": "blog/alignment-cheatsheet.html#so-what-does-what",
    "title": "Alignment cheatsheet",
    "section": "So, what does what?",
    "text": "So, what does what?\n\nh/vjust determines the alignment of the box against its coordinates\nh/valign determines the alignment of the text within the box\nfor h/valign 0 to 1 moves the text from bottom to top and from left to right inside the boxes (huh, that’s the same direction as 0 to 1 on the axes – that should make it easier to remember!)\nfor h/vjust 0 to 1 goes… well I’m not too sure!\n\nI think my confusion around how to verbalise the direction lies in what we use as our reference point. Are we aligning the top of the box to the coordinate, or are we aligning the box to the bottom of the coordinate? If we keep our focus on the bottom/middle top of the box, vjust = 1 means “make the top of this box line up with the y-coordinate” and the 0 to 1 direction stays consistent with the direction of the y axis. Happy days!\nBut I think I’ll probably still end up referring back to this plot.\nOriginally posted: https://www.cararthompson.com/posts/2021-09-02-alignment-cheat-sheet/alignment-cheat-sheet.html"
  },
  {
    "objectID": "blog/alignment-cheatsheet.html#citation",
    "href": "blog/alignment-cheatsheet.html#citation",
    "title": "Alignment cheatsheet",
    "section": "Citation",
    "text": "Citation\nFor attribution, please cite this work as:\nThompson, Cara. 2021. “Alignment Cheatsheet.” September 2, 2021. https://www.cararthompson.com/posts/2021-09-02-alignment-cheat-sheet/alignment-cheat-sheet.html."
  },
  {
    "objectID": "blog/nhsr-rguably-the-best-conference-in-the-world.html",
    "href": "blog/nhsr-rguably-the-best-conference-in-the-world.html",
    "title": "NHSr – Rguably the Best Conference in the World",
    "section": "",
    "text": "The 2020 vintage was my first time at an NHSr Community event, and boy was it impressive.\nBeing the year that is in it, everything was online. That still feels a bit surreal. My experience of these kind of events this year has been a bit hit-and-miss, but NHSr got pretty everything right, making a rewarding and enjoyable week. Proof that when you have the saviour-faire, the joie-de-vivre will still come through. Sans souci.\nThis must have been a whole team effort from lots of people, but I have to give special plaudits to Anastasiia and Mohammed.\nMy personnel highlights of the week:\n\nThe Tuesday talk by Athanasia Monika Mowinckel on brain imaging\nBen Alcock’s maps of COVID outbreaks\nThe Thursday afternoon (what a line-up)\nHillary Juma on the sense of community\nand the post conference chat on the Friday afternoon.\n\nMy take-aways from the conference:\n\nif you can measure the success of a conference by the number of new packages you end up installing, then NHSr rates pretty highly.\nR embodies positive thinking and community spirit, and NHSr has that in spades. There was a palpable sense of engagement by everyone throughout the week\nThe diversity of work going on. Lots of teams making some very useful packages (monstR, phsmethods, phstemplates, opensafely, ggseg, thematic, COCATOO….), lots of teams working on the fostering of data science in the NHS (Health Foundation, APHA, NHSx, Heart Foundation, for example), lots of front line work, lots of speakers more on the philosophy of R (Mohammed, Alberto Cairo….).\nAnd penguins. The Palmer penguins seemed like a daily feature. Except, oddly, by Allison Horst, the person who actually put the dataset together.\n\nAnd I got to talk. I’m always nervous about presenting, but always glad I did it afterwards. I just wish I had some of the charisma of a Ben Goldacre. For one thing, we’re in the middle of a six week lockdown in Ireland, so it gave me the chance to talk to people who aren’t in my family, or the usual gang from work. Preparing the talk was an experience in itself; part of it was laying out the diversity of ways to visualise our data in R so I ended up making a series of plots in formats I hadn’t used before. And I got a really nice question from Zoe Turner (about catering for accessibility in plots), an issue that has set me thinking, definitely necessary of further investigation.\nAnd regrets:\n\nMid November is a busy enough time in a university so it was tricky to juggle the conference with a batch of other demands. There were lots of talks I regret missing, and listening to the playback is never quite the same as catching them live\nI lost count of the number of times I caught myself thinking ‘darn, I wish I’d gone to that workshop’. The workshops were the week before, the one I went to, gt tables with Rich Iannone, was terrific, but there were so many more\nI’d never used crowdcast before. Next time I’ll make a mental note not to have any material in the bottom 20% of the screen, it can get obscured by the vignettes that appear around the screen\n\nFinal thoughts. In my book, NHSr Community is on the right track. The people behind it have a clear vision. My impression is that there is an ethos; taking healthcare to a new level using measurement and statistics, and a spirit; fostering new skills with an attitude of open engagement. It’s pretty inspiring.\nEugene Hickey, Technology University, Dublin - Lecturer in Physics\nThis blog has been formatted to remove Latin Abbreviations\n\n\n\n Back to topReuseCC0 1.0"
  },
  {
    "objectID": "ts-and-cs.html",
    "href": "ts-and-cs.html",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders.\nNHS-R Community has prepared the content of this website responsibly and carefully. However, NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nNHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nNHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#legal-disclaimer",
    "href": "ts-and-cs.html#legal-disclaimer",
    "title": "Terms and conditions",
    "section": "",
    "text": "Statements of fact and opinion published on this website are those of the respective authors and contributors and not necessarily those of NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders.\nNHS-R Community has prepared the content of this website responsibly and carefully. However, NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders disclaim all warranties, express or implied, as to the accuracy of the information contained in any of the materials on this website or on other linked websites or on any subsequent links. This includes, but is not by way of limitation:\n\nany implied warranties of merchantability and fitness for a particular purpose.\nany liability for damage to your computer hardware, data, information, materials and business resulting from the information or the lack of information available.\nany errors, omissions, or inaccuracies in the information.\nany decision made or action taken or not taken in reliance upon the information.\n\nNHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders make no warranty as to the content, accuracy, timeliness or completeness of the information or that the information may be relied upon for any reason and bear no responsibility for the accuracy, content or legality of any linked site or for that of any subsequent links. NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders make no warranty that the website service will be uninterrupted or error-free or that any defects can be corrected.\nNHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders shall not be liable for any losses or damages (including without limitation consequential loss or damage) whatsoever from the use of, or reliance on, the information in this website, or from the use of the internet generally. Links to other websites or the publication of advertisements do not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders.\nThese disclaimers and exclusions shall be governed by and construed in accordance with the laws of England and Wales under the exclusive jurisdiction of the courts of England and Wales. Those who choose to access this site from outside the United Kingdom are responsible for compliance with local laws if and to the extent local laws are applicable.\nBy using this site, you agree to these terms and conditions of use."
  },
  {
    "objectID": "ts-and-cs.html#site-content",
    "href": "ts-and-cs.html#site-content",
    "title": "Terms and conditions",
    "section": "Site content",
    "text": "Site content\nAll content published through this site is open under the CC0 licence unless otherwise stated. We make every reasonable effort to locate, contact and acknowledge copyright owners and wish to be informed by any copyright owners who are not properly identified and acknowledged on this website so that we may make any necessary corrections.\nWhere licence terms for individual articles, videos, images and other published content permit republication, you may do so in accordance with the stated terms of the respective licence(s)."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-do-we-link-to",
    "href": "ts-and-cs.html#what-websites-do-we-link-to",
    "title": "Terms and conditions",
    "section": "What websites do we link to?",
    "text": "What websites do we link to?\nNHS-R Community editors and contributors recommend external web links on the basis of their suitability and usefulness for our users.\nIt is not our policy to enter into agreements for reciprocal links.\nThe inclusion of a link to an organisation’s or individual’s website does not constitute an endorsement or an approval by NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders of any product, service, policy or opinion of the organisation or individual. NHS-R Community, its editors, The Strategy Unit, The Health Foundation or other partners and funders are not responsible for the content of external websites."
  },
  {
    "objectID": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "href": "ts-and-cs.html#what-websites-will-we-not-link-to",
    "title": "Terms and conditions",
    "section": "What websites will we not link to?",
    "text": "What websites will we not link to?\nWe will not link to websites that contain racist, sexual or misleading content; that promote violence; that are in breach of any UK law; which are otherwise offensive to individuals or to groups of people.\nThe decision of NHS-R Community is final on publishing of content and no correspondence will be entered into.\nIf you wish to report a concern, please email nhs.rcommunity@nhs.net."
  },
  {
    "objectID": "ts-and-cs.html#software-and-services",
    "href": "ts-and-cs.html#software-and-services",
    "title": "Terms and conditions",
    "section": "Software and services",
    "text": "Software and services\nSource code and files for this site are available from GitHub. Use of our GitHub repository is governed by the Contributor Covenant Code of Conduct and further details for contribution can be found in NHS-R Statement on Tools.\nThis site is built using Quarto, an open-source scientific and technical publishing system developed by Posit. Quarto source code and software licences are available from GitHub.\nThe NHS-R Community website is hosted by GitHub Pages.\nThis site uses Google Analytics 4 for web analytics reporting."
  },
  {
    "objectID": "ts-and-cs.html#code-of-conduct",
    "href": "ts-and-cs.html#code-of-conduct",
    "title": "Terms and conditions",
    "section": "Code of conduct",
    "text": "Code of conduct\nThe NHS-R Community Code of Conduct is published in the NHS-R Way book and covers our expectations of behaviour in all situations, including events.\nContribution to this website/project can be found in the repository Code of Conduct."
  },
  {
    "objectID": "ts-and-cs.html#notice-and-takedown-policy",
    "href": "ts-and-cs.html#notice-and-takedown-policy",
    "title": "Terms and conditions",
    "section": "Notice and Takedown Policy",
    "text": "Notice and Takedown Policy\n\nincludes sensitive data redaction\nDetails of the Notice and Takedown Policy can be found in the NHS-R Way book along with details on how submissions with sensitive information will be rectified."
  },
  {
    "objectID": "reports/index.html",
    "href": "reports/index.html",
    "title": "Reports",
    "section": "",
    "text": "NHS-R 2025 Questionnaire Analysis\n\n\n\ndata-science\n\nengagement\n\ncommunity_building\n\n\n\nThis report describes the analysis of the results of a community-wide NHS-R online questionnaire which was active in Spring 2025.\n\n\n\n\n\n\n\n\n\n\nNo matching items\n Back to top"
  }
]