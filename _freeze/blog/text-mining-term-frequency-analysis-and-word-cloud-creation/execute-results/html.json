{
  "hash": "253b617534b570abdea5d34cff1be1ca",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Text Mining – Term Frequency analysis and Word Cloud creation in R\"\ndate: \"22 October 2018\"\ndate-modified: \"23 March 2024\"\ncategories:\n  - Text Mining\n  - Conference\nauthor: Gary Hutson\nimage: \"img/screenshot-wordcloud.png\"\nimage-alt: \"Wordcloud with cloud, plot and useful being the largest so most frequent\"\nsubtitle: >\n  \"Analysing the pre-conference workshop sentiments\"\nexecute: \n  eval: false\n---\n\nAnalysing the pre-conference workshop sentiments\n\nLoading in the required packages:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall_or_load_pack <- function(pack) {\n  create.pkg <- pack[!(pack %in% installed.packages()[, \"Package\"])]\n  if (length(create.pkg)) {\n    install.packages(create.pkg, dependencies = TRUE)\n  }\n  sapply(pack, require, character.only = TRUE)\n}\npackages <- c(\n  \"ggplot2\", \"tidyverse\", \"data.table\", \"wordcloud\", \"tm\", \"wordcloud2\",\n  \"scales\", \"tidytext\", \"devtools\", \"twitteR\", \"caret\", \"magrittr\", \"RColorBrewer\", \"tidytext\", \"ggdendro\",\n  \"tidyr\", \"topicmodels\", \"SnowballC\", \"gtools\"\n)\ninstall_or_load_pack(packages)\n```\n:::\n\n\nThis function was previously covered in blog post: [https://nhsrcommunity.com/blog/a-simple-function-to-install-and-load-packages-in-r/](https://nhsrcommunity.com/blog/a-simple-function-to-install-and-load-packages-in-r/).\n\nHere I specify that I want to load the main packages for dealing with sentiment and discourse analysis in R. Libraries such as {tm}, {wordcloud} and {wordcloud2} are loaded for working with this type of data.\n\n## Choosing the file to import\n\nThe file we have to import is a prepared csv file and instead of hard coding the path to load the file from I simply use:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npath <- choose.files()\n```\n:::\n\n\n![Screenshot of Select files window that pops up to allow files to be chosen](img/screenshot-choose.files.png)\n\nThis is a special function which allows you to open a dialog UI from R:\n\nFrom this dialog I select the csv file I want to be imported. Once I have selected the csv and hit open, the path variable will be filled with the location of the file to work with.\n\n##  Creating the R Data Frame\n\nTo create the data frame I can now pass the variable **path** to the `read_csv()` command:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nworkshop_sentiment <- read_csv(path, col_names = T)\n```\n:::\n\n\nThis will read the textual data from the workshops in to a data frame with 2 columns. The first relates to what the attendees enjoyed about the workshop and  the second relates to improvements that can be made:\n\n![Screenshot of the highlights and improvements text from a pre-conference survey](img/screenshot-text-conf.png)\n\n# Separate the master data frame\n\nThe master data frame now needs to be separated into two separate data frames, as text analysis requires one column with the number of rows for each sentence, as demonstrated. Here I use {magrittr} to divide this into two new data frames:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nws_highlights <- workshop_sentiment %>%\n  .[, 1]\n\n# Copy for improvements\n\nws_improvements <- workshop_sentiment %>%\n  .[, 2]\n```\n:::\n\n\nThe `ws_highlights` data frame uses the first column and  the `ws_improvements` data frame uses the second.\n\n## Function to create textual corpus\n\nAs I want to replicate this for highlights and improvements – I have created a function that could be replicated with any text analysis to create what is known as a text corpus (see: [https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf](https://cran.r-project.org/web/packages/tm/vignettes/tm.pdf)) this creates a series of documents, in our case sentences.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_tm <- function(x) {\n  library(tm)\n\n  corpus_tm <- Corpus(VectorSource(x))\n}\n```\n:::\n\n\nThis function allows you to pass any data frame to the function and creates a corpus for each data frame you pass to the function. The data frame would be passed to the x parameter. The `VectorSource()` function creates an element for each part of the corpus.\n\n## Create Corpus for Highlights and Improvements data frame\n\nNow the function has been created, I can simply pass the two separate data frames I created before to create two corpuses:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_positive <- corpus_tm(ws_highlights$Highlights)\n\ncorpus_improvements <- corpus_tm(ws_improvements$Improvements)\n```\n:::\n\n\nThe code block above shows that I create a corpus for the positive (highlights) data frame and an improvements corpus. This will display as hereunder in your environment:\n\n![Screenshot of the corpus output from the Environment in RStudio](img/screenshot-corpus-output.png)\n\n##Function to clean data in the corpus\n\nThe most common cleaning task of working with text data is to remove things like punctuation, common English words, and so on This is something I have to repeat multiple times when dealing with discourse analysis:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclean_corpus <- function(corpus_to_use) {\n  library(magrittr)\n\n  library(tm)\n\n  corpus_to_use %>%\n    tm_map(removePunctuation) %>%\n    tm_map(stripWhitespace) %>%\n    tm_map(content_transformer(function(x) iconv(x, to = \"UTF-8\", sub = \"byte\"))) %>%\n    tm_map(removeNumbers) %>%\n    tm_map(removeWords, stopwords(\"en\")) %>%\n    tm_map(content_transformer(tolower)) %>%\n    tm_map(removeWords, c(\"etc\", \"i.e.\", \"e.g.\", stopwords(\"english\")))\n}\n```\n:::\n\n\nThe parameter here takes the corpus object previously created and uses the corpus passed to:\n\n- Remove punctuation  \n- Strip out whitespace between each text item, as the VectorSource has stripped out each word from each sentence in the data frame  \n- Change the underlying formatting of the text to UTF-8  \n- Remove numbers  \n- Remove common English word (stop words)  \n- Change the case to lower case  \n- Remove a custom vector of words to adjust for things like \"e.g.\", \"i.e.\", \"etc\".\n\nTo clean the corpus objects I simply pass the original corpus objects back through this function to perform cleaning:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncorpus_positive <- clean_corpus(corpus_positive)\n\ncorpus_improvements <- clean_corpus(corpus_improvements)\n```\n:::\n\n\nInspection of one of the data frames confirms that this has successfully been cleaned:\n\n## Create TermDocumentMatrix to attain frequent terms\n\nThe term document matrix (explained well here: [https://www.youtube.com/watch?v=dE10fBCDWQc](https://www.youtube.com/watch?v=dE10fBCDWQc)) can be used with the corpus to identify frequent terms by classification on a matrix. However, more code is needed to do this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n find_freq_terms_fun <- function(corpus_in){\n\n  doc_term_mat <- TermDocumentMatrix(corpus_in)\n\n  freq_terms <- findFreqTerms(doc_term_mat)[1:max(doc_term_mat$nrow)]\n\n  terms_grouped <-\n\n    doc_term_mat[freq_terms,] %>%\n\n    as.matrix() %>%\n\n    rowSums() %>%\n\n    data.frame(Term=freq_terms, Frequency = .) %>%\n\n    arrange(desc(Frequency)) %>%\n\n    mutate(prop_term_to_total_terms=Frequency/nrow(.))\n\n  return(data.frame(terms_grouped))\n\n }\n```\n:::\n\n\nThis function needs explanation. The function uses as a single parameter the corpus that you need to pass in, then a variable is created to create the `doc_term_mat` which uses the tm TermDocumentMatrix.\n\nNext, I use the `findFreqTerms` function to iterate from the first entry to the maximum number of rows in the matrix. These are the powerhouses of the function, as they highlight how many times a word has been used in a sentence across all the rows of text.\n\nThe `terms_grouped` variable then slices the term matrix with the frequent terms, this is converted to a matrix, sum of each row are calculated for example the number of times the word appears. Then, a data frame is created of the terms in the function with the headings term and Frequency.\n\nNext, we use the power of {dplyr} to use arrange by the frequency descending and to add a mutated column to the data frame to calculate the proportion of that specific term over all terms. The `return(data.frame(terms_group))` then forces R to return the results of the function.\n\nI then pass my data frames (highlights and improvements) to the function I have just created to see if this method works:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npositive_freq_terms <- data.frame(find_freq_terms_fun(corpus_positive))\n\nimprovement_freq_terms <- data.frame(find_freq_terms_fun(corpus_improvements))\n```\n:::\n\n\nThese will be built as data frames and can be viewed in R Studio's Data environment window:\n\n![Screenshot of the RStudio's data environment window with terms and frequency](img/screenshot-term-window.png)\n\nThis has worked just as expected. You could now use ggplot2 to produce a bar chart / pareto chart of the terms.\n\n## Create a Word Cloud with the {wordcloud2} package\n\nR has a {wordcloud} package that produces relatively nice looking word clouds, but {wordcloud2} surpasses this in terms of visualisation. To use this function is easy now I have the frequent terms data frame – using the highlights data frame this can be implemented by using the below syntax:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwordcloud2(positive_freq_terms[,1:2],\n\n           shape=\"pentagon\",\n\n           color=\"random-dark\")\n```\n:::\n\n\nTo use the function I pass the data frame and use the term and frequency fields only to use the visualisation. There are a number of options and these can be accessed by using the `help(\"wordcloud2\")` function. Here I use the shape and color parameters to set the display of the word cloud:\n\n![Screenshot of the word cloud with Code being the largest word, followed by plot, useful, pick and facets](img/screenshot-wordcloud.png)\n\nThis can be exported in the viewer window by using the Export function.\n\nThis word cloud relates to the pre workshop prior to the conference. I personally thought the NHS-R conference was amazing and I was honoured to have a spot to speak amongst so many other brilliant R users.\n\nR is so versatile – every day is like a school day when you are learning it, but what a journey.\n\nThis blog has been edited for [NHS-R styles](https://nhsrway.nhsrcommunity.com/style-guides.html#style-guide-for-code) and has been formatted to remove [Latin Abbreviations](https://nhsrway.nhsrcommunity.com/style-guides.html#avoid-latin-abbreviation)\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}