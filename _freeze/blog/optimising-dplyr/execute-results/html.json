{
  "hash": "d42c7aa8bd3154587ca0b24944a42366",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Optimising dplyr\"\nauthor:\n  - name: Tom Jemmett\n    orcid: 0000-0002-6943-2990\n    email: thomas.jemmett@nhs.net\n    affiliations:\n      - name: The Strategy Unit\ncategories:\n  - dplyr\n  - data.table\n  - tidyverse\ndate: \"15 July 2021\"\ndate-modified: \"19 July 2024\"\n---\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(tidyverse)\n  library(data.table)\n})\nset.seed(2214)\n```\n:::\n\n\nRecently in a project I was working on I encountered an issue where one code chunk in my RMarkdown report started to take a significant amount of time to run, and it was using almost 100% of my RAM. The data I was working with was an extract of various tables from [HES](https://digital.nhs.uk/data-and-information/data-tools-and-services/data-services/hospital-episode-statistics): each row was relating to some activity that was undertaken.\n\nThe particular bit of code that was causing me problems was trying to create a simple chart that showed what percentage of people had particular types of activity. As each person may have had more than one of any of the activity types I had to take distinct sets of rows first, then calculate the percentage of individuals having that activity type.\n\nHowever, there was a slight complication in our data: as this was part of a wider report we would group some of the activity (the Critical Care Bed Day's) together at one part in the report, but also be able to break these records down into Elective/Emergency.\n\nThe table below shows what some of this activity data looked like (we just show the type's of activity and the subgroups below).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactivity_data <- tribble(\n  ~type,                  ~sub_group,                     ~alt_sub_group,        ~n,\n  \"Urgent Service Event\", \"Emergency Admission\",          NA,                    2.00,\n  \"Urgent Service Event\", \"A&E Attendance\",               NA,                    3.00,\n  \"Urgent Service Event\", \"111 Call\",                     NA,                    2.00,\n  \"Planned Contact\",      \"Outpatient Attendance\",        NA,                    4.00,\n  \"Planned Contact\",      \"Mental Health Contact\",        NA,                    0.12,\n  \"Planned Contact\",      \"IAPT Contact\",                 NA,                    0.10,\n  \"Planned Admission\",    \"Day Case Admission\",           NA,                    0.25,\n  \"Planned Admission\",    \"Regular Attendance Admission\", NA,                    0.07,\n  \"Planned Admission\",    \"Elective Admission\",           NA,                    0.10,\n  \"Bed\",                  \"Critical Care Bed Day\",        \"Elective Admission\",  0.08,\n  \"Bed\",                  \"Critical Care Bed Day\",        \"Emergency Admission\", 0.10\n)\n\n# don't show the n column here, this is just used to generate rows later\nactivity_data %>% select(-n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 11 × 3\n   type                 sub_group                    alt_sub_group      \n   <chr>                <chr>                        <chr>              \n 1 Urgent Service Event Emergency Admission          <NA>               \n 2 Urgent Service Event A&E Attendance               <NA>               \n 3 Urgent Service Event 111 Call                     <NA>               \n 4 Planned Contact      Outpatient Attendance        <NA>               \n 5 Planned Contact      Mental Health Contact        <NA>               \n 6 Planned Contact      IAPT Contact                 <NA>               \n 7 Planned Admission    Day Case Admission           <NA>               \n 8 Planned Admission    Regular Attendance Admission <NA>               \n 9 Planned Admission    Elective Admission           <NA>               \n10 Bed                  Critical Care Bed Day        Elective Admission \n11 Bed                  Critical Care Bed Day        Emergency Admission\n```\n\n\n:::\n:::\n\n\nTo demonstrate the issue with the initial summary code I produced we can generate some synthetic data. The real dataset included far more columns (e.g. the date of the activity, the organisation where the activity happened at), but for this demonstration we just generate a patient id.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nactivity_synthetic <- function(people) {\n  activity_data %>%\n    mutate(pid = map(n, function(.x) {\n      rpois(people, .x) %>%\n        imap(~rep(.y, .x)) %>%\n        flatten_dbl()\n    })) %>%\n    unnest(pid) %>%\n    select(-n)\n}\n\nactivity_100k <- activity_synthetic(100000)\nactivity_10k <- filter(activity_100k, pid <= 10000)\n```\n:::\n\n\nHere is a sample of this data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsample_n(activity_10k, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 10 × 4\n   type                 sub_group             alt_sub_group         pid\n   <chr>                <chr>                 <chr>               <dbl>\n 1 Planned Contact      Outpatient Attendance <NA>                 6253\n 2 Urgent Service Event A&E Attendance        <NA>                 1773\n 3 Planned Contact      Outpatient Attendance <NA>                 3300\n 4 Urgent Service Event 111 Call              <NA>                 9349\n 5 Bed                  Critical Care Bed Day Emergency Admission  5617\n 6 Planned Contact      Outpatient Attendance <NA>                 6971\n 7 Urgent Service Event Emergency Admission   <NA>                 1552\n 8 Urgent Service Event 111 Call              <NA>                 9607\n 9 Urgent Service Event A&E Attendance        <NA>                 8370\n10 Urgent Service Event Emergency Admission   <NA>                 4944\n```\n\n\n:::\n:::\n\n\nNow, for rendering in this particular part of the report we wanted to slightly modify the labels of activity used. We wanted to show the alternative subgroup labels along with the subgroup labels for the Critical Care Bed Day records.\n\nIdeally we would have done this at the data loading stage. However, because these fields were being used at other parts in the report we couldn't easily change the data at loading.\n\nThis is the code that I originally came up with, which I'm wrapping in a function for now so we can benchmark our different approaches later. First we update the `type` and `subgroup` columns, then we perform the summarisation steps (get the distinct rows for each individual, then count how many individuals there were for that activity).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr_naive <- function(activity) {\n  activity %>%\n    mutate(across(type, ~ifelse(sub_group == \"Critical Care Bed Day\",\n                                sub_group, .x)),\n           across(sub_group,\n                  ~ifelse(type == \"Critical Care Bed Day\",\n                          paste0(\"Critical Care (\",\n                                 word(alt_sub_group, 1),\n                                 \")\"),\n                          .x))) %>%\n    group_by(type, sub_group) %>%\n    distinct(pid) %>%\n    count() %>%\n    ungroup() %>%\n    arrange(type, sub_group)\n}\n```\n:::\n\n\nWe can test how long this takes to run with our 100,000 patient dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time( dplyr_naive(activity_100k) )[[\"elapsed\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 18.34\n```\n\n\n:::\n:::\n\n\nThis may not seem like a huge amount of time, but I was re-rendering this report upwards of 10 times an hour to ensure the pipeline was still working. Furthermore, we were rendering 12 versions of this report at a time (we had a report for each of the 11 [STPs](https://www.england.nhs.uk/integratedcare/stps/) in the Midlands, and an overall Midlands report), and this particular bit of code was chewing up all of my available memory, meaning we couldn't run in parallel.\n\nWhat is the problem with the approach above? When we update the `type` and `sub_group` columns in the mutate step R has to update all of the rows in the dataframe (in this case, 1,181,633 rows). But, R will not overwrite the original table in memory. Instead it will copy the entire data frame. This is what slows everything down.\n\nSo the question is, can we reduce the amount of rows we have to update? Yes! We can simply perform the summary steps before updating the labels! This is the approach I settled on: it's largely the same as above, I just have to include the `alt_sub_group` column in the group by step, and remove this column from the results at the end.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndplyr_improved <- function(activity) {\n  activity %>%\n    group_by(type, sub_group, alt_sub_group) %>%\n    distinct(pid) %>%\n    count() %>%\n    ungroup() %>%\n    mutate(across(type, ~ifelse(sub_group == \"Critical Care Bed Day\",\n                                sub_group, .x)),\n           across(sub_group,\n                  ~ifelse(type == \"Critical Care Bed Day\",\n                          paste0(\"Critical Care (\",\n                                 word(alt_sub_group, 1),\n                                 \")\"),\n                          .x))) %>%\n    select(-alt_sub_group) %>%\n    arrange(type, sub_group)\n}\n```\n:::\n\n\nHow does this perform?\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsystem.time( dplyr_improved(activity_100k) )[[\"elapsed\"]]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.19\n```\n\n\n:::\n:::\n\n\nSignificantly better!\n\n## **data.table**\n\nMany will tell you when start working with larger datasets in R you have to resort to using the `{data.table}` package. I'm not the most proficient `{data.table}` user in the world, so if you can think of a better way of solving this problem please let me know!\n\nFirst, here is basically the first approach translated to `{data.table}`\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.table_naive <- function(activity) {\n  adt <- as.data.table(activity)\n\n  adt[, type := ifelse(sub_group == \"Critical Care Bed Day\",\n                       \"Critical Care Bed Day\",\n                       type)]\n  adt[, sub_group := ifelse(type == \"Critical Care Bed Day\",\n                            paste0(\"Critical Care (\",\n                            word(alt_sub_group, 1),\n                            \")\"),\n                            sub_group)]\n  unique(adt)[order(type, sub_group), .(n = .N),\n              c(\"type\", \"sub_group\")]\n}\n```\n:::\n\n\nand here is the second approach\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.table_improved <- function(activity) {\n  adt <- as.data.table(activity) %>%\n    unique() %>%\n    .[, .(n = .N), c(\"type\", \"sub_group\", \"alt_sub_group\")]\n\n  adt[sub_group == \"Critical Care Bed Day\",\n      type := \"Critical Care Bed Day\"]\n  adt[type == \"Critical Care Bed Day\",\n      subgroup := paste0(\"Critical Care (\", word(alt_sub_group, 1), \")\")]\n  adt[, alt_sub_group := NULL]\n\n  adt[order(type, sub_group), ]\n}\n```\n:::\n\n\n## **benchmarking**\n\nNow, we can benchmark the different functions to see how they perform.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n  dplyr_naive(activity_10k),\n  data.table_naive(activity_10k),\n  dplyr_improved(activity_10k),\n  data.table_improved(activity_10k),\n  iterations = 10,\n  check = FALSE\n) %>%\n   select(expression, min, median, mem_alloc, n_gc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  expression                             min   median mem_alloc\n  <bch:expr>                        <bch:tm> <bch:tm> <bch:byt>\n1 dplyr_naive(activity_10k)         864.18ms    1.15s   44.85MB\n2 data.table_naive(activity_10k)       1.38s    1.64s   46.16MB\n3 dplyr_improved(activity_10k)       36.48ms  46.75ms    9.34MB\n4 data.table_improved(activity_10k)   49.2ms  67.56ms    7.75MB\n```\n\n\n:::\n:::\n\n\nThe table above shows how the improved approach is significantly better than the naive approach. We can see that the median time to run the function is significantly better (note that the naive functions are in seconds, but the improved functions are in milliseconds), but also the amount of memory that is being allocated is much, much lower. The `n_gc` value tells us how many times the [“garbage collector”](https://en.wikipedia.org/wiki/Garbage_collection_(computer_science)) ran. We want this figure to be low.\n\nWe can also see how the function works with the 100,000 row dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbench::mark(\n  dplyr_naive(activity_100k),\n  data.table_naive(activity_100k),\n  dplyr_improved(activity_100k),\n  data.table_improved(activity_100k),\n  iterations = 1,\n  check = FALSE\n) %>%\n  select(expression, min, median, mem_alloc, n_gc)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n  expression                              min   median mem_alloc\n  <bch:expr>                         <bch:tm> <bch:tm> <bch:byt>\n1 dplyr_naive(activity_100k)           18.06s   18.06s   455.3MB\n2 data.table_naive(activity_100k)      16.14s   16.14s   449.6MB\n3 dplyr_improved(activity_100k)      242.15ms 242.15ms    88.6MB\n4 data.table_improved(activity_100k)    1.09s    1.09s    69.6MB\n```\n\n\n:::\n:::\n\n\nAs we can see from both of these results both approaches in `{dplyr}` and `{data.table}` are broadly the same, with a slight performance edge to using `{data.table}` over `{dplyr}`. The big difference is to more sensibly arrange the order of operations so that you summarise first, then perform the costly data manipulation steps on a smaller data set.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}