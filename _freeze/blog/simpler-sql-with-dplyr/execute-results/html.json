{
  "hash": "52fb5f32ddb33cea0ffa6b16735a0f41",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simpler SQL with dplyr\"\ndate: \"7 June 2018\"\ndate-modified: '24 February 2024'\ncategories:\n  - R tips\n  - SQL\n  - dplyr\n  - Patient Flow\nauthor: John MacKintosh\nsubtitle: >\n  \"Comparing dplyr with SQL nested queries\"\nexecute: \n  eval: false\nimage: img/flow-patient-data.png  \n---\n\n\n# Comparing dplyr with SQL nested queries\n\nFollowing on from my last\n[post](https://johnmackintosh.net/blog/2018-05-13-apha-scotland-it-s-a-thing/),\nwhere I demonstrated R to some first time R users, I want to do a wee\ncomparison of dplyr V SQL, so that folks, particularly those in the NHS\nwho might be R curious, can see just what the fuss is about.\n\nTo do so I want to recap on the example I showed at the AphA Scotland \nevent.\n\nThis,in turn goes back to some work I’ve been doing with Neil Pettinger,\nwhere we are looking \n[at ways to visualise patient flow](https://johnmackintosh.net/blog/2017-12-21-flow/).\n\nThis relies on a spreadsheet that Neil originally put together. Part of\nmy demo was to explain how to recreate the visualisation in R, but I\nalso showed some of the data transformation steps carried out using\ndplyr and some fellow tidyverse helpers.\n\nIn this post I want to focus on that a but further, by showing the SQL\ncode I would write to arrive at the same end result.\n\nIn order to do this I imported Neil’s spreadsheet (which I’ve uploaded -\nwith Neil’s permission to the repo\n[RowOfDots](https://github.com/johnmackintosh/RowOfDots)) to into a SQL\nServer table (by using the built in import wizard, for a quick but not\nreproducible way of ingesting the data).\n\nHere’s how that looks:\n\n![](img/flow-patient-data.png){fig-alt=\"A screenshot of a spreadsheet with fake A&E data of times, names and ward transfers in and out.\"}\n\nNB - ALL patient names are entirely made up.\n\nAs a reminder, for this task we need to create a column that mimics\nExcel’s floor function and reduces the MovementDateTime field to the\nnearest 15 mins. We also want to get a count of how many patient were\neither moving IN or OUT during each 15 minute segment of the day.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME]\nGO\n```\n:::\n\n\nYou’d need to replace the database and table names to suit. I’m not\ngoing to explain the code for flooring the datetime field - just know\nthat it works, but you may want to compare the syntax for the case when\nstatement with the equivalent dplyr code ( see later).\n\nHere is the table output - with the 2 new columns at the end:\n\n![](img/flow-patient-data-colms.png){fig-alt=\"The same screenshot as before of fake data with two additional columns for Movement15 and counter\"}\n\nNow things get more complicated.\n\nI have a counter field, but I want to get a cumulative count by each 15\nminute segment, staging post and whether this was a movement in or out.\n\nOne way to do this is to wrap the original query inside another query,\nso that our newly created counter column can be utilised. This is a\nsimilar idea to the the method of mutating a column in dplyr, and having\nit available within the next pipe.\n\nWe have to make use of SQL’s windowing functionality to create virtual\ngroupings and orders within the data ( SQL is a set based language, and\nthere is no concept of row order within a set. Therefore to get a\ncumulative count, we need to make SQL think in terms of rows by\npartitioning the data by the desired grouping columns and providing\ncolumns to order by):\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT        x.[MovementDateTime],\nx.[FirstName],\nx.[LastName],\nx.[Ward_Dept],\nx.[Staging_Post],\nx.[Movement_Type],\nx.[IN_OUT],\nx.[Movement15],\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x\nUnderstanding windowing techniques is a great SQL skill to have. Don’t forget where you first saw this ;)!\n```\n:::\n\n\nUnderstanding windowing techniques is a great SQL skill to have. Don’t\nforget where you first saw this ;)!\n\nA couple of things to note here are that when we wrap or “nest” the\noriginal query, I gave it the alias ‘x’. You do need to provide an alias\nfor this inner query, or the outer query won’t work. Although not\nstrictly necessary, I also prefixed the column names in the outer query\nso it’s clear that I am selecting the columns from the “virtual” table\ndefined by the inner query.\n\nHere’s the output with our new Row number (or RNumber) field.\n\n![](img/flow-patient-data-rn.png){fig-alt=\"The same screenshot as before of fake data with the column R_Number added to the end\"}\n\nAlmost done, but this is still not in the right format - I need to get\nan accurate cumulative count. Once more, I take the previous query, and\nnest that inside a new query - so you can see this is similar to lots of\nbase R style manipulation where the code starts from the middle, or an\nend, and works back.\n\n\n::: {.cell}\n\n```{.sql .cell-code}\nSELECT y.MovementDateTime,\ny.FirstName,\ny.LastName,\ny.Ward_Dept,\ny.Staging_Post,\ny.Movement_Type,\ny.IN_OUT,\ny.Movement15,\ny.[counter],\ny.[counter] * y.R_Number AS Movement_15_SEQNO\nFROM (\nSELECT x.MovementDateTime,\nx.FirstName,\nx.LastName,\nx.Ward_Dept,\nx.Staging_Post,\nx.Movement_Type,\nx.IN_OUT,\nx.Movement15,\nx.[counter],\nROW_NUMBER() OVER (PARTITION BY IN_OUT, Movement_Type,Staging_Post,Movement15 ORDER BY (MovementDateTime))AS R_Number\nFROM\n(SELECT [MovementDateTime],\n[FirstName],\n[LastName],\n[Ward_Dept],\n[Staging_Post],\n[Movement_Type],\n[IN_OUT],\ncast(round(floor(cast([MovementDateTime] AS float(53))*24*4)/(24*4),5) AS smalldatetime) AS Movement15,\n(CASE WHEN IN_OUT = 'IN' THEN 1 ELSE -1 END) AS [counter]\nFROM [DB].[dbo].[TABLENAME])x) y\nORDER BY MovementDateTime\nGO\n```\n:::\n\n\nTo recap - our first query floored the movement time to 15 minute\nintervals and gave us a counter field, we then used that counter field\nto generate a row number field. Now, even if I’d ordered the result of\nthe second query by MovementDateTime, it still wouldn’t suffice because\nthe rownumbers are all positive, and I want them to be negative when the\nmovement was a movement OUT.\n\nWe can’t manipulate the row number field within the same query that it\nis created, so we nest the whole lot once more, this time arranging in\nthe correct time order and multiplying the counter field by our row\nnumber field.\n\nYou’ll notice the second query has been aliased (with a ‘y’) and the\ncolumns prefixed so that is is clear exactly where the query is\nobtaining the data from.\n\nThis gives us our final output:\n\n![](img/flow-patient-data-movement15.png){fig-alt=\"The same screenshot as before of fake data with the column Movement_15_SEQNO added to the end\"}\n\nA reminder of the dplyr code I used:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot_data <- data %>%\nmutate(Movement15 = lubridate::floor_date(MovementDateTime,\"15 minutes\")) %>%\ngroup_by(IN_OUT, Movement_Type,Staging_Post,Movement15) %>%\nmutate(counter = case_when(\nIN_OUT == 'IN' ~ 1,\nIN_OUT == 'OUT' ~ -1)) %>%\nmutate(Movement_15_SEQNO = cumsum(counter)) %>%\nungroup()\n```\n:::\n\n\nAnd here is the output - compare with above:\n\n![](img/flow-patient-data_rstudio.png){fig-alt=\"Screenshot with the same data columns but taken from RStudio. The image has a darker background with white text.\"}\n\nA lot more elegant? Definitely.\n\nAnother approach to writing the code in SQL would be to use a Common\nTable Expression, which is a more straightforward of writing and reading\nit. It’s a similar idea in that you create virtual tables with queries\nthat then run top to bottom until you get your final output. However\nthat is a post for another day :)\n\n**What I hope you get from this post is that dplyr and other packages\n(lubridate for example) really do make life easier for data\nmanipulation.**\n\nLook at the SQL for flooring the date, compared to the lubridate call.\nLook at the elegance of mutating new columns and having them available\nwithin the next chain, compared to horrendous multi-layered nested\nqueries (this one was pretty tame - imagine a few more levels on top of\nthat). You can see how traditional SQL can get unwieldy.\n\nDplyr is a fantastic asset to the R community, and I hope it might prove\nto be a great hook to get R further established within the analytical\ndepartments of the NHS.\n\nThis blog was written by John MacKintosh, [NHS](http://www.scot.nhs.uk/) data analyst based in\nInverness, Scotland, and was originally posted on his blog site [johnmackintosh.net]](https://johnmackintosh.net/blog/2018-05-31-dplyr-for-the-win/).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}